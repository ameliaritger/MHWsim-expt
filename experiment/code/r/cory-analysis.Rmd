---
title: "Corynactis MHWsim experiment data analysis"
author: "Amelia Ritger"
date: "2023-11-01"
output: html_document
---

# Load packages 
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(janitor)
library(here)
library(lubridate)
library(svglite) #save vector figures
library(ggpmisc) #statpolyeq ggplot
library(lme4) #GLMER
library(nlme) #nlme
library(glmmTMB) #GLMER
library(glarma)
#library(MASS) #GLARMA
library(tseries) #ADF test
library(urca) #KPSS test
library(forecast) #ts()
library(ggrepel) #geom_text_repel
library(broom) #tidy()
library(gridExtra) #grid.arrange()
library(vegan) #PERMANOVA
library(cluster) #Gower distance matrix
library(pairwiseAdonis) #pairwise comparisons with PERMANOVA
library(ape) #PCOA
library(AICcPermanova)
```

# Run this ONE TIME to load temperature/body size data, then save the file to your local machine
```{r}
#source("readRPiData.R", local = knitr::knit_global())
#source("readBodySizeData.R", local = knitr::knit_global())
```

# Load Tidied Data
```{r, include = FALSE}
source("mergeAllData.R", local = knitr::knit_global())

#Load temperature data AFTER you have run source(ReadRPiData)
rpi_temp <- read_csv(here("experiment", "data", "rpi_temp.csv"))
weekly_temp <- read_csv(here("experiment", "data", "weekly_temperature.csv"))
daily_temp <- read_csv(here("experiment", "data", "three_day_temperature.csv"))
```

## Create tidied df
```{r}
end_mhw <- ymd("2023-12-12")

all_size <- all %>%
  filter(!is.na(avg_size)) %>%
  mutate(avg_size_log = log(avg_size)) %>%
  group_by(tank, genet) %>%
  mutate(avg_size_diff = avg_size_log - lag(avg_size_log),
         avg_size_diff2 = avg_size_diff - lag(avg_size_diff)) %>%
  ungroup()

all_mhw <- all %>%
  filter(date <= end_mhw,
         date >= ymd("2023-10-06"))

#all_sum <- all %>%
  #filter(date == ymd("2023-10-06")) %>%
#  group_by(date) %>%
#  summarize(sum_n = sum(n))
```

# Count max/total number of polyps
```{r}
all_sum <- all %>%
  group_by(date) %>%
  #get sum of n_true
  summarize(nTRUE = sum(n_true),
            n = sum(n))

all_sum_genet <- all %>%
  filter(date == min(date)) %>%
  group_by(treatment, genet) %>%
  summarize(n = sum(n))

ggplot(all_sum_genet, aes(x = treatment, y = n, group = genet, color=genet)) +
  geom_segment(aes(yend = 0), size = 2, position = position_dodge(width = 0.8)) +
  geom_point(aes(fill = genet), size = 6, position = position_dodge(width = 0.8), shape = 21) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_x_discrete(labels = c("cold" = "Control", 
                              "severe" = "Severe MHW", 
                              "extreme" = "Extreme MHW")) +
  labs(x = "", y = "Number of polyps", fill = "Genet") +
  guides(color = "none") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 20),
        legend.position = "top") +
  coord_flip()

#ggsave(here("experiment", "figures", "WSN", "starting_n.png"), width=12, height=7)
```


# Visualize the data

## Data QA/QC plots
```{r}
ggplot(all, aes(x=date, y=diff_n_cont, color=genet)) +
  geom_point() +
  facet_wrap(~treatment) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal()
```

## Plot basic growth rate across entire experiment
```{r, fig.width=10}
ggplot(all, aes(x=date, y=n_true, color=genet)) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  geom_point(alpha=0.5) +
  geom_smooth(method = "lm", formula = y ~ x, se = F) +
  stat_poly_eq(use_label("eq"), formula = y ~ x, na.rm=TRUE) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  labs(x = "Date",
       y = "Cumulative population growth (no. polyps)",
       color = "Genet")

#ggsave(here("experiment", "figures", "growth_genet.png"), width=12, height=7)
```

```{r}
ggplot(all_mhw, aes(x=date, y=n_true)) +
  geom_point(aes(color=genet), size=1.5) +
  #geom_smooth(aes(fill=genet, color=genet), method="nls", formula = y ~ a * exp(-b * x) + C, start = list(a = max(all_mhw$avg_size), b = 0.1, C = min(all_mhw$avg_size)), se=T) + 
  geom_smooth(aes(fill=genet, color=genet), method="glm", method.args = list(family = gaussian(link = "log")), se=T) +
  #geom_smooth(aes(fill=genet, color=genet),method="loess", span=1, se=T) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_x_date(date_labels = "%b %d",
               breaks = scales::date_breaks("2 weeks"),
               limits = c(min(all_mhw$date), max = NA)) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  labs(x = "",
       y = "Number of ramets",
       color = "Genet",
       fill = "Genet") +  
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, vjust = 1, hjust=1, size=22),
        axis.text.y=element_text(size=22),
        axis.title.x=element_blank(),
        axis.title.y=element_text(size=24),
        legend.text=element_text(size=18),
        legend.title=element_text(size=22),
        strip.text = element_text(size = 24),
        strip.background = element_rect(fill="white"),
        legend.position = "top")

#ggsave(here("experiment", "figures", "dissertation", "n_polyps.png"), height=8, width=12)
```


## Plot growth rate of all genets during and after MHW
```{r}
ggplot(all, aes(x=date, y=growth_rate, color = treatment)) +
  geom_point(alpha=0.1) +
  facet_wrap(~mhw, scales="free", labeller = labeller(mhw = c("during" = "During MHW", "after" = "After MHW"))) +
  geom_smooth(method = "lm", formula = y ~ x, se = F) +
  stat_poly_eq(use_label("eq"), formula = y ~ x, parse = TRUE, label.x.npc = "left") +
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +
  theme_minimal() +
  coord_cartesian(ylim = c(-5, 25), expand=TRUE) +   #set equal y axes coordinates
  labs(x = "Date",
       y = "Cumulative population growth (no. polyps)",
       color = "Treatment")

#ggsave(here("experiment", "figures", "growth_MHW.png"), width=12, height=7)
```

## Plot growth rate of each genet during and after MHW
```{r, fig.width=10, fig.height=7}
ggplot(all, aes(x=date, y=growth_rate, color = genet)) +
  geom_point(alpha=0.1) +
  facet_grid(mhw~treatment, 
             #switch = 'x',
             scales = "free_x",
             labeller = labeller(mhw = c("during" = "During MHW", "after" = "After MHW"),
                                 treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  geom_smooth(method = "lm", formula = y ~ x, se = F) +
  stat_poly_eq(use_label("eq"), formula = y ~ x, parse = TRUE, label.x.npc = "left") +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  #coord_cartesian(ylim = c(-5, 25), expand=TRUE) +   #set equal y axes coordinates
  labs(x = "Date",
       y = "Cumulative population growth (no. polyps)",
       color = "Genet")
  theme(strip.placement = "outside")

#ggsave(here("experiment", "figures", "growth_MHW_genet.png"), width=12, height=10)
```

## Plot growth rate of each genet during and after MHW - but not faceted by MHW
```{r}
label_plot <- c("0.13", "0.09", "0.071", "0.0522", "0.0761",
                "0.151", "0.0933", "0.0252", "0.116", "0.0413", #extreme during
                "0.166", "0.142", "0.0522", "0.134", "0.0902",
                "-0.0023", "0.00741", "0.026", "0.00843", "-0.0131",
                "0.0102", "0.00652", "0.00881", "0.00421", "-0.0117", #extreme after
                "-0.0104", "-0.000895", "0.0217", "0.0127", "-0.0131")
color_plot <- rep(c("#9370DB", "#C21B78", "#FF9933", "#FF3333", "#662B45"), times = 6)
treatment_plot <- rep(c(rep("cold", 5), rep("extreme", 5), rep("severe", 5)), times = 2)
date_plot <- c(rep("2023-10-01", 15), rep("2024-01-10", 15))

p <- ggplot(all, aes(x=date, y=growth_rate_cont, color=genet, shape=mhw)) +
  geom_point(alpha=0.15) +
  facet_wrap(~factor(treatment, levels=c("cold", "severe", "extreme"), labels=c("Ambient", "Severe MHW", "Extreme MHW"))) +   geom_rect(aes(xmin = as.Date("2023-11-30"), xmax = as.Date("2023-12-10"), ymin = -5, ymax = -4.5), color = "dimgray", fill = "dimgray", alpha = 0.4) +  # Add horizontal bar
    geom_rect(aes(xmin = as.Date("2024-02-14"), xmax = as.Date("2024-02-21"), ymin = -5, ymax = -4.5), color = "dimgray", fill = "dimgray", alpha = 0.4) +  # Add horizontal bar
  geom_vline(xintercept = as.Date("2023-12-10"), linetype = "dashed") +  # Add vertical line delineating during/after MHW
  geom_smooth(method = "lm", se = FALSE) +
  #stat_poly_eq(use_label("eq"), formula = y ~ x, parse = TRUE) +  #un-comment this to get updated values to add to label_plot
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_shape_manual(values = c("during" = 16, "after" = 17)) +  # Define shapes for mhw levels
  theme_minimal() +
  coord_cartesian(ylim = c(-5, 25), expand=TRUE) +
  labs(x = "Date",
       y = "Cumulative population growth (no. polyps)",
       color = "Genet") +
  guides(shape = "none")

loop_count = 0
for (i in 1:length(label_plot)) {
  p <- p + geom_text(data = data.frame(date = as.Date("2023-12-30"), treatment = treatment_plot[i], mhw = "during"), 
              label=paste0("m = ", label_plot[i]), x = as.Date(date_plot[i]), y = 25-loop_count, hjust = 0, vjust = 1, color=color_plot[i])
  loop_count <- loop_count + 1
  if (loop_count > 4) {
    loop_count <- 0  # Reset counter after every 5th iteration
  }
}
p

#ggsave(here("experiment", "figures", "growth_MHW_genet_combined.png"), width=12, height=7)
```

## Plot raw population numbers
```{r}
g <- ggplot(all, aes(x=date, y=n_true, color=genet, shape=mhw)) +
  geom_point(alpha=0.15) +
  facet_wrap(~factor(treatment, levels=c("cold", "severe", "extreme"), labels=c("Ambient", "Severe MHW", "Extreme MHW"))) +
  geom_rect(aes(xmin = as.Date("2023-11-30"), xmax = as.Date("2023-12-10"), ymin = 5, ymax = 5.5), color = "dimgray", fill = "dimgray", alpha = 0.2) +  # Add horizontal bar
  geom_rect(aes(xmin = as.Date("2024-02-14"), xmax = as.Date("2024-02-21"), ymin = 5, ymax = 5.5), color = "dimgray", fill = "dimgray", alpha = 0.2) +  # Add horizontal bar
  geom_vline(xintercept = as.Date("2023-12-10"), linetype = "dashed", linewidth = 0.5) +  # Add vertical line delineating during/after MHW
  geom_smooth(method = "lm", se = FALSE) +
 # stat_poly_eq(use_label("eq"), formula = y ~ x, parse = TRUE) +  
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_shape_manual(values = c("during" = 16, "after" = 17)) +  # Define shapes for mhw levels
  theme_minimal() +
  #coord_cartesian(ylim = c(5, 40), expand=TRUE) +
  labs(x = "Date",
       y = "Number of polyps",
       color = "Genet") +
  guides(shape = "none")

loop_count = 0
for (i in 1:length(label_plot)) {
  g <- g + geom_text(data = data.frame(date = as.Date("2023-12-30"), treatment = treatment_plot[i], mhw = "during"), 
              label=paste0("m = ", label_plot[i]), x = as.Date(date_plot[i]), y = 40-loop_count, hjust = 0, vjust = 1, color=color_plot[i])
  loop_count <- loop_count + 1
  if (loop_count > 4) {
    loop_count <- 0  # Reset counter after every 5th iteration
  }
}
g

#ggsave(here("experiment", "figures", "n_MHW_genet_combined.png"), width=12, height=7)
```

## Plot behavior data
```{r, fig.width=10}
# %closed facet grid
ggplot(all, aes(x=genet, y=percent_closed, fill=genet, group=genet)) +
  geom_point(aes(color=genet)) +
  geom_violin(position="dodge", alpha=0.5, outlier.colour="transparent") +
  facet_grid(mhw~treatment)

# %closed facet wrap
ggplot(all, aes(x = genet, y = percent_closed, fill = genet, group = interaction(genet, mhw), shape = mhw)) +
  geom_point(aes(color = genet), position = position_dodge(width = 0.9)) +
  geom_violin(aes(size = mhw), position = position_dodge(width = 0.9), alpha = 0.5, outlier.colour = "transparent") +
  facet_wrap(~treatment) +
  scale_size_manual(values = c("during" = 1, "after" = 0.5)) + # Define sizes for mhw levels
  scale_shape_manual(values = c("during" = 16, "after" = 17))  # Define shapes for mhw levels

# %fullyOpen facet wrap
ggplot(all, aes(x = genet, y = percent_fully_open, fill = genet, group = interaction(genet, mhw), shape = mhw)) +
  geom_point(aes(color = genet), position = position_dodge(width = 0.9), alpha = 0.5) +
  geom_violin(aes(size = mhw), position = position_dodge(width = 0.9), alpha = 0.3, color = alpha("black", 0.75)) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_size_manual(values = c("during" = 1, "after" = 0.5), labels = c("During MHW", "After MHW")) + # Define sizes for mhw levels
  scale_shape_manual(values = c("during" = 16, "after" = 17), labels = c("During MHW", "After MHW")) + # Define shapes for mhw levels
  # change color of violin 
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  labs(x = "Genet",
       y = "% open polyps",
       color = "Genet",
       shape = "MHW",
       size = "MHW") +
  guides(fill = "none",
         color = "none")

#ggsave(here("experiment", "figures", "open_MHW_genet.png"), width=12, height=7)
```

## Other way to visualize behavior data
```{r}
all_mhw_group <- all_mhw %>%
  mutate(week = floor_date(date, unit = "week")) %>%
  group_by(week, treatment, genet) %>%
  summarize(avg = mean(percent_open),
            sd = sd(percent_open))

ggplot(all_mhw_group, aes(x=week, y=avg)) +
  geom_point(aes(color=genet), size=3) +
  geom_errorbar(aes(ymin = avg - sd, ymax = avg + sd, color=genet), width = 0.2) +
  #geom_smooth(aes(fill=genet, color=genet), method="loess", span=2, se=F) +
  geom_line(aes(color=genet)) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_x_date(date_labels = "%b %d",
               breaks = scales::date_breaks("2 weeks"),
               limits = c(min(all_mhw$date), max = NA)) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  labs(x = "",
       y = "Number of ramets",
       color = "Genet",
       fill = "Genet") +  
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, vjust = 1, hjust=1, size=22),
        axis.text.y=element_text(size=22),
        axis.title.x=element_blank(),
        axis.title.y=element_text(size=24),
        legend.text=element_text(size=18),
        legend.title=element_text(size=22),
        strip.text = element_text(size = 24),
        strip.background = element_rect(fill="white"),
        legend.position = "top")

ggplot(all_mhw_group, aes(x=week, y=avg, color=treatment, fill=treatment)) +
  geom_smooth(method="glm", method.args = list(family = gaussian(link = "log")), se=T, alpha=0.2) +
  geom_point(size=3) +
  geom_errorbar(aes(ymin = avg - sd, ymax = avg + sd), width = 0.2) +
  #geom_smooth(method="glm", method.args = list(family = gaussian(link = "log")), se=T) +
  #geom_smooth(method="loess", span=2, se=F) +
  #geom_line(aes(color=treatment)) +
  facet_wrap(~genet, ncol=3,
             labeller =  labeller(genet = c("A" = "Genet A", "B" = "Genet B", "C" = "Genet C", "D" = "Genet D", "E" = "Genet E"))) +
  scale_x_date(date_labels = "%b %d",
               breaks = scales::date_breaks("2 weeks"),
               limits = c(min(all_mhw$date), max = NA)) +
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                    labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +
  scale_fill_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                    labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +
  labs(x = "",
       y = "Proportion of open ramets",
       color = "Treatment",
       fill = "Treatment") +  
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, vjust = 1, hjust=1, size=22),
        axis.text.y=element_text(size=22),
        axis.title.x=element_blank(),
        axis.title.y=element_text(size=24),
        legend.text=element_text(size=20),
        legend.title=element_text(size=24),
        strip.text = element_text(size = 24),
        strip.background = element_rect(fill="white"),
        legend.position = "top")

#ggsave(here("experiment", "figures", "dissertation", "behavior.png"), height=12, width=12)
```

## Plot total biomass over time
```{r}
ggplot(all_mhw, aes(x=date, y=total_biomass)) +
  geom_point(aes(color=genet), size=0.5) +
  geom_smooth(aes(fill=genet, color=genet), method="lm", se=T) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  labs(x = "Date",
       y = "Total biomass (g)",
       color = "Genet",
       fill = "Genet")

#ggsave(here("experiment", "figures", "total_biomass_mhw.png"), width=12, height=7)
```

## Plot average body size over time
```{r}
ggplot(all_mhw, aes(x=date, y=log(avg_size))) +
  geom_point(aes(color=genet), size=1.5) +
  #geom_smooth(aes(fill=genet, color=genet), method="nls", formula = y ~ a * exp(-b * x) + C, start = list(a = max(all_mhw$avg_size), b = 0.1, C = min(all_mhw$avg_size)), se=T) + 
  geom_smooth(aes(fill=genet, color=genet), method="glm", method.args = list(family = gaussian(link = "log")), se=T) +
  #geom_smooth(aes(fill=genet, color=genet),method="loess", span=1.4, se=T) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_x_date(date_labels = "%b %d",
               breaks = scales::date_breaks("2 weeks"),
               limits = c(min(all_mhw$date), max = NA)) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  labs(x = "",
       y = "log(average body size) (mm)",
       color = "Genet",
       fill = "Genet") +  
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, vjust = 1, hjust=1, size=22),
        axis.text.y=element_text(size=22),
        axis.title.x=element_blank(),
        axis.title.y=element_text(size=24),
        legend.text=element_text(size=18),
        legend.title=element_text(size=22),
        strip.text = element_text(size = 24),
        strip.background = element_rect(fill="white"),
        legend.position = "top")

#ggsave(here("experiment", "figures", "dissertation", "logSize.png"), height=8, width=12)
```

## Plot mortality (lol)
```{r}
ggplot(all, aes(x=date, y=dying_dead, color = treatment)) +
  geom_point(alpha=0.1) +
  facet_wrap(~treatment, 
             labeller = labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  geom_smooth(method = "lm", formula = y ~ x, se = F) +
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00")) +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0, 10), labels = function(x) format(x, nsmall = 0)) +
  labs(x = "Date",
       y = "Mortality (no. polyps)",
       color = "Treatment") +
  guides(color = "none")

#ggsave(here("experiment", "figures", "mortality_MHW.png"), width=12, height=7)
```

############################################################################################################

# Start running stats

# Count data

### Prepare data
```{r}
all_count <- all %>%
  filter(date >= ymd("2023-10-06"),
         date <= end_mhw) %>%
  #convert date to a numeric sequence
  mutate(day_numeric = as.numeric(as.Date(date)-min(as.Date(date))),
         mhw_binary = as.factor(ifelse(mhw == "during", 0, 1))) %>%
  dplyr::select(day_numeric, date, tank, genet, treatment, mhw, mhw_binary, n_true, growth_rate, growth_rate_cont) %>%
  rename(growth_rate_mhw = growth_rate) %>% #relabel for better understanding. growth_rate_mhw is where the growth rate resets to zero when the MHW ends. I believe I won't need growth_rate_mhw for GLARMA model because I can add state as a potential covariate.
  mutate(genet_tank = paste(tank, genet, sep = ""), #merge genet and tank because each unique genet x tank value is going to be autocorrelated
         intercept = 1) # create intercept of 1s for glarma
```

### Check distribution
```{r}
hist(all_count$growth_rate_cont, breaks = seq(min(all_count$growth_rate_cont), max(all_count$growth_rate_cont) + 0.5, by = 2), 
     main = "Histogram of Growth Rate", xlab = "Growth Rate", 
     ylab = "Frequency", col = "lightblue")

hist(all_count$n_true, breaks = seq(min(all_count$n_true)-0.5, max(all_count$n_true) + 0.5, by = 2), 
     main = "Histogram of Counts", xlab = "Count", 
     ylab = "Frequency", col = "lightblue")
```

## Count data - GLARMA 

### Compare Poisson with NB model
```{r}
# Fit a Poisson model
poisson_growth <- glmer((growth_rate_cont+4) ~ treatment + day_numeric + (1|tank), family = poisson, data = all_count)
poisson_n <- glmer(n_true ~ treatment + day_numeric + (1|tank), family = poisson, data = all_count)

# Fit a Negative Binomial model
nb_growth <- glmer.nb((growth_rate_cont+4) ~ treatment + day_numeric + (1|tank), data = all_count)
nb_n <- glmer.nb(n_true ~ treatment + day_numeric + (1|tank), data = all_count)

# Compare models
AIC(poisson_growth, nb_growth, poisson_n, nb_n)
```
NB is a better fit with random Tank effect, but Poisson is better fit with random GenetTank effect

### Growth rate: QQ plot, chi square test, overdispersion ratio
```{r}
qqnorm(all_count$growth_rate_cont)
qqline(all_count$growth_rate_cont, col = "red")

# Create a table of observed counts
observed_counts <- table(all_count$growth_rate_cont)
# Calculate expected counts based on Poisson distribution
lambda <- mean(all_count$growth_rate_cont)
expected_counts <- dpois(as.numeric(names(observed_counts)), lambda) * length(observed_counts)
# Perform the Chi-squared test
chisq.test(observed_counts, p = expected_counts, rescale.p = TRUE)

growth_mean <- mean(all_count$growth_rate_cont)
growth_var <- var(all_count$growth_rate_cont)
# Check ratio
overdispersion_ratio <- growth_var / growth_mean
print(overdispersion_ratio) #growth rate is overdispersed

shapiro.test(all_count$growth_rate_cont)
```

### Counts: QQ plot, chi square test, overdispersion ratio
```{r}
qqnorm(all_count$n_true)
qqline(all_count$n_true, col = "red")

# Create a table of observed counts
observed_counts <- table(all_count$n_true)
# Calculate expected counts based on Poisson distribution
lambda <- mean(all_count$n_true)
expected_counts <- dpois(as.numeric(names(observed_counts)), lambda) * length(observed_counts)
# Perform the Chi-squared test
chisq.test(observed_counts, p = expected_counts, rescale.p = TRUE)

n_mean <- mean(all_count$n_true)
n_var <- var(all_count$n_true)
# Check ratio
overdispersion_ratio <- n_var / n_mean
print(overdispersion_ratio) #growth rate is overdispersed

shapiro.test(all_count$n_true)
```

Neither count data nor growth rate appear to follow poisson distribution. And both data metrics are overdispersed (mean << variance), so let's check with negative binomial distribution.

Growth rate contains negative numbers, which complicates the fact that it appears to follow negative binomial distribution. Let's focus on the raw count numbers for now and come back to growth rate later.

### Negative binomial distribution?
```{r, eval=FALSE}
nb_fit <- glmmTMB(n_true ~ time + (1|genet_tank), data = all_count, family = nbinom1)
summary(nb_fit)

residuals_nb <- residuals(nb_fit)
fitted_values_nb <- fitted(nb_fit)

glm.nb(n_true ~ 1, data = all_count) %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)

plot(residuals_nb, fitted_values_nb,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs. Fitted Values")
abline(h = 0, col = "red", lty = 2)
```
Doesn't look too good. 

### Let's just jump straight into GLARMA and see what happens
```{r}
y <- all_count$n_true %>%
  as.matrix()
X <- model.matrix(n_true ~ treatment + genet+ treatment:genet + mhw, data = all_count)
x <- model.matrix(n_true ~ treatment + genet + treatment:genet, data = all_count)

#equivalent but more wordy
#all_count_glarma <- all_count %>%
#  mutate(dummy = 1) %>%
#  pivot_wider(names_from = treatment, values_from = dummy, values_fill = #list(dummy = 0)) %>%
#  mutate(dummy = 1) %>%
#  pivot_wider(names_from = genet, values_from = dummy, values_fill = #list(dummy = 0)) %>%
#  mutate(dummy = 1) %>%
#  pivot_wider(names_from = genet_tank, values_from = dummy, values_fill = #list(dummy = 0))
# x <- all_count_glarma %>%
#   select(intercept, severe, extreme, B, C, D, E) %>%
#   as.matrix()

#glarma_model <- glarma(y, X, phiLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model <- glarma(y, x, phiLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_state <- glarma(y, X, phiLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

summary(glarma_model)
summary(glarma_model_state)
#interaction model is better all around - which doesn't deviate from the GLMM
# adding in state (MHW before vs after) improves model fit
```

#### Let's test GLARMA models now with different lags and distributions
```{r}
glarma_model_nr <- glarma(y, x, phiLags = c(1), type = "Poi", method = "NR", residuals = "Pearson", maxit = 100, grad = 1e-6)
summary(glarma_model_nr) #no effect of NR vs FS other than computation time being faster for NR
#glarma_model_nb <- glarma(y, x, phiLags = c(1), type = "NegBin", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6, alphaInit = 0.1)
#NegBin doesn't run --> WHY?????? My data do not follow poisson

#Let's just keep doing model comparison with different lags I guess...
glarma_model_2 <- glarma(y, x, phiLags = c(2), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_3 <- glarma(y, x, phiLags = c(3), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_4 <- glarma(y, x, phiLags = c(4), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_5 <- glarma(y, x, phiLags = c(5), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_6 <- glarma(y, x, phiLags = c(6), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_7 <- glarma(y, x, phiLags = c(7), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_8 <- glarma(y, x, phiLags = c(8), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

print(c(extractAIC(glarma_model), extractAIC(glarma_model_2), extractAIC(glarma_model_3), extractAIC(glarma_model_4), extractAIC(glarma_model_5), extractAIC(glarma_model_6), extractAIC(glarma_model_7)))
#lag 6 has lowest AIC, lag 5 has second lowest AIC
summary(glarma_model)
summary(glarma_model_6)

glarma_model_1_2 <- glarma(y, x, phiLags = c(1:2), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_3 <- glarma(y, x, phiLags = c(1:3), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_6 <- glarma(y, x, phiLags = c(1:6), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
print(c(extractAIC(glarma_model), extractAIC(glarma_model_6), extractAIC(glarma_model_1_6)))
summary(glarma_model_1_6)

glarma_model_1_7 <- glarma(y, x, phiLags = c(1:7), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_8 <- glarma(y, x, phiLags = c(1:8), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_9 <- glarma(y, x, phiLags = c(1:9), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_15 <- glarma(y, x, phiLags = c(1:15), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_30 <- glarma(y, x, phiLags = c(1:30), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

glarma_model_30 <- glarma(y, x, phiLags = c(30), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

models <- list(
  glarma_model = glarma_model,
  glarma_model_2 = glarma_model_2,
  glarma_model_3 = glarma_model_3,
  glarma_model_4 = glarma_model_4,
  glarma_model_6 = glarma_model_6,
  glarma_model_30 = glarma_model_30,
  glarma_model_1_2 = glarma_model_1_2,
  glarma_model_1_6 = glarma_model_1_6,
  glarma_model_1_15 = glarma_model_1_15,
  glarma_model_1_30 = glarma_model_1_30
)

for (model_name in names(models)) {
  aic_value <- extractAIC(models[[model_name]])  # [2] extracts the AIC value
  aic_value <- format(aic_value, nsmall = 2)  # Format to 2 decimal places
  print(paste(model_name, "=", aic_value))
}
# grad = 2.22e-16 is a more computationally intensive/"precise" option
```

#### Examine residuals
```{r}
plot_residuals <- function(model, model_name) {
  # Extract residuals and fitted values
  res <- residuals(model)
  fitted_values <- fitted(model)
  
  # Create a data frame for plotting
  residuals_data <- data.frame(
    Fitted = fitted_values,
    Residuals = res
  )
  
    # Residuals vs Fitted Plot
  p1 <- ggplot(residuals_data, aes(x = Fitted, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = paste(model_name, " - Residuals vs Fitted"), 
         x = "Fitted Values", 
         y = "Residuals") +
    theme_minimal()
  
   # Histogram of Residuals
  p2 <- ggplot(residuals_data, aes(x = Residuals)) +
    geom_histogram(binwidth = 0.1, fill = "blue", color = "black", alpha = 0.7) +
    labs(title = paste(model_name, " - Histogram of Residuals"), 
         x = "Residuals", 
         y = "Frequency") +
    theme_minimal()
  
  # QQ Plot
  p3 <- ggplot(residuals_data, aes(sample = Residuals)) +
    geom_qq() +
    geom_qq_line(color = "red") +
    labs(title = paste(model_name, " - QQ Plot of Residuals")) +
    theme_minimal()
  
  # Print plots
  print(p1)
  print(p2)
  print(p3)
}

#glarma_model_1_10 <- glarma(y, x, phiLags = c(1:10), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
#glarma_model_1_11 <- glarma(y, x, phiLags = c(1:11), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_12 <- glarma(y, x, phiLags = c(1:12), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_13 <- glarma(y, x, phiLags = c(1:13), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_14 <- glarma(y, x, phiLags = c(1:14), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
```

#### Plot residuals
```{r}
plot_residuals(glarma_model_1_12, "1-12")
plot_residuals(glarma_model_1_13, "1-13")
```
Between lag 1:12 and 1:13 is where the QQ plot and residuals plot changes shape significantly and shifts to a new regime

#### Check lags with day + week/month
```{r}
glarma_model_week <- glarma(y, x, phiLags = c(1, 2), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_2week <- glarma(y, x, phiLags = c(1, 4), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_month <- glarma(y, x, phiLags = c(1, 8), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_2month <- glarma(y, x, phiLags = c(1, 16), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
```

#### Plot it
```{r}
#plot_residuals(glarma_model_week, "day+week")
plot_residuals(glarma_model_2week, "day+2week")
plot_residuals(glarma_model_1_2, "dayTHROUGHweek")
plot_residuals(glarma_model_month, "day+1month")
plot_residuals(glarma_model_1_8, "dayTHROUGHmonth")
plot_residuals(glarma_model_2month, "day+2month")
```

All of these are way worse fits compared to the 1:12 models, at least in terms of residual gaps

```{r}
#plot.glarma(glarma_model) #not good fit
plot.glarma(glarma_model_3)
plot.glarma(glarma_model_week)
plot.glarma(glarma_model_6)
plot.glarma(glarma_model_1_6)
```

### Add thetaLag
```{r, eval=FALSE}
#glarma_model_theta <- glarma(y, x, thetaLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
extractAIC(glarma_model_theta)
extractAIC(glarma_model)
plot.glarma(glarma_model)
plot.glarma(glarma_model_theta)
```
Very similar fits, phiLag is slightly better AIC so let's just stick with phiLag

### Compare "final" contender models
```{r}
summary(glarma_model_1_3) #one day through 1 week
summary(glarma_model_1_9) #one day through 1 month
summary(glarma_model_1_12) #one day through 5 weeks
summary(glarma_model_1_13) #one day through 6 weeks
summary(glarma_model_1_30) #one through 30

glarma_model_1_end <- glarma(y, x, phiLags = c(1:36), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
extractAIC(glarma_model_1_end)
summary(glarma_model_1_end) #one day ago through "start" of experiment
```
Glarma model with phiLags(1:36) is all around the best fit... but that seems fishy. Ask Chris! And let's move on.

### Let's check the lag values with the state model
```{r}
glarma_model_week <- glarma(y, X, phiLags = c(16), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_2week <- glarma(y, X, phiLags = c(1, 16), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_month <- glarma(y, X, phiLags = c(1:8), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_2month <- glarma(y, X, phiLags = c(1:16), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

models <- list(
  glarma_model_state = glarma_model,
  glarma_model_week = glarma_model_week,
  glarma_model_2week = glarma_model_2week,
  glarma_model_month = glarma_model_month,
  glarma_model_2month = glarma_model_2month
)

for (model_name in names(models)) {
  aic_value <- extractAIC(models[[model_name]])  # [2] extracts the AIC value
  aic_value <- format(aic_value, nsmall = 2)  # Format to 2 decimal places
  print(paste(model_name, "=", aic_value))
}
#phiLags (1,16) significantly better, 1,2 1,4, 1,8 have equivalent AIC
#phiLags (1:16) is significantly better, 1:8 is also good; 1:2, 1:4 have similar AIC to 1,2 and 1,4
#phiLags (16) is best, 2, 4, 8 have similar AIC values
#comparing "best of the best", 16 and 1, 16 are worst. 1:8 and 1:16 are best

glarma_model_long <- glarma(y, X, phiLags = c(1:36), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
extractAIC(glarma_model_long)
summary(glarma_model_long)

plot_residuals(glarma_model_state, "Lag1")
plot.glarma(glarma_model_state)

glarma_model_state_13<- glarma(y, X, phiLags = c(1:13), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
```

### Plot it
```{r}
plot_residuals(glarma_model_state_13, "Lag13")
plot.glarma(glarma_model_state_13)
plot_residuals(glarma_model_long, "LagAll")
plot.glarma(glarma_model_long)
```

Once you get to lags(1:36), the addition of the state variable doesn't matter (the model AICs are equivalent between phiLag(1:36) for model with state versus model without state) So... where to go from here? Probably compare with summaries. 

### Let's try working with the lagged count data?
```{r}
count_glarma_lag <- all_count %>%
  arrange(date) %>%
  group_by(tank, genet) %>%
  mutate(lag1 = lag(n_true, 1))

count_glarma_mhw <- all_count %>%
  filter(mhw == "during")
  
y2 <- count_glarma_mhw$n_true %>%
  as.matrix()
x2 <- model.matrix(n_true ~ treatment+genet, data = count_glarma_mhw)
glarma_model_mhwonly <- glarma(y2, x2, phiLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
plot_residuals(glarma_model_mhwonly, "Lag13")
plot.glarma(glarma_model_mhwonly)
fitted_values <- fitted(glarma_model_mhwonly)
result_df <- data.frame(PredictorVariables = x2, FittedValues = fitted_values) %>%
  filter(fitted_values < 15)

res <- residuals(glarma_model_mhwonly)
fitted_values <- fitted(glarma_model_mhwonly)
  
  # Create a data frame for plotting
residuals_data <- data.frame(
    Fitted = fitted_values,
    Residuals = res
  )

ggplot(residuals_data, aes(x = Fitted, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = paste(model_name, " - Residuals vs Fitted"), 
         x = "Fitted Values", 
         y = "Residuals") +
    theme_bw() 
```

## Count data - something different, GLMM with autocorrelation
```{r}
count_glmm1 <- lme(n_true ~ treatment + genet,
                   random = ~1|tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|tank))
count_glmm2 <- lme(n_true ~ treatment + genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
AIC(count_glmm1, count_glmm2)
summary(count_glmm1)
summary(count_glmm2)

#genet_tank is definitely the correct "random effect" I need to incorporate, especially because it has lower residual standard dev and high autocorrelation (Phi) and MUCH lower AIC

count_glmm3 <- lme(n_true ~ treatment + genet + treatment:genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))

AIC(count_glmm2, count_glmm3)
summary(count_glmm2)
summary(count_glmm3)

#even with added complexity and higher DF, count_glmm3 has a lower AIC suggesting the interaction improves the model fit significantly enough to outweigh the penalty from model complexity.

count_glmm4 <- lme(n_true ~ treatment + genet + treatment:genet, #same syntax as treatment*genet
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1())
AIC(count_glmm4, count_glmm3) #same AIC
```

### Check residuals
```{r}
residual_values <- residuals(count_glmm3)
fitted_values <- fitted(count_glmm3)

plot(fitted_values, residual_values,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs. Fitted Values",
     pch = 19,
     col = "blue")

# Add a horizontal line at 0 for reference
abline(h = 0, col = "red", lty = 2)

qqnorm(residual_values)
qqline(residual_values, col = "red")
title("QQ Plot of Residuals")
```
Residual plot looks off...

### Simpler models
```{r}
#count_temp <- all_count %>%
#  mutate(week = as.numeric(difftime(date, min(weekly_temp$friday), units = "weeks")) + 1,
#         tank = as.numeric(tank)) %>%
#  #select(date, week, tank, mhw, genet, avg_size_log) %>%
#  left_join(weekly_temp, by = c("date" = "friday", "tank"))

count_glmm5 <- lme(n_true ~ treatment,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
count_glmm6 <- lme(n_true ~ genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
summary(count_glmm5)
summary(count_glmm6)
AIC(count_glmm4, count_glmm5, count_glmm6) #Treatment only is the worst AIC. so, treatment alone isn't best predictor.
BIC(count_glmm4, count_glmm5, count_glmm6) #Treatment only is the worst AIC. so, treatment alone isn't best predictor.

count_glmm7 <- lme(n_true ~ treatment*genet + mhw,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
AIC(count_glmm7) #adding during vs after MHW (state) is worse 
summary(count_glmm7)

count_glmm8 <- lme(n_true ~ treatment*mhw,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
AIC(count_glmm8) #bad
```

At least for the GLMM, the treatment*genet with genet_tank random effect is best performing.

### Let's super simplify by only looking at during MHW
```{r}
all_count_mhw <- all_count %>%
  filter(mhw == "during")

count_glmm10 <- lme(n_true ~ treatment,
                   random = ~1|genet_tank,
                   data = all_count_mhw,
                   correlation = corAR1(form = ~ 1|genet_tank))

count_glmm11 <- lme(n_true ~ genet,
                   random = ~1|genet_tank,
                   data = all_count_mhw,
                   correlation = corAR1(form = ~ 1|genet_tank))

count_glmm12 <- lme(n_true ~ treatment*genet,
                   random = ~1|genet_tank,
                   data = all_count_mhw,
                   correlation = corAR1(form = ~ 1|genet_tank))

AIC(count_glmm10, count_glmm11, count_glmm12) #treatment alone still the worst
BIC(count_glmm10, count_glmm11, count_glmm12) #treatment alone still the worst

summary(count_glmm11)
summary(count_glmm12)
```
Treatment alone is the WORST, genet alone doesn't make a lot of sense especially when plotting the faceted data and my understanding of the experiment, so let's move forward with treatmentxgenet interaction model. 

### Let's try other correlation options
```{r}
autoArima <- auto.arima(all_count$n_true, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(autoArima)

count_glmm13 <- lme(n_true ~ treatment*genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 1, q = 1))

summary(count_glmm13)
BIC(count_glmm13)

#count_glmm15 <- lme(n_true ~ treatment*genet,
#                   random = ~1|genet_tank,
#                   data = all_count,
#                   correlation = corCAR1(form = ~ day_numeric | genet_tank))
# I don't need corCAR1 because my time points are evenly spaced (relatively) - so corCAR1 is overkill

#summary(count_glmm15)
#AIC(count_glmm14, count_glmm15) #corARMA better
#BIC(count_glmm14, count_glmm15) #corARMA better
```

## Let's for loop through a bunch of p/q options for the corARMA
```{r}
p_values <- c(0, 1, 2, 3)  # Values for p
q_values <- c(0, 1, 2)  # Values for q

# Initialize an empty list to store the models
model_list <- list()

# Initialize an empty data frame to store the BIC values
bic_table <- data.frame(p = integer(), q = integer(), BIC = numeric(), stringsAsFactors = FALSE)

# Loop through all combinations of p and q
for (p in p_values) {
  for (q in q_values) {
    # Fit the model with the current p and q values
    model_name <- paste("count_glmm_p", p, "_q", q, sep = "")  # Create a unique model name
    model <- tryCatch({
      lme(n_true ~ treatment * genet,
          random = ~1 | genet_tank,
          data = all_count,
          correlation = corARMA(p = p, q = q))
    }, error = function(e) {
      # If there is an error, store the error message in the list
      return(e$message)
    })
    
    # Check if the model fit was successful (not an error message)
    if (inherits(model, "lme")) {
      # Extract BIC value
      bic_value <- BIC(model)
      
      # Append the BIC value and the corresponding p and q to the data frame
      bic_table <- rbind(bic_table, data.frame(p = p, q = q, BIC = bic_value))
    } else {
      # If there's an error, you can also record it in the table
      bic_table <- rbind(bic_table, data.frame(p = p, q = q, BIC = NA))
    }
  }
}
```

Some models didn't fit but many did! And it looks like best fit is p=2, q=1

Let's now check this "best fit" model with different inputs
```{r}
options <- c("treatment", "genet", "treatment+genet", "treatment*genet")

# Initialize an empty data frame to store the results
bic_table <- data.frame(model = character(), BIC = numeric(), stringsAsFactors = FALSE)

# Loop through all combinations of fixed effects
for (option in options) {
  # Create the formula dynamically using as.formula
  formula <- as.formula(paste("n_true ~", option))
  # Fit the model with the current option and fixed p=2, q=1 for ARMA
  model <- tryCatch({
    lme(formula,
        random = ~1 | genet_tank,
        data = all_count,
        correlation = corARMA(p = 2, q = 1))
  }, error = function(e) {
    # Print the error if there is one
    print(paste("Error for option:", option, ":", e$message))
    return(NULL)
  })
  
  # If the model successfully fitted, extract BIC
  if (!is.null(model)) {
    bic_value <- BIC(model)
    # Add the model name and BIC to the table
    bic_table <- rbind(bic_table, data.frame(model = option, BIC = bic_value))
  } else {
    # If there's an error, store NA for BIC
    bic_table <- rbind(bic_table, data.frame(model = option, BIC = NA))
  }
}
```
Genet only and Treatment only are the best fitting BICs

Let's double check p=2 and q=1 are correct!
```{r}
options <- c("treatment", "genet", "treatment+genet", "treatment*genet")
p_values <- c(0, 1, 2, 3)  # Values for p
q_values <- c(0, 1, 2)  # Values for q

# Initialize an empty data frame to store the results
bic_table <- data.frame(model = character(), p = integer(), q = integer(), BIC = numeric(), AIC = numeric(), stringsAsFactors = FALSE)

# Loop through all combinations of options, p, and q
for (option in options) {
  for (p in p_values) {
    for (q in q_values) {
      # Create the formula dynamically using as.formula for the fixed effects (option)
      formula <- as.formula(paste("n_true ~", option))
      
      # Fit the model with the current option and fixed p, q for ARMA
      model <- tryCatch({
        lme(formula,
            random = ~1 | genet_tank,
            data = all_count,
            correlation = corARMA(p = p, q = q))
      }, error = function(e) {
        # Print the error if there is one
        print(paste("Error for option:", option, "p:", p, "q:", q, ":", e$message))
        return(NULL)
      })
      
      # If the model successfully fitted, extract BIC
      if (!is.null(model)) {
        bic_value <- BIC(model)
        aic_value <- AIC(model)
        # Add the model name, p, q, and BIC to the table
        bic_table <- rbind(bic_table, data.frame(model = option, p = p, q = q, BIC = bic_value, AIC = aic_value))
      } else {
        # If there's an error, store NA for BIC
        bic_table <- rbind(bic_table, data.frame(model = option, p = p, q = q, BIC = NA, AIC = NA))
      }
    }
  }
}
```

p=2, q=1 is definitely top 2 best models. p=3, q=1 comes in close second. Genet only is slightly better (VERY SLIGHTLY) than Treatment+Genet.

But, this is count data! So let's see if we need to do Poisson or NB. 
Are the data overdispersed? 
```{r, eval=FALSE}
mean_n_true <- mean(all_count$n_true, na.rm = TRUE)
var_n_true <- var(all_count$n_true, na.rm = TRUE)

overdispersion_by_group <- all_count %>%
  group_by(genet_tank) %>%
  summarise(
    mean_n_true = mean(n_true, na.rm = TRUE),
    var_n_true = var(n_true, na.rm = TRUE)
  )

# Check overdispersion for each group
overdispersion_by_group %>%
  mutate(overdispersion = ifelse(var_n_true > mean_n_true, "Yes", "No"))
```
The population-wide data is overdispersed, the groups are not. But lme() doesn't support NB or Poisson data. But other GLMM options don't support autocorrelation...so that's something for the discussion section then I guess bc I don't have the time!


# Growth data

## Prep data
```{r}
all_growth <- all %>%
  filter(date >= ymd("2023-10-06"),
         date <= end_mhw,
         !is.na(avg_size)) %>%
  #convert date to a numeric sequence
  mutate(day_numeric = as.numeric(as.Date(date)-min(as.Date(date))),
         mhw_binary = as.factor(ifelse(mhw == "during", 0, 1)),
         avg_size_log = log(avg_size)) %>%
  dplyr::select(day_numeric, date, tank, genet, treatment, mhw, mhw_binary,avg_size, avg_size_log) %>%
  mutate(genet_tank = paste(tank, genet, sep = "")) #merge genet and tank because each unique genet x tank value is going to be autocorrelated
```

## Check distribution
```{r}
hist(all_growth$avg_size, 
     main = "Histogram of Avg Size", xlab = "", 
     ylab = "Frequency", col = "lightblue")

hist(all_growth$avg_size_log, 
     main = "Histogram of log(Avg Size)", xlab = "", 
     ylab = "Frequency", col = "lightblue")
```
Definitely log transform avg size

```{r}
options <- c("treatment", "genet", "treatment+genet", "treatment*genet")
p_values <- c(0, 1, 2, 3)  # Values for p
q_values <- c(0, 1, 2)  # Values for q

# Initialize an empty data frame to store the results
bic_table <- data.frame(model = character(), p = integer(), q = integer(), BIC = numeric(), AIC = numeric(), stringsAsFactors = FALSE)

# Loop through all combinations of options, p, and q
for (option in options) {
  for (p in p_values) {
    for (q in q_values) {
      # Create the formula dynamically using as.formula for the fixed effects (option)
      formula <- as.formula(paste("avg_size_log ~", option))
      
      # Fit the model with the current option and fixed p, q for ARMA
      model <- tryCatch({
        lme(formula,
            random = ~1 | genet_tank,
            data = all_growth,
            correlation = corARMA(p = p, q = q))
      }, error = function(e) {
        # Print the error if there is one
        print(paste("Error for option:", option, "p:", p, "q:", q, ":", e$message))
        return(NULL)
      })
      
      # If the model successfully fitted, extract BIC
      if (!is.null(model)) {
        bic_value <- BIC(model)
        aic_value <- AIC(model)
        # Add the model name, p, q, and BIC to the table
        bic_table <- rbind(bic_table, data.frame(model = option, p = p, q = q, BIC = bic_value, AIC = aic_value))
      } else {
        # If there's an error, store NA for BIC
        bic_table <- rbind(bic_table, data.frame(model = option, p = p, q = q, BIC = NA, AIC = NA))
      }
    }
  }
}
```

p=3, q=1 are top 2 best models. Treatment only is slightly better than Genet only. 

# Behavior data

## Prep data
```{r}
all_behavior <- all %>%
  filter(date >= ymd("2023-10-06"),
         date <= end_mhw) %>%
  #convert date to a numeric sequence
  mutate(day_numeric = as.numeric(as.Date(date)-min(as.Date(date))),
         mhw_binary = as.factor(ifelse(mhw == "during", 0, 1))) %>%
  dplyr::select(day_numeric, date, tank, genet, treatment, mhw, mhw_binary,percent_open, percent_fully_open, percent_closed, n, n_true) %>%
  mutate(genet_tank = paste(tank, genet, sep = ""),
         day_factor = as.factor(day_numeric))
```

## Check distribution and dispersion
```{r}
hist(all_behavior$percent_open, 
     main = "Histogram of %open", xlab = "", 
     ylab = "Frequency", col = "lightblue")

hist(all_behavior$percent_fully_open, 
     main = "Histogram of %fully open", xlab = "", 
     ylab = "Frequency", col = "lightblue")

hist(all_behavior$percent_closed, 
     main = "Histogram of %closed", xlab = "", 
     ylab = "Frequency", col = "lightblue")

mean(all_behavior$percent_open, na.rm = TRUE)
var(all_behavior$percent_open, na.rm = TRUE) #not overdispersed!
overdispersion_by_group <- all_behavior %>%
  group_by(genet_tank) %>%
  summarise(
    mean_n_true = mean(percent_open, na.rm = TRUE),
    var_n_true = var(percent_open, na.rm = TRUE)
  )

ggplot(all_behavior, aes(x = date, y = percent_open, group=genet_tank, color=genet_tank)) +
  geom_line() +
  geom_point() +
  labs(title = "Percentage of Behavior Over Time", x = "Time", y = "Percentage of Behavior") +
  theme_minimal()

# Check autocorrelation within each group
grouped_data <- split(all_behavior, all_behavior$genet_tank)
lapply(names(grouped_data), function(group_name) {
  group_data <- grouped_data[[group_name]]
  acf(group_data$percent_closed, 
      lag.max = 30, 
      main = paste("ACF for Group:", group_name))
})
```

16/75 groups are temporally autocorrelated for percent_open/percent_closed and 20/75 for percent_fully_open. So.... can I ignore it for simplicity (for the dissertation at least...)
9D, 7D/C/B, 5B, 4D/B/A, 2B, 14B/13D, 12C, 10D: lag 2
9C: 5, 8B: 7, 4E: 1&3

Amb: 6
Sev: 7
Ext: 3

A: 1
B: 6
C: 3
D: 5
E: 1

```{r}
overdispersion_by_group <- all_behavior %>%
  group_by(genet_tank) %>%
  summarise(
    mean_n_true = mean(percent_fully_open, na.rm = TRUE),
    var_n_true = var(percent_fully_open, na.rm = TRUE)
  ) #my data are definitely not overdispersed so that's good

model_simple <- lmer(percent_fully_open ~ treatment * genet * day_numeric + (1 | genet_tank), data = all_behavior)
summary(model_simple)

all_behavior$percent_fully_open_logit <- log(all_behavior$percent_fully_open / (1 - all_behavior$percent_fully_open))

model_binomial_treat <- glmmTMB(percent_fully_open ~ treatment + (1 | genet_tank), data = all_behavior, family = binomial(), weights=n)
model_binomial_genet <- glmmTMB(percent_fully_open ~ genet + (1 | genet_tank), data = all_behavior, family = binomial(), weights=n)
model_binomial_treat_genet <- glmmTMB(percent_fully_open ~ treatment + genet + (1 | genet_tank), data = all_behavior, family = binomial(), weights=n)
model_binomial_treat_by_genet <- glmmTMB(percent_fully_open ~ treatment * genet + (1 | genet_tank), data = all_behavior, family = binomial(), weights=n)

model_binomial_genet_ar <- glmmTMB(percent_fully_open ~ genet + (1 | genet_tank) + ar1(day_factor + 0|genet), data = all_behavior, family = binomial(), weights=n)
model_binomial_treat_ar <- glmmTMB(percent_fully_open ~ treatment + (1 | genet_tank) + ar1(day_factor + 0|treatment), data = all_behavior, family = binomial(), weights=n)
model_binomial_treat_genet_ar <- glmmTMB(percent_fully_open ~ treatment + genet + (1 | genet_tank) + ar1(day_factor + 0|treatment), data = all_behavior, family = binomial(), weights=n)
model_binomial_treat_genet_ar2 <- glmmTMB(percent_fully_open ~ treatment + genet + (1 | genet_tank) + ar1(day_factor + 0|genet), data = all_behavior, family = binomial(), weights=n)
model_binomial_treat_genet_ar3 <- glmmTMB(percent_fully_open ~ treatment * genet + (1 | genet_tank) + ar1(day_factor + 0|treatment), data = all_behavior, family = binomial(), weights=n)

model_binomial_treat_genet_day_ar <- glmmTMB(percent_fully_open ~ treatment * genet + day_numeric + (1 | genet_tank) + ar1(day_factor + 0|treatment), data = all_behavior, family = binomial(), weights=n)
model_binomial_treat_genet_day_ar2 <- glmmTMB(percent_fully_open ~ treatment * genet * day_numeric + (1 | genet_tank) + ar1(day_factor + 0|treatment), data = all_behavior, family = binomial(), weights=n)

model_binomial_treat_genet_day <- glmmTMB(percent_fully_open ~ treatment * genet * day_numeric + (1 | genet_tank), data = all_behavior, family = binomial(), weights=n)

all_behavior$percentmin <- ifelse(all_behavior$percent_fully_open == 0, all_behavior$percent_fully_open + 0.001, 
                                  ifelse(all_behavior$percent_fully_open == 1, all_behavior$percent_fully_open - 0.001, all_behavior$percent_fully_open))

model_beta <- glmmTMB(percentmin ~ treatment * genet * day_numeric + (1|tank) + (1 | tank:genet) + ar1(day_factor + 0|treatment), data = all_behavior, family = beta_family(), weights=n)

BIC(model_binomial_genet, model_binomial_treat_genet, model_binomial_genet_ar, model_binomial_treat_ar, model_binomial_treat_genet_ar, model_binomial_treat_genet_ar2, model_binomial_treat_genet_ar3, model_binomial_treat_genet_day, model_beta)
```

## All the residual plots SUCK so let's just compare BICs of the beta models - because the data should be treated as beta, not binomial, distributed
```{r}
# 1. Treatment only + AR1
model_beta_treatment_ar1 <- glmmTMB(
  percentmin ~ treatment + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 2. Genet only + AR1
model_beta_genet_ar1 <- glmmTMB(
  percentmin ~ genet + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 3. Treatment + Genet + AR1
model_beta_treatment_plus_genet_ar1 <- glmmTMB(
  percentmin ~ treatment + genet + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 4. Treatment * Genet (interaction) + AR1
model_beta_treatment_times_genet_ar1 <- glmmTMB(
  percentmin ~ treatment * genet + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 5. Treatment only + day_numeric + AR1
model_beta_treatment_plus_day_ar1 <- glmmTMB(
  percentmin ~ treatment + day_numeric + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 6. Genet only + day_numeric + AR1
model_beta_genet_plus_day_ar1 <- glmmTMB(
  percentmin ~ genet + day_numeric + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 7. Treatment + Genet + day_numeric + AR1
model_beta_treatment_plus_genet_plus_day_ar1 <- glmmTMB(
  percentmin ~ treatment + genet + day_numeric + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 8. Treatment * Genet + day_numeric + AR1
model_beta_treatment_times_genet_plus_day_ar1 <- glmmTMB(
  percentmin ~ treatment * genet + day_numeric + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 9. Treatment + Genet * day_numeric + AR1
model_beta_treatment_plus_genet_times_day_ar1 <- glmmTMB(
  percentmin ~ treatment + genet * day_numeric + (1 | tank) + (1 | tank:genet) ,
  data = all_behavior,
  family = beta_family(),
  weights = n
)

# 10. Treatment * Genet * day_numeric + AR1 (your original fixed effects with AR1 in formula)
model_beta_full_ar1 <- glmmTMB(
  percentmin ~ treatment * genet * day_numeric + (1 | tank) + (1 | tank:genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

BIC_treatment_genet2 <- BIC(model_beta_treatment_ar1,model_beta_genet_ar1,model_beta_treatment_plus_genet_ar1,model_beta_treatment_times_genet_ar1,model_beta_treatment_plus_day_ar1,model_beta_genet_plus_day_ar1,model_beta_treatment_plus_genet_plus_day_ar1,model_beta_treatment_times_genet_plus_day_ar1,model_beta_treatment_plus_genet_times_day_ar1,model_beta_full_ar1)
```

```{r}
model_beta1 <- glmmTMB(
  percentmin ~ treatment + (1 | tank) + (1 | tank:genet) + ar1(day_factor + 0|tank+genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

model_beta2 <- glmmTMB(
  percentmin ~ genet * day_numeric + (1 | tank) + (1 | tank:genet) + ar1(day_factor + 0|tank+genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

model_beta3 <- glmmTMB(
  percentmin ~ treatment * genet + (1 | tank) + (1 | tank:genet) + ar1(day_factor + 0|tank+genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

model_beta4 <- glmmTMB(
  percentmin ~ treatment * genet + day_numeric + (1 | tank) + (1 | tank:genet) + ar1(day_factor + 0|tank+genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

BIC_check <- BIC(model_beta1, model_beta2, model_beta3, model_beta4)
```


## Inspect residuals versus fitted values for each grouping level of a random intercept factor
```{r, eval=FALSE}
library(DHARMa)
model_to_test <- model_beta

testDispersion(simulateResiduals(model_to_test))
summary(model_to_test)

# Create simulated residuals
sim_res <- simulateResiduals(model_to_test)
plot(sim_res)

# Extract fitted values corresponding to the model's data
fitted_model_data <- fitted(model_to_test)

# Get the grouping factor corresponding to the model's data
grouping_factor_model_data <- model_to_test$frame$tank

# Check the lengths to confirm the issue
length(sim_res$scaledResiduals)
length(fitted_model_data)
length(grouping_factor_model_data)

# Create a data frame for plotting using the model's data
plot_data <- data.frame(
  scaledResiduals = sim_res$scaledResiduals,
  fitted.values = fitted_model_data,
  genet_tank = grouping_factor_model_data
)

ggplot(plot_data, aes(x = fitted.values, y = scaledResiduals, color = genet_tank)) +
  geom_point(alpha = 0.6) + # Add some transparency if there's overlap
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuals vs. Fitted Values by genet_tank",
       x = "Fitted Values",
       y = "Scaled Residuals",
       color = "genet_tank") +
  theme_bw() +
  theme(legend.position = "right", # Adjust legend position as needed
        legend.text = element_text(size = 6)) 

fitted(model_to_test)

hist(all_behavior$percentmin, main="Histogram of close to 1")
hist(fitted_values, main = "Histogram of Fitted Probabilities")
plot(fitted_values, all_behavior$percent_fully_open, xlab = "Fitted Probability", ylab = "Observed Percent Open")
abline(0, 1, col = "red")

```

## Model 10 is consistently the best BIC. Let's check each ar1() option then.
```{r}
model_0 <- glmmTMB(
  percentmin ~ treatment * genet * day_numeric + (1 | tank),
  data = all_behavior,
  family = beta_family(),
  weights = n
)
model_1 <- glmmTMB(
  percentmin ~ treatment * genet * day_numeric + (1 | tank:genet) + ar1(day_factor + 0|tank+genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)
model_2 <- glmmTMB(
  percentmin ~ treatment * genet * day_numeric + (1 | tank) + (1 | tank:genet) + ar1(day_factor + 0|tank+genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)
model_3 <- glmmTMB(
  percentmin ~ treatment * genet * day_numeric + (1 | tank) + (1 | tank:genet) + ar1(day_factor + 0|tank+genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)
model_4 <- glmmTMB(
  percentmin ~ treatment * genet * day_numeric + (1 | tank) + (1 | tank:genet) + ar1(day_factor + 0|tank+genet),
  data = all_behavior,
  family = beta_family(),
  weights = n
)

BIC_compare <- BIC(model_0, model_1, model_2, model_3, model_4)

summary(model_4)
```

Woof. The most complex model has by far the best BIC... 

# well, here's the best model: 
percentmin ~ treatment * genet * day_numeric + (1 | tank) + (1 | tank:genet) + ar1(day_factor + 0 | tank+genet), family = beta_family(), weights = n)

    Variance and Std.Dev.: These show the estimated variance and standard deviation of the random intercepts for each grouping factor.
        tank (Intercept): The variance is very small (1.011e-08), suggesting very little variability in the baseline log-odds of the behavior across different tanks after accounting for other factors.
        tank:genet (Intercept): The variance is larger (6.657e-02), indicating more substantial variability in the baseline log-odds of the behavior for different genotypes within the same tank. This suggests that the effect of genotype can vary across tanks.
        tank.1 day_factor0 and genet day_factor0: These relate to the AR(1) correlation structure. The variances (0.4961 and 0.1553) and standard deviations (0.7043 and 0.3940) indicate the magnitude of the random variation associated with the AR(1) process. The correlation (-0.26) between consecutive day_factor levels within tank and the correlation (0.33) within genet suggest a moderate level of autocorrelation. The negative correlation within tank implies that if the residual on one day is positive, the residual on the next day tends to be negative, and vice versa. The positive correlation within genet suggests that residuals on consecutive days for a given genotype tend to have the same sign.
    Number of obs: The total number of observations in your dataset.
    groups: The number of levels for each random grouping factor.

3. Dispersion Parameter:

    Dispersion parameter for beta family (): 15: For a beta distribution, the variance is related to the mean and a dispersion parameter. A dispersion parameter significantly different from 1 might indicate over- or under-dispersion in your data relative to a standard beta distribution. Here, 15 is considerably larger than 1, suggesting overdispersion. This means that the variance in your percent_behavior is greater than what would be expected based solely on the mean and the beta distribution.

4. Conditional Model (Fixed Effects):

This table presents the estimates, standard errors, z-values, and p-values for the fixed effect coefficients. Remember that because you used a logit link, these coefficients are on the log-odds scale. To interpret the effects on the original probability scale, you would need to exponentiate them (and consider the interactions carefully).

    (Intercept): This is the estimated log-odds of the percent_behavior for the baseline levels of treatment and genet when day_numeric is 0. The significant p-value (8.69e-14 ***) indicates that this baseline log-odds is significantly different from zero.
    treatmentsevere and treatmentextreme: These coefficients represent the difference in log-odds of the behavior for the severe and extreme treatments compared to the baseline treatment (holding other variables at their reference levels or zero). Both are negative and highly significant, suggesting that both severe and extreme treatments are associated with a decrease in the log-odds (and thus the probability) of the behavior compared to the baseline treatment.
    genetB, genetC, genetD, genetE: These coefficients represent the difference in log-odds of the behavior for each of these genotypes compared to the baseline genotype. genetB, genetC, and genetE have negative and significant coefficients, indicating a lower log-odds of the behavior compared to the baseline genotype. genetD's coefficient is not significant, suggesting no significant difference from the baseline genotype.
    day_numeric: The coefficient is small and not significant, suggesting that overall, there is no strong linear relationship between day_numeric and the log-odds of the behavior across all treatments and genotypes at their baseline levels.
    Interaction Terms (e.g., treatmentsevere:genetB): These coefficients represent the additional effect of the interaction between the two variables on the log-odds of the behavior, beyond their individual main effects. For example, treatmentsevere:genetB (0.548145, p = 0.021071 *) indicates that the effect of the severe treatment on the log-odds of the behavior is different for genetB compared to the baseline genotype. A positive coefficient here suggests that the decrease in log-odds associated with the severe treatment is less pronounced for genetB.
    Three-way Interaction Terms (e.g., treatmentsevere:genetB:day_numeric): These coefficients indicate how the two-way interaction between treatment and genet changes with day_numeric. For example, treatmentsevere:genetB:day_numeric (-0.017571, p < 2e-16 ***) suggests that the difference in the effect of the severe treatment between genetB and the baseline genotype changes significantly with each unit increase in day_numeric. The negative coefficient implies that the mitigating effect (as seen in the positive two-way interaction) of genetB on the severe treatment decreases as day_numeric increases.
 
# Let's run a PERMANOVA of everything (like Minuti et al 2021)

## Prep PERMANOVA data
```{r}
all_permanova <- all %>%
  filter(date > ymd("2023-10-06"), #remove 10-06 to make # days between mhw/recovery equivalent
         !is.na(avg_size)) %>%
  mutate(day_numeric = as.numeric(as.Date(date)-min(as.Date(date))),
         avg_size_log = log(avg_size),
         mhw = ifelse(date <= ymd("2023-12-12"), "during", "after")) %>%
  dplyr::select(day_numeric, date, tank, genet, treatment, mhw, avg_size, avg_size_log, percent_open, percent_fully_open, percent_closed, n, n_true, growth_rate) %>%
  mutate(mhw_treatment = interaction(mhw, treatment),
         mhw_genet = interaction(mhw, genet),
         treatment_genet = interaction(treatment, genet))

#response_matrix <- all_permanova[, c("percent_open", "n_true", "avg_size")]
#distance_matrix <- daisy(response_matrix, metric = "gower")

# check homogeneity of mutlivariate dispersion
#dispersion <- betadisper(distance_matrix, group = all_permanova$genet)
#anova(dispersion) #A significant p-value indicates that at least one group has a different dispersion than others.
#permutest(dispersion, pairwise = TRUE) # If the overall ANOVA is significant, this performs pairwise permutation tests to see which specific groups have significantly different dispersions

hist(all_permanova$percent_open)
hist(all_permanova$n_true)
hist(all_permanova$avg_size_log)

response_matrix <- all_permanova[, c("percent_open", "n_true", "avg_size_log")]
distance_matrix <- daisy(response_matrix, metric = "gower")

# check homogeneity of mutlivariate dispersion
dispersion <- betadisper(distance_matrix, group = all_permanova$treatment_genet)
anova(dispersion) #A significant p-value indicates that at least one group has a different dispersion than others.
#we need to log-transform body size dats because there is no statistically diff dispersion between GENETS 
permutest(dispersion, pairwise = TRUE) # If the overall ANOVA is significant, this performs pairwise permutation tests to see which specific groups have significantly different dispersions
boxplot(dispersion)
```

## Let's start "simple" and look at timepoint-by-treatment differences
```{r}
permanova_mhw_treatment <- adonis(distance_matrix ~ mhw_treatment,
                                   data = all_permanova,
                                   permutations = 999)
#print(permanova_mhw_treatment)

pairwise_comparisons <- pairwise.adonis(distance_matrix, factors = all_permanova$mhw_treatment, p.adjust.m = "bonferroni")
pairwise_comparisons
```


The heatwave treatments (severe and extreme) during the heatwave were significantly different from the ambient conditions during the heatwave.
The conditions after the heatwave (ambient, severe, and extreme) were significantly different from the ambient conditions during the heatwave.
There were also significant differences between some of the "during" heatwave conditions and the "after" heatwave conditions within the same treatment level (e.g., during.cold vs. after.cold).

There was A LOT (80%) of unexplained variation in this simple model though. Perhaps genet explains a lot of that? 

## Let's now look at timepoint-by-genet differences
```{r}
permanova_mhw_genet <- adonis(distance_matrix ~ mhw_genet,
                               data = all_permanova,
                               permutations = 999)
#print(permanova_mhw_genet)

pairwise_comparisons <- pairwise.adonis(distance_matrix, factors = all_permanova$mhw_genet, p.adjust.m = "bonferroni")
pairwise_comparisons
```

Every single genet combo is significant... AND there was a lot less (54%) unexplained variation! So gosh darn it, let's do the most complex model. 

## The "most complex" PERMANOVA
```{r}
permanova_mhw_treatment_genet <- adonis(distance_matrix ~ mhw * treatment * genet,
                                  data = all_permanova,
                                  permutations = 999)
#print(permanova_mhw_treatment_genet)

all_permanova$mhw_treatment_genet <- interaction(all_permanova$mhw, all_permanova$treatment, all_permanova$genet)
pairwise_mhw_treatment_genet <- pairwise.adonis(distance_matrix, factors = all_permanova$mhw_treatment_genet, p.adjust.m = "bonferroni")
print(pairwise_mhw_treatment_genet)
```

# Hmm that has very similar (48%) residual variation as genet only. Let's try a combo treatment_mhw * genet model? 
```{r}
permanova_mhw_treatment_genet2 <- adonis(distance_matrix ~ mhw_treatment * genet,
                                   data = all_permanova,
                                   permutations = 999)

print(permanova_mhw_treatment_genet2)
```
This actually gives us the same results. So I think it's the same as mhw * treamtment * genet

There's still 48% of residual variation, let's see if the progression of time (days into the MHW/recovery) could explain some of it better?
Keep
```{r}
all_permanova$relative_day <- NA

# Assign relative days for each phase
for (phase in unique(all_permanova$mhw)) {
  idx <- all_permanova$mhw == phase
  min_day <- min(all_permanova$day_numeric[idx], na.rm = TRUE)
  all_permanova$relative_day[idx] <- all_permanova$day_numeric[idx] - min_day
}

permanova_mhw_genet_day <- adonis(distance_matrix ~ mhw_genet + relative_day, data = all_permanova, permutations = 999)
#print(permanova_mhw_genet_day) #residual R2 is 46%

permanova_mhw_treatment_genet_day <- adonis(distance_matrix ~ mhw * treatment * genet + relative_day, data = all_permanova, permutations = 999)
#print(permanova_mhw_treatment_genet_day) #residual R2 is 40%
permanova_mhw_treatment_genet_day$aov.tab

#Because I don't think the response is going to be linear over time (re: relative day), let's explore a quadratic term for relative_day
#permanova_mhw_treatment_genet_nonlinear_day <- adonis(distance_matrix ~ mhw * treatment * genet + relative_day + I(relative_day^2), data = all_permanova, permutations = 999)
#print(permanova_mhw_treatment_genet_nonlinear_day)
#permanova_mhw_treatment_genet_nonlinear_day$aov.tab # This removes less than 1% from the residual R2... not worth it
```

# Let's visualize distance matrix with ordination plot
```{r}
response_matrix <- all_permanova[, c("percent_open", "n_true", "avg_size_log")]
distance_matrix <- daisy(response_matrix, metric = "gower")

pcoa_result <- pcoa(distance_matrix)
pcoa_scores <- as.data.frame(pcoa_result$vectors)
colnames(pcoa_scores) <- paste0("PCoA", 1:ncol(pcoa_scores))

# Combine the PCoA scores with your all_permanova data
plot_data <- cbind(all_permanova, pcoa_scores)

# Determine the percentage of variance explained by the first two axes
eigenvalues <- pcoa_result$values$Eigenvalues
percent_explained_axis1 <- round((eigenvalues[1] / sum(eigenvalues)) * 100, 1)
percent_explained_axis2 <- round((eigenvalues[2] / sum(eigenvalues)) * 100, 1)
```

# Plot the PCOAs
```{r}
ggplot(plot_data, aes(x = PCoA1, y = PCoA2, color = mhw)) +
  geom_point(size = 3) +
    stat_ellipse(geom = "polygon", alpha = 0.2, aes(fill = mhw), level = 0.95) + # 95% confidence ellipse
  labs(
    title = "PCoA Plot Colored by MHW Period",
    x = paste0("PCoA1 (", percent_explained_axis1, "% explained)"),
    y = paste0("PCoA2 (", percent_explained_axis2, "% explained)"),
    color = "MHW Period"
  ) +
  theme_bw()

ggplot(plot_data, aes(x = PCoA1, y = PCoA2, color = treatment)) +
  geom_point(size = 3) +
    stat_ellipse(geom = "polygon", alpha = 0.2, aes(fill = treatment), level = 0.95) + # 95% confidence ellipse
  labs(
    title = "PCoA Plot Colored by Treatment",
    x = paste0("PCoA1 (", percent_explained_axis1, "% explained)"),
    y = paste0("PCoA2 (", percent_explained_axis2, "% explained)"),
    color = "Treatment"
  ) +
  theme_bw()

ggplot(plot_data, aes(x = PCoA1, y = PCoA2, shape = genet, color = genet)) +
  geom_point(size = 3) +
  stat_ellipse(geom = "polygon", alpha = 0.2, aes(fill = genet), level = 0.95) + # 95% confidence ellipse
  labs(
    title = "PCoA Plot Shaped and Colored by Genet",
    x = paste0("PCoA1 (", percent_explained_axis1, "% explained)"),
    y = paste0("PCoA2 (", percent_explained_axis2, "% explained)"),
    shape = "Genet",
    color = "Genet"
  ) +
  theme_bw()

ggplot(plot_data, aes(x = PCoA1, y = PCoA2, color = mhw_treatment)) +
  geom_point(size = 3) +
  stat_ellipse(geom = "polygon", alpha = 0.2, aes(fill = mhw_treatment), level = 0.95) + # 95% confidence ellipse
  labs(
    title = "PCoA Plot Colored by MHW and Treatment Interaction",
    x = paste0("PCoA1 (", percent_explained_axis1, "% explained)"),
    y = paste0("PCoA2 (", percent_explained_axis2, "% explained)"),
    color = "MHW x Treatment"
  ) +
  theme_bw()
```

Measured variables (especially population growth and body size) are strongly autocorrelated which is no-bueno for PERMANOVA so let's super simplify the data.
6 time points: start/mid/end of MHW, start/mid/end of recovery
```{r}
all_simple <- all %>%
  filter(date >= ymd("2023-10-06"), #remove 10-06 to make # days between mhw/recovery equivalent
         !is.na(avg_size)) %>%
  mutate(day_numeric = as.numeric(as.Date(date)-min(as.Date(date))),
         avg_size_log = log(avg_size),
         mhw = ifelse(date <= ymd("2023-12-12"), "during", "after")) %>%
  dplyr::select(day_numeric, date, tank, genet, treatment, mhw, avg_size, avg_size_log, percent_open, percent_fully_open, percent_closed, n, n_true, growth_rate) %>%
  mutate(mhw_treatment = interaction(mhw, treatment),
         mhw_genet = interaction(mhw, genet),
         treatment_genet = interaction(treatment, genet)) %>%
  group_by(mhw) %>%
  mutate(week = as.numeric(as.Date(date)-min(as.Date(date)))/7,
         relative_day = day_numeric - min(day_numeric, na.rm = TRUE)) %>%
  ungroup() %>%
  filter(case_when(mhw=="during" ~ week %in% c(0, 5, 9),
                   mhw=="after" ~ week %in% c(0, 4, 8))) %>%
  mutate(mhw_week = interaction(mhw, week))

hist(all_simple$percent_open)
hist(all_simple$n_true)
hist(all_simple$avg_size_log)

response_matrix <- all_simple[, c("percent_open", "n_true", "avg_size_log")]
distance_matrix <- daisy(response_matrix, metric = "gower")

# check homogeneity of mutlivariate dispersion
dispersion <- betadisper(distance_matrix, group = all_simple$mhw_week)
anova(dispersion) #A significant p-value indicates that at least one group has a different dispersion than others.
permutest(dispersion, pairwise = TRUE) # If the overall ANOVA is significant, this performs pairwise permutation tests to see which specific groups have significantly different dispersions
boxplot(dispersion)
```

## Let's merge "time into the mhw" with "mhw" 
```{r}
permanova_mhw <- adonis(distance_matrix ~ mhw_week,
                        data = all_simple,
                        permutations = 999)
permanova_mhw$aov.tab #R2 = 0.56
#pairwise_comparisons <- pairwise.adonis(distance_matrix, factors = all_simple$mhw, p.adjust.m = "bonferroni")
#pairwise_comparisons

permanova_treatment <- adonis(distance_matrix ~ treatment,
                               data = all_simple,
                               permutations = 999)
permanova_treatment$aov.tab #R2 = 0.98
#pairwise_comparisons <- pairwise.adonis(distance_matrix, factors = all_simple$mhw_genet, p.adjust.m = "bonferroni")
#pairwise_comparisons

permanova_genet <- adonis(distance_matrix ~ genet,
                                  data = all_simple,
                                  permutations = 999)
permanova_genet$aov.tab #R2 = 0.74

permanova_mhw_genet <- adonis(distance_matrix ~ mhw_week*genet,
                                  data = all_simple,
                                  permutations = 999)
permanova_mhw_genet$aov.tab #R2 = 0.28

permanova_mhw_treatment <- adonis(distance_matrix ~ mhw_week*treatment,
                                  data = all_simple,
                                  permutations = 999)
permanova_mhw_treatment$aov.tab #R2 = 0.54

permanova_mhw_treatment_genet <- adonis(distance_matrix ~ mhw_week*treatment*genet,
                                  data = all_simple,
                                  permutations = 999)
permanova_mhw_treatment_genet$aov.tab #R2 = 0.22

permanova_mhw_treatment_genet2 <- adonis(distance_matrix ~ mhw_week+treatment+genet,
                                  data = all_simple,
                                  permutations = 999)
permanova_mhw_treatment_genet2$aov.tab #R2 = 0.28

permanova_mhw_treatment_genet3 <- adonis(distance_matrix ~ mhw_week*treatment+genet,
                                  data = all_simple,
                                  permutations = 999)
permanova_mhw_treatment_genet3$aov.tab #R2 = 0.27

permanova_mhw_treatment_genet4 <- adonis(distance_matrix ~ mhw_week+treatment*genet,
                                  data = all_simple,
                                  permutations = 999)
permanova_mhw_treatment_genet4$aov.tab #R2 = 0.26

permanova_mhw_treatment_genet5 <- adonis(distance_matrix ~ mhw_week*genet+treatment,
                                  data = all_simple,
                                  permutations = 999)
permanova_mhw_treatment_genet5$aov.tab #R2 = 0.26

#pairwise_mhw_treatment_genet <- pairwise.adonis(distance_matrix, factors = all_simple$mhw_treatment_genet, p.adjust.m = "bonferroni")
#print(pairwise_mhw_treatment_genet)
```

# Calculate adjusted R-squared to compare models
```{r}
calculate_adjusted_r_squared <- function(adonis_model) {
  n_observations <- 450 # Setting n_observations directly to 450
  r_squared <- 1-adonis_model$aov.tab["Residuals", "R2"] # Get Model R-squared
  df_model <- sum(adonis_model$aov.tab[-nrow(adonis_model$aov.tab), "Df"]) # Total Df of model terms
  df_residuals <- adonis_model$aov.tab["Residuals", "Df"] # Df of residuals

  adjusted_r_squared <- 1 - ((1 - r_squared) * (n_observations - 1) / df_residuals)
  return(adjusted_r_squared)
}
```

## Calculate adjusted R2
```{r}
calculate_adjusted_r_squared(permanova_mhw_treatment_genet)
permanova_mhw_treatment_genet$aov.tab

calculate_adjusted_r_squared(permanova_mhw_treatment_genet4)
permanova_mhw_treatment_genet4$aov.tab

#model0 <- adonis2(distance_matrix ~ mhw_week*genet,
#                                  data = all_simple,
#                                  permutations = 999)
#model1 <- adonis2(distance_matrix ~ mhw_week*treatment*genet,
#                                  data = all_simple,
#                                  permutations = 999)
model2 <- adonis2(distance_matrix ~ mhw_week+treatment*genet,
                                  data = all_simple,
                                  permutations = 999)

#model3 <- adonis2(distance_matrix ~ mhw_week*genet+treatment,
#                                  data = all_simple,
#                                  permutations = 999)

#model4 <- adonis2(distance_matrix ~ mhw_week+treatment+genet,
#                                  data = all_simple,
#                                  permutations = 999)

permanova_mhw_treatment_genet4$aov.tab #mhw + treatment*genet
permanova_mhw_treatment_genet5$aov.tab #mhw*genet + treatment 
permanova_mhw_treatment_genet2$aov.tab #mhw + treatment + genet

pairwise_comparisons <- pairwise.adonis(distance_matrix, factors = all_simple$genet, p.adjust.m = "bonferroni")
print(pairwise_comparisons)

model2 
model3
model4

AICperm0 <- AICc_permanova2(model0)
AICperm1 <- AICc_permanova2(model1)
AICperm2 <- AICc_permanova2(model2)
AICperm3 <- AICc_permanova2(model3)
AICperm4 <- AICc_permanova2(model4)

AIC_table <- data.frame(
  Model = c("model0", "model1", "model2", "model3", "model4"),
  AICc = c(AICperm0$AICc, AICperm1$AICc, AICperm2$AICc, AICperm3$AICc, AICperm4$AICc)
)
```

## Let's try "cheating" with the AICcPermanova package
```{r, eval=FALSE}
AllModels <- make_models(vars = c("mhw_week", "treatment", "genet", "week", "mhw"))
NonColinear <- filter_vif(all_forms = AllModels, env_data = all_simple)

distance_data <- all_simple %>%
  select("percent_open", "n_true", "avg_size_log")

Fitted <- fit_models(
  all_forms = NonColinear,
  veg_data = distance_data,
  env_data = all_simple,
  ncores = 4,
  method = "gower"
)
```

```{r}
AICc.PERMANOVA2 <- function(adonis2.model) {
    
    # check to see if object is an adonis2 model...
    
    if (is.na(adonis2.model$SumOfSqs[1]))
        stop("object not output of adonis2 {vegan} ")
    
    # Ok, now extract appropriate terms from the adonis model Calculating AICc
    # using residual sum of squares (RSS or SSE) since I don't think that adonis
    # returns something I can use as a likelihood function... maximum likelihood
    # and MSE estimates are the same when distribution is gaussian See e.g.
    # https://www.jessicayung.com/mse-as-maximum-likelihood/;
    # https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1
    # So using RSS or MSE estimates is fine as long as the residuals are
    # Gaussian https://robjhyndman.com/hyndsight/aic/ If models have different
    # conditional likelihoods then AIC is not valid. However, comparing models
    # with different error distributions is ok (above link).
    
    
    RSS <- adonis2.model$SumOfSqs[ length(adonis2.model$SumOfSqs) - 1 ]
    MSE <- RSS / adonis2.model$Df[ length(adonis2.model$Df) - 1 ]
    
    nn <- adonis2.model$Df[ length(adonis2.model$Df) ] + 1
    
    k <- nn - adonis2.model$Df[ length(adonis2.model$Df) - 1 ]
    
    AIC <- 2*k + nn*log(RSS/nn)
    AIC.g <- 2*k + nn * (1 + log( 2 * pi * RSS / nn))
    AIC.MSE <- 2*k + nn * log(MSE)
    AIC.pi <- k + nn*(1 + log( 2*pi*RSS/(nn-k) )   )
    AICc <- AIC + (2*k*(k + 1))/(nn - k - 1)
    AICc.MSE <- AIC.MSE + (2*k*(k + 1))/(nn - k - 1)
    AICc.pi <- AIC.pi + (2*k*(k + 1))/(nn - k - 1)
    
    output <- list("AIC" = AIC, "AICc" = AICc, "AIC.g" = AIC.g, 
                   "AIC.MSE" = AIC.MSE, "AICc.MSE" = AICc.MSE,
                   "AIC.pi" = AIC.pi, "AICc.pi" = AICc.pi, "k" = k, "N" = nn)
    
    return(output)   
    
}
```


```{r}
AIC1 <- AICc.PERMANOVA2(model1)
AIC2 <- AICc.PERMANOVA2(model2)
```

## Let's run some post-hoc on our "final" model

### pairwise comparisons
```{r}
library(pairwiseAdonis)
model_final <- adonis2(distance_matrix ~ mhw_week+treatment*genet,
                       data = all_simple,
                       permutations = 999)

pairwise_results <- pairwise.adonis(
  x = distance_matrix,
  factors = all_simple$treatment_genet,
  sim.method = "gower", # Or the dissimilarity method you used for your distance_matrix
  p.adjust.m = "BH"     # Benjamini-Hochberg FDR correction (recommended)
)

pair_sorted <- pairwise_results %>%
  arrange(p.adjusted) %>%
  mutate(row_number = row_number())

within_treatment_comparisons <- pairwise_results %>%
  filter(
    str_remove(str_split_i(pairs, " vs ", 1), "\\.[A-Z]$") ==
    str_remove(str_split_i(pairs, " vs ", 2), "\\.[A-Z]$")
  )
```

### ANOSIM
```{r}
anosim_response <- anosim(distance_matrix, grouping = all_simple$treatment_genet, permutations = 999)
print(anosim_response)
summary(anosim_response)
```


# DONE. 


### Let's look at "final" GLMM options with corARMA
```{r, eval=FALSE}
count_glmm14 <- lme(n_true ~ treatment*genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 4, q = 1))

count_glmm16 <- lme(n_true ~ treatment*genet + mhw,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 4, q = 1))

count_glmm17 <- lme(n_true ~ treatment*genet + mhw:treatment,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 4, q = 1),
                   control = lmeControl(maxIter = 100, msMaxIter = 100))
summary(count_glmm14)
summary(count_glmm16)
summary(count_glmm17)
AIC(count_glmm14, count_glmm16, count_glmm17) 
BIC(count_glmm14, count_glmm16, count_glmm17)
```
Adding in the "state" of the marine heatwave makes things worse all around / mhw isn't significant. Which I guess makes sense since even in the control the curves flattened out?


### Let's plot something up for the GLMM
```{r}
all_count_plot <- all_count %>%
  dplyr::select(date, tank, genet, genet_tank, treatment, n_true) %>%
  group_by(date, treatment, genet) %>%
  summarize(mean = mean(n_true),
            sd = sd(n_true),
            se = sd(n_true)/sqrt(length(n_true)),
            ci = qt(0.95, length(n_true)-1)*se,
            lower = mean - ci,
            upper = mean + ci) %>%
  dplyr::select(date, treatment, genet, mean, sd, lower, upper)


ggplot(all_count_plot, aes(x=date,color=genet)) +
  geom_point(aes(y=mean)) +
  #geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +  # Error bars
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = genet), alpha = 0.2, color = NA) +  # Cloud effect
  facet_wrap(~treatment) +
  labs(x = "Date",
       y = "Count")

# Calculate residuals
all_count$residuals <- residuals(count_glmm14)

# Plot residuals
ggplot(all_count, aes(x = predicted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Predicted Values", y = "Residuals") +
  theme_minimal()

ggplot(all_count, aes(x = treatment, y = n_true, color = genet)) +
  geom_point(position = position_jitter(width = 0.2)) +
  stat_summary(fun = mean, geom = "line", aes(group = genet)) +
  labs(x = "Treatment", y = "n_true") +
  theme_minimal()
```

#### We might need a nonlinear regression, the curves are logistic
```{r, eval=FALSE}
glmer1 <- glmer.nb(n_true ~ treatment*genet + (1|genet_tank), 
                data = all_count)

summary(glmer1)
#Well that is terrible.
```

### Check dispersion
```{r}
all_count_dispersion <- all_count %>%
  group_by(genet_tank) %>%
  summarize(mean = mean(n_true),
            var = var(n_true),
            ratio = var/mean)

all_count_dispersion_genet <- all_count %>%
  group_by(genet) %>%
  summarize(mean = mean(n_true),
            var = var(n_true),
            ratio = var/mean)
```
Actually, if the data is organized by individual groupings (genet_tank OR genet only) it's not actually that overdispersed. Some of them are actually underdispersed. Let's keep this in mind for model interpretation. 

## Plot up count data for WSN 
### Treatment not significant
```{r}
all_count_treatment <- all_count %>%
  dplyr::select(date, tank, genet, genet_tank, treatment, n_true) %>%
  group_by(date, treatment) %>%
  summarize(mean = mean(n_true),
            sd = sd(n_true),
            se = sd(n_true)/sqrt(length(n_true)),
            ci = qt(0.95, length(n_true)-1)*se,
            lower = mean - ci,
            upper = mean + ci) %>%
  dplyr::select(date, treatment, mean, sd, lower, upper)

image <- ggplot(all_count_treatment, aes(x=date,color=treatment)) +
  geom_vline(xintercept = as.numeric(as.POSIXct("2023-12-12")), linetype = "dashed", color = "black", alpha = 0.8) +
  geom_smooth(aes(y=mean)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = treatment), alpha = 0.2, color = NA) +  # Cloud effect
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +  
  scale_fill_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +
  scale_x_datetime(date_labels = "%b %d", 
                   breaks= seq(min(all_count_treatment$date), max(all_count_treatment$date), length=7)) +
  labs(x = "Date",
       y = "Number of polyps",
       color = "") +
  guides(fill = "none",
         color=guide_legend(override.aes=list(fill=NA))) +
  theme_bw() +
  theme(text = element_text(size = 20),
        legend.position = "top")

#ggsave(file = here("experiment", "figures", "WSN","count_mhw.svg"), plot = image, width=12, height=7)
```


```{r}
ggplot(all_count_plot, aes(x=date,color=treatment)) +
  geom_vline(xintercept = as.numeric(as.POSIXct("2023-12-12")), linetype = "dashed", color = "black", alpha = 0.8) +
  geom_smooth(aes(y=mean), span = 0.5) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = genet), alpha = 0.2, color = NA) +  # Cloud effect
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Control", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_x_datetime(date_labels = "%b %d", 
                   breaks= seq(min(all_count_plot$date), max(all_count_plot$date), length=7)) +
  labs(x = "Date",
       y = "Number of polyps",
       color = "Genet") +
  guides(fill = "none",
         color=guide_legend(override.aes=list(fill=NA))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 20),
        legend.position = "top")

#ggsave(file = here("experiment", "figures", "WSN","count_treatment_genet.svg"), plot = image, width=12, height=7)

image <- ggplot(all_count_plot, aes(x=date,color=treatment)) +
  geom_vline(xintercept = as.numeric(as.POSIXct("2023-12-12")), linetype = "dashed", color = "black", alpha = 0.8) +
  geom_smooth(aes(y=mean), span = 0.5) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = treatment), alpha = 0.2, color = NA) +  # Cloud effect
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +  
  scale_fill_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +
  facet_wrap(~genet,
             scales = "free_y",
             labeller =  labeller(treatment = c("cold" = "Control", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_x_datetime(date_labels = "%b %d", 
                   breaks= seq(min(all_count_plot$date), max(all_count_plot$date), length=7)) +
  labs(x = "Date",
       y = "Number of polyps",
       color = "") +
  guides(fill = "none",
         color=guide_legend(override.aes=list(fill=NA))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 20),
        legend.position = "top")

#ggsave(file = here("experiment", "figures", "WSN","count_genet_treatment.svg"), plot = image, width=12, height=7)
```


```{r}
ggplot(all_count, aes(x = treatment, y = n_true, color = genet)) +
  geom_point(position = position_jitter(width = 0.2)) +
  stat_summary(fun = mean, geom = "line", aes(group = genet)) +
  labs(x = "Treatment", y = "n_true") +
  theme_minimal()
```


## Body size data - ARIMA forecasting
https://stats.stackexchange.com/questions/281666/how-does-acf-pacf-identify-the-order-of-ma-and-ar-terms
https://stats.stackexchange.com/questions/32634/difference-time-series-before-arima-or-within-arima

### Log transform data
```{r}
# scatter plot of transformed data
ggplot(all_size, aes(x=date, y=avg_size_log)) +
  geom_point(aes(color=treatment)) +
  labs(x = "Average body size (mm)",
       y = "Frequency")
# histogram of transformed data
ggplot(all_size, aes(x=avg_size_log)) +
  geom_histogram(binwidth=0.1) +
  labs(x = "log(Average body size (mm))",
       y = "Frequency")
# transformed data density plot
ggplot(all_size, aes(x = avg_size_log)) +
  geom_density() +
  ggtitle("Density Plot of Transformed Data")
# Q-Q plot of log-transformed data
qqnorm(all_size$avg_size_log)
qqline(all_size$avg_size_log, col = "red")
```
It's not perfectly normal but definitely better log-transformed. Looks a bit like exponential decay, so let's make the data stationary by differencing.

### Difference to make the data stationary
```{r}
ggplot(all_size, aes(x=date)) +
  geom_point(aes(y=avg_size_diff), color="black", alpha=0.3) +
  geom_point(aes(y=avg_size_diff2), color="blue", alpha=0.3) +
  geom_smooth(aes(y=avg_size_diff), method="lm", color="black") +
  geom_smooth(aes(y=avg_size_diff2), method="lm", color="blue") +
  labs(x = "Average body size (mm)",
       y = "Frequency")
```

### Plot a slightly more complex vis of the first differenced data
```{r, fig.width=10}
# Fit linear models and extract coefficients
models <- all_size %>%
  group_by(treatment, genet, mhw) %>%
  do(tidy(lm(avg_size_diff ~ date, data = .))) %>%
  select(treatment, genet, term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  mutate(equation = sprintf("y = %.2f + %.2e * x", `(Intercept)`, date)) %>%
  rename(slope = date) %>%
  ungroup() %>%
  group_by(mhw) %>%
  mutate(x_axis = as.POSIXct(ifelse(mhw == "during", "2023-10-15", "2024-01-15")),
         y_axis = rep(seq(from = 0.2, to = 0.5, length.out = 5), length.out = n())) %>%
  ungroup() %>%
  select(-mhw)

# Join regression equations with the original data
all_size_reg <- all_size %>%
  left_join(models, by = c("treatment", "genet"), relationship = "many-to-many")

ggplot(all_size_reg) +
  geom_point(aes(x=date, y=avg_size_diff, color=genet), alpha=0.3) +
  geom_smooth(aes(x=date, y=avg_size_diff, color=genet ,linetype = mhw), method="lm") +
  facet_wrap(~treatment) +
  geom_label_repel(data = models, aes(x=x_axis, y=y_axis, label = equation, color = genet), size = 3) +
  labs(x = "Date",
       y = "Differenced average body size (mm)") +
  theme_bw()

#ggsave(here("experiment", "figures", "body_size_regression.png"), width=15, height=7)
```

### Run ADF and KPSS tests on first and second differenced data
```{r}
# List to store results
adf_results <- list()

# Loop through each group
for (group in unique(all_size$treatment)) {
  group_data <- all_size %>%
    filter(treatment == group,
           !is.na(avg_size_diff2)) %>%
    pull(avg_size_diff2) #chnage this to avg_size_diff for first differenced data
  # Perform the ADF test
  adf_test <- adf.test(group_data, alternative = "stationary")
  # Store the result
  adf_results[[group]] <- adf_test
}

# Print results
adf_results

# List to store results
kpss_results <- list()
# Loop through each group
for (group in unique(all_size$treatment)) {
  group_data <- all_size %>%
    filter(treatment == group,
           !is.na(avg_size_diff2)) %>%
    pull(avg_size_diff2) #chnage this to avg_size_diff for first differenced data
  # Perform the KPSS test
  kpss_test <- ur.kpss(group_data, type = "mu")  # Use "tau" if you expect trend stationarity
  # Store the result
  kpss_results[[group]] <- summary(kpss_test)
}

# Print results
kpss_results
```
Both p-value is less than 0.01 and test statistic is lower than critical values, so second-order differenced data is now almost certainly stationary. First-order differencing only showed strong evidence for stationarity with the ADF test.

### Plot ACF and PACF for the log-transformed body size data
```{r}
Acf(all_size$avg_size_log, main = "ACF of Log-transformed Series")
Pacf(all_size$avg_size_log, main = "PACF of Log-transformed Series")

Acf(all_size$avg_size_diff, main = "ACF of Differenced Series")
Pacf(all_size$avg_size_diff, main = "PACF of Differenced Series")

Acf(all_size$avg_size_diff2, main = "ACF of 2-Differenced Series")
Pacf(all_size$avg_size_diff2, main = "PACF of 2-Differenced Series")
```
There is definitely a lot of autocorrelation with the data, for both 1-diff and 2-diff. Makes sense because body size should be highly correlated over time.

### Remove NA values and convert factors to numeric
```{r}
all_size_na <- all_size %>%
  filter(!is.na(avg_size_diff)) %>%
  mutate(genet = as.numeric(genet),
         treatment = as.numeric(treatment),
         mhw = as.numeric(mhw)) %>%
  select(date, tank, genet, n, treatment, mhw, avg_size, avg_size_log, avg_size_diff, avg_size_diff2)
```

### First-differenced data

#### Auto ARIMA - basic
```{r}
autoArima <- auto.arima(all_size_na$avg_size_diff, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(autoArima)
```
I ran this all again with second-differenced data, and the model doesn't appear significantly better (in some cases like BIC, not better) and so it doesn't justify the added complexity of a second-differenced model. so let's stick with first-differenced model.

#### Check model residuals
```{r}
residuals_auto <- residuals(autoArima)
plot(residuals_auto, main = "Residuals of AUTOSARIMA Model")
Acf(residuals_auto, main = "ACF of Residuals")
Box.test(residuals_auto, type = "Ljung-Box")

adf_test_residuals <- adf.test(residuals_auto, alternative = "stationary") #definitely stationary
qqnorm(residuals_auto)
qqline(residuals_auto)
shapiro.test(residuals_auto)

ggplot() +
  geom_histogram(aes(x = residuals_auto), bins = 20, color = "black") +
  labs(x = "Residuals",
       y = "Frequency")

fitted_values <- fitted(autoArima)
plot(fitted_values, residuals_auto, main="Residuals vs Fitted Values")
abline(h=0, col="red")
```
2-order differencing is not significantly better than 1-order differencing so let's stick with 1-order. 

AR(1) model: This is autoregressive data, and the current value of the time series depends linearly on its immediately preceding value - makes sense because body size shouldn't change that drastically week by week. 
I(1): First-order differencing applied to make the series stationary - which makes sense because preliminary analyses showed slope of zero with second-differenced data. 
MA(2): current value of the time series depends on the last two periods forecast errors - means that the current value of the series depends on the errors made in the previous two periods, which doesn't make a ton of intuitive sense except maybe if there are big changes to body size (i.e. reproduction?) that just means we won't see those changes finish until 2 weeks later? <<< CHECK THIS LOGIC.

#### Check ts trends/seasonality
```{r}
all_size_ts <- ts(all_size_na$avg_size_diff, frequency = 52) #weekly data as ts object
class(all_size_ts)
summary(all_size_ts)
acf(all_size_ts)
pacf(all_size_ts)

decomposition <- stl(all_size_ts, s.window = "periodic")
plot(decomposition)
```
There's no apparent seasonality and a slight positive trend in the data

#### Fit ARIMA with different I values
```{r}
# Fit ARIMA with different seasonal orders
model1 <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 1))
model2 <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2))
model3 <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 3))

Acf(residuals(model1), main = paste("ACF of Residuals model1"))
Pacf(residuals(model1), main = paste("PACF of Residuals model1"))

Acf(residuals(model2), main = paste("ACF of Residuals model2"))
Pacf(residuals(model2), main = paste("PACF of Residuals model2"))

Acf(residuals(model3), main = paste("ACF of Residuals model3"))
Pacf(residuals(model3), main = paste("PACF of Residuals model3"))

Box.test(residuals(model1), lag = 20, type = "Ljung-Box")
Box.test(residuals(model2), lag = 20, type = "Ljung-Box")
Box.test(residuals(model3), lag = 20, type = "Ljung-Box")
```
I(2) is definitely the best fit. 

#### Consider potential covariates
```{r}
#create matrix of dummy variables incorporating all potential covariates
xreg_All <- model.matrix(~ treatment + genet + mhw - 1, data = all_size_na)
xreg_NoGenet <- model.matrix(~ treatment + mhw - 1, data = all_size_na)
xreg_MHW <- model.matrix(~ mhw - 1, data = all_size_na)
xreg_Treatment <- model.matrix(~ treatment - 1, data = all_size_na)

arima_covariates_all <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2), xreg = xreg_All)
summary(arima_covariates_all)

arima_covariates_noGenet <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2), xreg = xreg_NoGenet)
summary(arima_covariates_noGenet)

arima_covariates_mhw <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2), xreg = xreg_MHW)
summary(arima_covariates_mhw)

arima_covariates_treatment <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2), xreg = xreg_Treatment)
summary(arima_covariates_treatment)
```
Looks like genet is not a significant covariate, but treatment and during/after MHW are slightly impactful. <<< DISCUSS WITH CHRIS.

Auto-ARIMA with Covariates model (check AR, I, MA)
```{r}
xreg_Covariate <- model.matrix(~ genet + mhw - 1, data = all_size_na)

autoArima_Covariate <- auto.arima(all_size_na$avg_size_diff, seasonal = TRUE, stepwise = FALSE, approximation = FALSE, xreg = xreg_Covariate)
summary(autoArima_Covariate)
```
No change to AR, I, or MA with any of the potential covariates.

*So, for first-differenced data, here are the two model options*
#### 
Basic ARIMA(1,1,2): 
Coefficients:
         ar1      ma1     ma2
      0.7682  -1.5296  0.5354
s.e.  0.0518   0.0662  0.0646

sigma^2 = 0.002058:  log likelihood = 2258.51
AIC=-4509.02   AICc=-4508.99   BIC=-4488.19

                      ME       RMSE        MAE MPE MAPE      MASE         ACF1
Training set 0.001281871 0.04529954 0.03479757 Inf  Inf 0.7851294 -0.009148644

####
####
ARIMA(1,1,2) with MHW/Treatment covariates:
Coefficients:
         ar1      ma1     ma2  treatment     mhw
      0.7661  -1.5293  0.5351    -0.0029  0.0012
s.e.  0.0539   0.0682  0.0662     0.0025  0.0155

sigma^2 = 0.002059:  log likelihood = 2259.17
AIC=-4506.35   AICc=-4506.28   BIC=-4475.1

                      ME       RMSE        MAE MPE MAPE      MASE         ACF1
Training set 0.001324029 0.04527702 0.03477904 NaN  Inf 0.7847113 -0.008522662
####

#### Check model residuals for MHW/Treatment ARIMA
```{r}
residuals_auto <- residuals(arima_covariates_noGenet)
plot(residuals_auto, main = "Residuals of AUTOSARIMA Model")
Acf(residuals_auto, main = "ACF of Residuals")
Box.test(residuals_auto, type = "Ljung-Box")

adf_test_residuals <- adf.test(residuals_auto, alternative = "stationary") #definitely stationary
qqnorm(residuals_auto)
qqline(residuals_auto)
shapiro.test(residuals_auto)

ggplot() +
  geom_histogram(aes(x = residuals_auto), bins = 20, color = "black") +
  labs(x = "Residuals",
       y = "Frequency")

fitted_values <- fitted(arima_covariates_noGenet)
plot(fitted_values, residuals_auto, main="Residuals vs Fitted Values")
abline(h=0, col="red")
```

### Now, let's check out ARIMA models on second-differenced data (since it is technically more stationary)
```{r}
autoArima2 <- auto.arima(all_size_na$avg_size_diff2, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(autoArima2)
```

AR(1) model: This is autoregressive data, and the current value of the time series depends linearly on its immediately preceding value - makes sense because body size shouldn't change that drastically week by week. 
I(0): No differencing applied to make the series stationary - which makes sense because the body size data has been differenced twice which we know makes it stationary.
MA(4): current value of the time series depends on the last FOUR periods forecast errors - means that the current value of the series depends on the errors made in the previous four periods. Greater value than first-differenced data model MA(2). Meaning??

#### Check model residuals
```{r}
residuals_auto2 <- residuals(autoArima2)
plot(residuals_auto2, main = "Residuals of AUTOSARIMA Model")
Acf(residuals_auto2, main = "ACF of Residuals")
Box.test(residuals_auto2, type = "Ljung-Box")

adf_test_residuals2 <- adf.test(residuals_auto2, alternative = "stationary") #definitely stationary
qqnorm(residuals_auto2)
qqline(residuals_auto2)
shapiro.test(residuals_auto2)

ggplot() +
  geom_histogram(aes(x = residuals_auto2), bins = 20, color = "black") +
  labs(x = "Residuals",
       y = "Frequency")

fitted_values2 <- fitted(autoArima2)
plot(fitted_values2, residuals_auto2, main="Residuals vs Fitted Values")
abline(h=0, col="red")
```
Shapiro-Wilks test says residuals are close to normal distribution but not normally distributed... but the plots look fine.

#### Consider potential covariates
```{r}
#create matrix of dummy variables incorporating all potential covariates
xreg_All <- model.matrix(~ treatment + genet + mhw - 1, data = all_size_na)
xreg_NoGenet <- model.matrix(~ treatment + mhw - 1, data = all_size_na)
xreg_NoTreatment <- model.matrix(~ genet + mhw - 1, data = all_size_na)
xreg_NoMHW <- model.matrix(~ treatment + genet - 1, data = all_size_na)
xreg_MHW <- model.matrix(~ mhw - 1, data = all_size_na)
xreg_Treatment <- model.matrix(~ treatment - 1, data = all_size_na)
xreg_Genet <- model.matrix(~ genet - 1, data = all_size_na)

arima_no_covariates2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4))
summary(arima_no_covariates2)

arima_covariates_all2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_All)
summary(arima_covariates_all2)

arima_covariates_noGenet2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_NoGenet)
summary(arima_covariates_noGenet2)

arima_covariates_noTreatment2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_NoTreatment)
summary(arima_covariates_noTreatment2)

arima_covariates_noMHW2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_NoMHW)
summary(arima_covariates_noMHW2)

arima_covariates_Genet2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_Genet)
summary(arima_covariates_Genet2)

arima_covariates_mhw2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_MHW)
summary(arima_covariates_mhw2)

arima_covariates_treatment2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_Treatment)
summary(arima_covariates_treatment2)
```
TreatmentxGenetxMHW model performs the worst. TreatmentxMHW, GenetxMHW, TreatmentxGenet perform second worst. No covariates performs best, Treatment alone, Genet alone, and MHW alone all perform second best. This probably means we should use no covariates in second-differenced data model?

Auto-ARIMA with Covariates model (check AR, I, MA)
```{r}
xreg_Covariate2 <- model.matrix(~ treatment + genet + mhw - 1, data = all_size_na)

autoArima_Covariate2 <- auto.arima(all_size_na$avg_size_diff2, seasonal = TRUE, stepwise = FALSE, approximation = FALSE, xreg = xreg_Covariate2)
summary(autoArima_Covariate2)
```
Same as first-differenced data, no change to AR, I, or MA with any of the potential covariates.

*For second-differenced data, here is the apparent best model option*
####
Series: all_size_na$avg_size_diff2 
ARIMA(1,0,4) with zero mean 
Coefficients:
         ar1      ma1     ma2      ma3      ma4
      0.9289  -0.7029  0.0217  -0.0357  -0.1054
s.e.  0.0412   0.0506  0.0356   0.0353   0.0303

sigma^2 = 0.005237:  log likelihood = 1541.35
AIC=-3070.71   AICc=-3070.64   BIC=-3039.8

Training set error measures:
                       ME      RMSE        MAE       MPE    MAPE      MASE
Training set 0.0007083893 0.0722271 0.05541204 -20.12162 319.208 0.7668489
                    ACF1
Training set 0.000179781
#### 

### Plot differenced data with O.G. values

#### Genet only
```{r, fig.width=10}
# Create the plots
plot_undifferenced <- ggplot(all_size_na, aes(x = date, y = avg_size_log, group=as.factor(genet))) +
  geom_point(aes(color=as.factor(genet)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(genet)), method="lm") +
  ggtitle("Undifferenced Data") +
  xlab("Date") +
  ylab("Log Body size (mm)") +
  theme_bw()

plot_differenced <- ggplot(all_size_na, aes(x = date, y = avg_size_diff, group=as.factor(genet))) +
  geom_point(aes(color=as.factor(genet)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(genet)), method="lm") +
  ggtitle("Differenced Data") +
  xlab("Date") +
  ylab("Differenced Body Size (mm)") +
  theme_bw()

plot_differenced2 <- ggplot(all_size_na, aes(x = date, y = avg_size_diff2, group=as.factor(genet))) +
  geom_point(aes(color=as.factor(genet)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(genet)), method="lm") +
  ggtitle("2-Differenced Data") +
  xlab("Date") +
  ylab("2-Differenced Body Size (mm)") +
  theme_bw()

combined_plot <- grid.arrange(plot_undifferenced, plot_differenced, plot_differenced2, ncol = 3)

#ggsave(here("experiment", "figures", "differenced_comparison_genet.png"), combined_plot, width=14, height=7)
```
Well damn, that makes sense why genet is not a significant covariate in the differenced data models! 

#### Treatment only
```{r, fig.width=10}
# Create the plots
plot_undifferenced <- ggplot(all_size_na, aes(x = date, y = avg_size_log, group=as.factor(treatment))) +
  geom_point(aes(color=as.factor(treatment)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(treatment)), method="lm") +
  ggtitle("Undifferenced Data") +
  xlab("Date") +
  ylab("Log Body size (mm)") +
  theme_bw()

plot_differenced <- ggplot(all_size_na, aes(x = date, y = avg_size_diff, group=as.factor(treatment))) +
  geom_point(aes(color=as.factor(treatment)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(treatment)), method="lm") +
  ggtitle("Differenced Data") +
  xlab("Date") +
  ylab("Differenced Body Size (mm)") +
  theme_bw()

plot_differenced2 <- ggplot(all_size_na, aes(x = date, y = avg_size_diff2, group=as.factor(treatment))) +
  geom_point(aes(color=as.factor(treatment)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(treatment)), method="lm") +
  ggtitle("2-Differenced Data") +
  xlab("Date") +
  ylab("2-Differenced Body Size (mm)") +
  theme_bw()

combined_plot <- grid.arrange(plot_undifferenced, plot_differenced, plot_differenced2, ncol = 3)

#ggsave(here("experiment", "figures", "differenced_comparison_treatment.png"), combined_plot, width=14, height=7)
```
No difference in linear slope between three treatments

#### MHW only
```{r, fig.width=10}
# Create the plots
plot_undifferenced <- ggplot(all_size_na, aes(x = date, y = avg_size_log, group=as.factor(mhw))) +
  geom_point(aes(color=as.factor(mhw)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(mhw)), method="lm") +
  ggtitle("Undifferenced Data") +
  xlab("Date") +
  ylab("Log Body size (mm)") +
  theme_bw()

plot_differenced <- ggplot(all_size_na, aes(x = date, y = avg_size_diff, group=as.factor(mhw))) +
  geom_point(aes(color=as.factor(mhw)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(mhw)), method="lm") +
  ggtitle("Differenced Data") +
  xlab("Date") +
  ylab("Differenced Body Size (mm)") +
  theme_bw()

plot_differenced2 <- ggplot(all_size_na, aes(x = date, y = avg_size_diff2, group=as.factor(mhw))) +
  geom_point(aes(color=as.factor(mhw)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(mhw)), method="lm") +
  ggtitle("2-Differenced Data") +
  xlab("Date") +
  ylab("2-Differenced Body Size (mm)") +
  theme_bw()

combined_plot <- grid.arrange(plot_undifferenced, plot_differenced, plot_differenced2, ncol = 3)

#ggsave(here("experiment", "figures", "differenced_comparison_mhw.png"), combined_plot, width=14, height=7)
```
MHW slope is definitely different in undifferenced data.

### Use basic first-differenced ARIMA to forecast.
```{r}
future_predictions <- forecast(autoArima, h = 50)  # Forecast h periods ahead

plot(future_predictions)
```

Just for conceptual understanding, create a quick autoARIMA model on undifferenced data and plot forecast
```{r}
autoArima_undiff <- auto.arima(all_size_na$avg_size_log, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(autoArima_undiff)

future_predictions <- forecast(autoArima_undiff, h = 50)  # Forecast h periods ahead
plot(future_predictions)
```

### TL;DR First Differenced AutoARIMA model = ARIMA(1,1,2)

## Body size data - MARSS, what we should have been doing all along...
https://atsa-es.github.io/MARSS/

### Prep data for MARSS model
```{r}
start_date <- min(weekly_temp$friday)

all_size_marss <- all %>%
  filter(!is.na(avg_size)) %>%
  mutate(week = as.numeric(difftime(date, start_date, units = "weeks")) + 1,
         avg_size_log = log(avg_size),
         sd_size_log = log(sd_size),
         tank = as.numeric(tank)) %>%
  select(date, week, tank, mhw, genet, avg_size_log) %>%
  left_join(weekly_temp, by = c("date" = "friday", "tank"))

marss_data <- all_size_marss %>%
  mutate(genet_tank = paste(genet, tank, sep = "_"))

marss_data_wide <- marss_data %>%
  select(-date, -tank, -genet, -mhw, -treatment, -avg_temp, -min_temp, -max_temp) %>%
  pivot_wider(names_from = genet_tank, values_from = avg_size_log) %>%
  column_to_rownames(var = "week") %>%
  as.matrix() %>%
  t()

marss_data_trimmed <- marss_data_wide[1:(nrow(marss_data_wide) - 50), 1:(ncol(marss_data_wide) - 6)]

#create temperature matrix as covariate
temp_covariate <- marss_data %>%
  select(week, avg_temp, genet_tank) %>%
  pivot_wider(names_from = genet_tank, values_from = avg_temp) %>%
  column_to_rownames(var = "week") %>%
  as.matrix() %>%
  t()

temp_covariate[duplicated(temp_covariate),]

temp_covariate_trimmed <- temp_covariate[1:(nrow(temp_covariate) - 50), 1:(ncol(temp_covariate) - 6)]

```

### Fit MARSS model
```{r}
model_list1 <- list(
  B = "identity",
  U = "unequal",
  Q = "diagonal and equal",
  Z = "identity",
  A = "scaling",
  R = "diagonal and equal"
)
fit1 <- MARSS(marss_data_trimmed, model=model_list1, method="BFGS")
tidy(fit1)
autoplot(fit1)

model_list2 <- list(
  B = "identity",
  U = "unequal",
  Q = "diagonal and equal",
  Z = "identity",
  A = "scaling",
  R = "diagonal and equal",
  C = "unconstrained",
  c = temp_covariate
)
fit2 <- MARSS(marss_data_wide, model=model_list2, method="BFGS")
tidy(fit2)
autoplot(fit2)

model_list3 <- list(
  B = "identity", # State transition; "identity" because body size is autocorrelated
  U = "unequal", # "unequal" because expect different genets/treatments respond differently to external factors
  Q = "diagonal and unequal", # Process noise; expect variability in body size differs between genets
  Z = "identity", # Measurement matrix; measuring body size directly without transformation
  A = "scaling", # Initial states; not "zero" because initial body size differed between genets
  R = "diagonal and equal", # Observation noise; expect measurement error would not change between genets
  C = "unconstrained",
  c = temp_covariate
)
fit3 <- MARSS(marss_data_wide, model=model_list3, method="BFGS")
tidy(fit3)
autoplot(fit3)

c(AIC(fit1), AIC(fit2), AIC(fit3))
```

### Let's move on to much simpler models - from ATSA plankton data
```{r}
the.mean <- apply(marss_data_trimmed, 1, mean, na.rm = TRUE)
the.sigma <- sqrt(apply(marss_data_trimmed, 1, var, na.rm = TRUE))
marss_data_trimmed <- (marss_data_trimmed - the.mean) * (1/the.sigma)

the.mean <- apply(temp_covariate_trimmed, 1, mean, na.rm = TRUE)
the.sigma <- sqrt(apply(temp_covariate_trimmed, 1, var, na.rm = TRUE))
temp_covariate_trimmed <- (temp_covariate_trimmed - the.mean) * (1/the.sigma)

#Observation error only model
Q <- U <- x0 <- "zero"
B <- Z <- "identity"
d <- temp_covariate_trimmed
A <- "zero"
D <- "unconstrained"
y <- marss_data_trimmed  # to show relationship between dat & the equation
model.list1 <- list(B = B, U = U, Q = Q, Z = Z, A = A, D = D, 
    d = d, x0 = x0)
kem <- MARSS(y, model = model.list1, method="BFGS")

#Process error only model
R <- A <- U <- "zero"
B <- Z <- "identity"
Q <- "equalvarcov"
C <- "unconstrained"
model.list2 <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
    C = C, c = temp_covariate_trimmed)
kem2 <- MARSS(marss_data_trimmed, model = model.list2, method="BFGS")

R <- A <- U <- "zero"
B <- Z <- "identity"
Q <- "equalvarcov"
C <- "unconstrained"
c <- temp_covariate_trimmed
y <- marss_data_trimmed
model.list2 <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
    C = C, c = c)
kem2 <- MARSS(y, model = model.list2)

#Process and Observation error model
D <- R <- d <- A <- U <- "zero"
Z <- "identity"
B <- "diagonal and unequal"
Q <- "equalvarcov"
C <- "unconstrained"
c <- covariates
x0 <- "unequal"
tinitx <- 1
model.list3 <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
    D = D, d = d, C = C, c = c, x0 = x0, tinitx = tinitx)
kem3 <- MARSS(marss_data_wide, model = model.list3)

kem_default <- MARSS(marss_data_trimmed)
kem_bfgs <- MARSS(marss_data_trimmed, method="BFGS")
#correlated hidden state processes
kem_unconstrained <- MARSS(marss_data_trimmed, model=list(Q = "unconstrained"), control = list(safe = TRUE, trace=1))
#equally correlated hidden state processes
kem_equalvar <- MARSS(marss_data_trimmed, model=list(Q = "equalvarcov"))
```


### Monte Carlo parameter estimation
```{r}
MARSSmcinit <- function(MLEobj,
                        control = list(
                          numInits = 500, numInitSteps = 10,
                          MCbounds = list(
                            B = c(0, 1), U = c(-1, 1), Q = c(1, 1),
                            Z = c(0, 1), A = c(-1, 1), R = c(1, 1), x0 = c(-1, 1)
                          )
                        ),
                        silent = FALSE) {
  control.default <- list(numInits = 500, numInitSteps = 10, MCbounds = list(B = c(0, 1), U = c(-1, 1), Q = c(1, 1), Z = c(0, 1), A = c(-1, 1), R = c(1, 1), x0 = c(-1, 1)))
  if (!is.null(control)) {
    if (!is.list(control)) stop("MARSSmcinit: control must be a list")
    if (any(!(names(control) %in% names(control.default)))) stop(paste("MARSSmcinit: allowed control list elements are", names(control.default)))
    control.new <- control.default
    for (i in names(control)) control.new[[i]] <- control[[i]]
    control <- control.new
  }
  drawProgressBar <- FALSE
  if (!silent) { # then we can draw a progress bar
    cat("\n")
    cat("> Starting Monte Carlo Initializations\n")
    prev <- MARSS:::progressBar() # this is an internal function to MARSS
    drawProgressBar <- TRUE
  }
  MODELobj <- MLEobj[["marss"]]
  y <- MODELobj$data
  par.dims <- attr(MODELobj, "model.dims")
  m <- par.dims[["x"]][1]
  n <- par.dims[["y"]][1]
  TT <- par.dims[["data"]][2]
  ## YM matrix for handling missing values
  YM <- matrix(as.numeric(!is.na(y)), n, TT)
  # Make sure the missing vals in y are zeroed out
  y[YM == 0] <- 0

  free.tmp <- MODELobj$free
  dim.tmp <- list(Z = c(n, m), A = c(n, 1), R = c(n, n), B = c(m, m), U = c(m, 1), Q = c(m, m), x0 = c(m, 1))
  bounds.tmp <- control$MCbounds
  init <- bestinits <- MLEobj$start
  bestLL <- -1.0e10

  # loop over numInits: # of random draws of initial values
  for (loop in 1:control$numInits) {
    init.loop <- init

    # Draw random values
    en <- c("Z", "A", "R", "B", "U", "Q", "x0")
    for (el in en) {
      dim.param <- dim.tmp[[el]]
      if (!MARSS:::is.fixed(free.tmp[[el]])) { # is.fixed is a utility func in MARSS
        bounds.param <- bounds.tmp[[el]]
        # use the first fixed and free in a temporally varying model; arbitrary
        tmp <- list(f = MARSS:::sub3D(MODELobj$fixed[[el]], t = 1), D = MARSS:::sub3D(MODELobj$free[[el]], t = 1))
        if (el %in% c("Q", "R")) { # random starts drawn from a wishart dist
          if (bounds.param[1] < dim.param[1]) {
            df <- dim.param[1]
          } else {
            df <- bounds.param[1]
          }
          S <- diag(bounds.param[2], dim.param[1])
          # draw a random matrix from wishart
          tmp.random <- MARSS:::rwishart(df, S) / df
          # reapply the sharing and fixed constraints
          par.random <- solve(t(tmp$D) %*% tmp$D) %*% t(tmp$D) %*% (MARSS:::vec(tmp.random) - tmp$f)
        } else {
          par.random <- matrix(runif(dim(tmp$D)[2], bounds.param[1], bounds.param[2]), dim(tmp$D)[2], 1)
          if (el %in% c("B")) {
            tmp.max <- max(abs(eigen(par.random, only.values = TRUE)$values))
            # rescale to bring the max abs eigenvalues to between 0 and 1
            par.random <- par.random / (tmp.max / runif(1, .01, .99))
          }
          if (el %in% c("x0")) {
            x0init <- init$x0 # where the original start is
            x.lo <- ifelse(x0init > 0, exp(bounds.param[1]) * x0init, exp(bounds.param[2]) * x0init)
            x.hi <- ifelse(x0init > 0, exp(bounds.param[2]) * x0init, exp(bounds.param[1]) * x0init)
            par.random <- matrix(runif(dim(tmp$D)[2], x.lo, x.hi), dim(tmp$D)[2], 1)
          }
        }
      } else {
        par.random <- matrix(0, 0, 1)
      }
      init.loop[[el]] <- par.random
    }

    ## Call MARSSkem() with these inits
    MLEobj$start <- init.loop
    MLEobj$control$maxit <- control$numInitSteps
    MLEobj$control$minit <- 1
    MLEobj$control$silent <- TRUE # don't output
    MLEobj <- MARSSkem(MLEobj) # get new fit using this init

    if (drawProgressBar == TRUE) prev <- MARSS:::progressBar(loop / control$numInits, prev)

    ## Check whether the likelihood is the best observed
    ## Only use bootstrap param draws where loglike did not go down during numInitSteps
    if (MLEobj$logLik > bestLL) {
      # update the best initial parameter estimates
      bestinits <- MLEobj$par
      bestLL <- MLEobj$logLik
    }
  } # end numInits loop

  return(bestinits)
}


fit1 <- MARSS(marss_data_wide)
MCinits <- MARSSmcinit(fit1, control = list(numInits = 10))
fit2 <- MARSS(marss_data_wide, inits = MCinits)
str(MCinits)
```
MARSSmcinit suggests 
- B should be identity but could be diagonal and unequal (unlikely but can try diagonal and equal). 
        Identity AIC: -4110, AICc -3949, log-likelihood: 2355
        Diagonal and equal AIC: -4580, AICc -4418, log-likelihood: 2591
        Diagonal and unequal AIC: 
- U should be unequal or unconstrained
- Q should be either diagonal and unequal or unconstrained
- Z should be identity
- A should be zero but scalar is possible (but unlikely)
- R could be anything but Identity (because value is 0.00187)
        Equalvarcov: measurement error equal but might be different
        Diag+Equal: Equal variance for all observations but covariance is zero
        Diag+Unequal: Some measurements might have more error variability than others
        Unconstranied: Most complex, do this at the very end (and at end of day)

structure of U and Q specify our hypotheses/assumptions about how the environment is shared between populations

### testing basic models and working our way up
#### Basic model
```{r}
model_list2 <- list(
  B = "identity",
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = "zero",
  R = "identity"
)
fit2 <- MARSS(marss_data_wide, model=model_list2)
```

#### A 
```{r}
#genotype_vec <- rep(c("A", "B", "C", "D", "E"), times = 15)
#AA <- model.matrix(~ (genotype_vec) - 1) 
#colnames(AA) <- levels(factor(genotype_vec))

# Create the genotype vector
genotype_vec <- rep(c("A", "B", "C", "D", "E"), times = 15)
genotype_numeric <- as.numeric(factor(genotype_vec, levels = c("A", "B", "C", "D", "E")))
A_matrix <- matrix(genotype_numeric, ncol = 1)

model_list4 <- list(
  B = "identity",
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit4 <- MARSS(marss_data_wide, model=model_list4)
```

#### B
```{r}
#create unique B matrix where each genet has its own ID
n_genets <- 5
n_tanks <- 15
n_time <- 19
n_states <- n_genets * n_tanks
B_matrix <- matrix(0, n_states, n_states)
for (i in 0:(n_genets - 1)) {
  start_row <- i * n_tanks + 1
  end_row <- (i + 1) * n_tanks
  B_matrix[start_row:end_row, start_row:end_row] <- diag(n_tanks)
}

model_list5 <- list(
  B = B_matrix,
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit5 <- MARSS(marss_data_wide, model=model_list5)

model_list6 <- list(
  B = "diagonal and unequal",
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit6 <- MARSS(marss_data_wide, model=model_list6)

model_list7 <- list(
  B = "diagonal and equal",
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit7 <- MARSS(marss_data_wide, model=model_list7)

#diagonal and equal fits less good but is way faster to run - by 10+ minutes. need to follow up with biological interpretation since "diagonal and equal" means genet responses to environment is uniform vs. "diagonal and unequal" means each genet responds differently to environment (aka one genet thrives more)

# Fit model with priors
priors5 <- MARSS(marss_data_wide, model=model_list5,control=list(maxit=100, allow.degen=TRUE, trace=1, safe=TRUE), fit=TRUE)
fit8 <- MARSS(marss_data_wide, model=model_list5, control=list(maxit = 1000), inits=priors5$par)

#doesn't improve fit BUT it did speed everything up so definitely do this with legit models in the future.
```

#### U
```{r}
model_list9 <- list(
  B = "diagonal and equal",
  U = "unequal",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit9 <- MARSS(marss_data_wide, model=model_list9)
#priors9 <- MARSS(marss_data_wide, model=model_list9,control=list(maxit=100, allow.degen=TRUE, trace=1, safe=TRUE), fit=TRUE)
#fit9prior <- MARSS(marss_data_wide, model=model_list9, control=list(maxit = 1000), inits=priors9$par)

model_list10 <- list(
  B = "diagonal and equal",
  U = "unconstrained",
  Q = "identity",
  Z = "identity", 
  A = A_matrix,
  R = "identity"
)
fit10 <- MARSS(marss_data_wide, model=model_list10)
```
Unconstrained U was the same model results as unequal U, meaning that process error was UNEQUAL (and not potentially the same, which would have been an option with unconstrained)

#### Q
On FUERTE mac, I ran diagonalxequal, diagonalxunequal, and unconstrained and found that diagonal and equal and unconstrained were better fits (though none converged) and had the same AIC values
```{r}
```

#### R
On FUERTE mac, I ran everything except R=identity. Only the two diagonal models converged, and "diagonal and equal" had a slightly better fit. 
```{r}
```

### Add tempearture covariate
```{r}
# Make C matrix for temperature covariate
C_matrix <- matrix(0, nrow = 75, ncol = 19)

# Fill the C matrix
for (time_point in 1:19) {
  for (genotype in 1:5) {
    # Each group of 5 genotypes is influenced by the temperature at the given time point
    C_matrix[(time_point - 1) * 5 + genotype, time_point] <- 1
  }
}
```


### harbor seal data
```{r}
data(harborSealWA, package = "MARSS")
dat <- MARSS::harborSealWA
years <- dat[, "Year"]
dat <- dat[, !(colnames(dat) %in% c("Year", "HC"))]
dat <- t(dat)  # transpose to have years across columns
colnames(dat) <- years
n <- nrow(dat) - 1

kem <- MARSS(dat)

mod.list.0 <- list(B = matrix(1), U = matrix("u"), Q = matrix("q"), 
                   Z = matrix(1, 4, 1), A = "scaling", R = "diagonal and unequal", 
                   x0 = matrix("mu"), tinitx = 0)
fit.0 <- MARSS(dat, model = mod.list.0)
autoplot(fit.0)

mod.list.1 <- list(B = matrix(1), U = matrix("u"), Q = matrix("q"), 
                   Z = matrix(1, 4, 1), A = "scaling", R = "diagonal and equal", 
                   x0 = matrix("mu"), tinitx = 0)
fit.1 <- MARSS(dat, model = mod.list.1)
# more supported
c(AIC(fit.0), AIC(fit.1))

# Q2. Change to observation errors R="unconstrained" What does that mean? Are the errors correlated across sites? Why might that happen?
mod.list.2 <- list(B = matrix(1), U = matrix("u"), Q = matrix("q"), 
                   Z = matrix(1, 4, 1), A = "scaling", R = "unconstrained", 
                   x0 = matrix("mu"), tinitx = 0)
fit.2 <- MARSS(dat, model = mod.list.2)
# less supported
c(AIC(fit.0), AIC(fit.1), AIC(fit.2))
# Means that the observation variances are correlated across sites

# Q3. Look at the observation variance and correlation matrix
M=coef(fit.2, type="matrix")$R # Variance matrix
cov2cor(M) # Correlation matrix
# Some of the errors seem to be correlated across sites, some positive, some negative

# Task Group 2
# https://nwfsc-timeseries.github.io/atsa-labs/sec-mss-segind.html
# Fit a model with four underlying state (population process)
mod.list.1 <- list(B = diag(1,4), U = matrix("u",4,1), Q = "diagonal and equal", 
                   Z = diag(1,4), A = "scaling", R = "diagonal and unequal", 
                   x0 = "unequal", tinitx = 0)
fit.1 <- MARSS(dat, model = mod.list.1)
# Q1. Look at the plots
autoplot(fit.1)

# Plot 2. States - That's the population estimate
# Q2. In the model you fit, are the 4 state processes (the lines) correlated with each other? Just based on the model you fit? Think about Q.

# No, not correlated. A good year in subpop 1 does not imply a good year in subpop 2.

# Q3. What does this model say about the observation variances? Same, different?

# Different levels of variance and independent errors.

# Q4. Are we assuming that good years are correlated across sites or uncorrelated?

# uncorrelated.

# Task Group 3
# https://nwfsc-timeseries.github.io/atsa-labs/sec-mss-segind.html
# Fit a model with four underlying state (population process)
mod.list.2 <- list(B = diag(1,4), U = matrix("u",4,1), Q = "equalvarcov", 
                   Z = diag(1,4), A = "scaling", R = "diagonal and equal", 
                   x0 = "unequal", tinitx = 0)
fit.2 <- MARSS(dat, model = mod.list.2)
# Q1. Look at the plots
autoplot(fit.2)

# Plot 2. States - That's the population estimate
# Q2. In the model you specified, is the variability in the 4 state processes (the lines) correlated with each other? Think about Q.

# Correlated but with the same variance. A good year in subpop 1 does not imply a good year in subpop 2.

#Q.diag      0.01218
#Q.offdiag   0.00983

#So pretty highly correlated.


# Q3. Compare to a model with Q="diagonal and equal" and R="equalvarcov". Which one fits the data better based on AICc?
mod.list.3 <- list(B = diag(1,4), U = matrix("u",4,1), Q = "diagonal and equal", 
                   Z = diag(1,4), A = "scaling", R = "equalvarcov", 
                   x0 = "unequal", tinitx = 0)
fit.3 <- MARSS(dat, model = mod.list.3)
# The model with R="equalvarcov" is less supported
c(AIC(fit.2), AIC(fit.3))

```

### fish spawn data
```{r}
install.packages("rCAX")
library(rCAX)
f <- list(nmfs_popid=7)
tab <- rcax_hli("NOSA")

columbia.river <- tab %>%
  filter(esu_dps %in% c("Steelhead (Middle Columbia River DPS)","Steelhead (Upper Columbia River DPS)","Steelhead (Lower Columbia River DPS)","Salmon, coho (Lower Columbia River ESU)","Salmon, Chinook (Lower Columbia River ESU)")) %>%
  select(species, esu_dps, majorpopgroup, esapopname, commonpopname, run, spawningyear, nosaij) %>%
  rename("value" = nosaij) %>%
  #remove NA values
  filter(!is.na(value)) %>%
  mutate(value = as.numeric(value))

esu <- unique(columbia.river$esu_dps)
colnames(columbia.river)
esuname <- esu[2]
dat <- columbia.river %>% 
  subset(esu_dps == esuname) %>% # get only this ESU
  mutate(log.spawner = log(value)) %>% # create a column called log.spawner
  select(esapopname, spawningyear, log.spawner) %>% # get just the columns that I need
  pivot_wider(names_from = "esapopname", values_from = "log.spawner") %>% 
  column_to_rownames(var = "spawningyear") %>% # make the years rownames
  as.matrix() %>% # turn into a matrix with year down the rows
  t()

dat[is.na(dat)] <- NA
tmp <- rownames(dat)
tmp <- stringr::str_replace(tmp, "Steelhead [(]Upper Columbia River DPS[)]", "")
tmp <- stringr::str_replace(tmp, "River - summer", "")
tmp <- stringr::str_trim(tmp)
rownames(dat) <- tmp


df <- columbia.river %>% subset(esu_dps %in% c("Steelhead (Lower Columbia River DPS)"))
ggplot(df, aes(x=spawningyear, y=log(value), color=majorpopgroup)) + 
  geom_point(size=0.2, na.rm = TRUE) + 
  theme(strip.text.x = element_text(size = 3)) +
  theme(axis.text.x = element_text(size = 5, angle = 90)) +
  facet_wrap(~esapopname)
  #ggtitle(paste0(esuname, collapse="\n"))

plotesu(esu[3])

cap <- cax()
datasets <- cap$datasets()
print(datasets)
columbia-river <- cap$get("dataset_name")  # Use the correct dataset identifier
load(file.path("experiment", "data", "draft", "columbia-river.rda"))
```

### lake plankton data
```{r}
data(lakeWAplankton, package = "MARSS") 
# lakeWA
fulldat <- lakeWAplanktonTrans
years <- fulldat[, "Year"] >= 1965 & fulldat[, "Year"] < 1975
dat <- t(fulldat[years, c("Greens", "Bluegreens")])
covariates <- t(fulldat[years, c("Temp", "TP")])

the.mean <- apply(dat, 1, mean, na.rm = TRUE)
the.sigma <- sqrt(apply(dat, 1, var, na.rm = TRUE))
dat <- (dat - the.mean) * (1/the.sigma)

the.mean <- apply(covariates, 1, mean, na.rm = TRUE)
the.sigma <- sqrt(apply(covariates, 1, var, na.rm = TRUE))
covariates <- (covariates - the.mean) * (1/the.sigma)
```



