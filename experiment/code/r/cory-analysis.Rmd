---
title: "Corynactis MHWsim experiment data analysis"
author: "Amelia Ritger"
date: "2023-11-01"
output: html_document
---

# Load packages 
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(janitor)
library(here)
library(lubridate)
library(svglite) #save vector figures
library(ggpmisc) #statpolyeq ggplot
library(lme4) #GLMER
library(nlme) #nlme
#library(glmmTMB) #GLMER
library(glarma)
#library(MASS) #GLARMA
library(tseries) #ADF test
library(urca) #KPSS test
library(forecast) #ts()
library(ggrepel) #geom_text_repel
library(broom) #tidy()
library(gridExtra) #grid.arrange()
library(MARSS)
```

# Run this ONE TIME to load temperature/body size data, then save the file to your local machine
```{r}
#source("readRPiData.R", local = knitr::knit_global())
#source("readBodySizeData.R", local = knitr::knit_global())
```

# Load Tidied Data
```{r, include = FALSE}
source("mergeAllData.R", local = knitr::knit_global())

#Load temperature data AFTER you have run source(ReadRPiData)
rpi_temp <- read_csv(here("experiment", "data", "rpi_temp.csv"))
weekly_temp <- read_csv(here("experiment", "data", "weekly_temperature.csv"))
daily_temp <- read_csv(here("experiment", "data", "three_day_temperature.csv"))
```

## Create tidied df
```{r}
all_size <- all %>%
  filter(!is.na(avg_size)) %>%
  mutate(avg_size_log = log(avg_size)) %>%
  group_by(tank, genet) %>%
  mutate(avg_size_diff = avg_size_log - lag(avg_size_log),
         avg_size_diff2 = avg_size_diff - lag(avg_size_diff)) %>%
  ungroup()
```

# Count max/total number of polyps
```{r}
all_sum <- all %>%
  group_by(date) %>%
  #get sum of n_true
  summarize(nTRUE = sum(n_true),
            n = sum(n))

all_sum_genet <- all %>%
  filter(date == min(date)) %>%
  group_by(treatment, genet) %>%
  summarize(n = sum(n))

ggplot(all_sum_genet, aes(x = treatment, y = n, group = genet, color=genet)) +
  geom_segment(aes(yend = 0), size = 2, position = position_dodge(width = 0.8)) +
  geom_point(aes(fill = genet), size = 6, position = position_dodge(width = 0.8), shape = 21) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_x_discrete(labels = c("cold" = "Control", 
                              "severe" = "Severe MHW", 
                              "extreme" = "Extreme MHW")) +
  labs(x = "", y = "Number of polyps", fill = "Genet") +
  guides(color = "none") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 20),
        legend.position = "top") +
  coord_flip()

ggsave(here("experiment", "figures", "WSN", "starting_n.png"), width=12, height=7)
```


# Visualize the data

## Data QA/QC plots
```{r}
ggplot(all, aes(x=date, y=diff_n_cont, color=genet)) +
  geom_point() +
  facet_wrap(~treatment) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal()
```

## Plot basic growth rate across entire experiment
```{r, fig.width=10}
ggplot(all, aes(x=date, y=growth_rate_cont, color=genet)) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  geom_point(alpha=0.5) +
  geom_smooth(method = "lm", formula = y ~ x, se = F) +
  stat_poly_eq(use_label("eq"), formula = y ~ x, na.rm=TRUE) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  labs(x = "Date",
       y = "Cumulative population growth (no. polyps)",
       color = "Genet")

#ggsave(here("experiment", "figures", "growth_genet.png"), width=12, height=7)
```

## Plot growth rate of all genets during and after MHW
```{r}
ggplot(all, aes(x=date, y=growth_rate, color = treatment)) +
  geom_point(alpha=0.1) +
  facet_wrap(~mhw, scales="free", labeller = labeller(mhw = c("during" = "During MHW", "after" = "After MHW"))) +
  geom_smooth(method = "lm", formula = y ~ x, se = F) +
  stat_poly_eq(use_label("eq"), formula = y ~ x, parse = TRUE, label.x.npc = "left") +
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +
  theme_minimal() +
  coord_cartesian(ylim = c(-5, 25), expand=TRUE) +   #set equal y axes coordinates
  labs(x = "Date",
       y = "Cumulative population growth (no. polyps)",
       color = "Treatment")

#ggsave(here("experiment", "figures", "growth_MHW.png"), width=12, height=7)
```

## Plot growth rate of each genet during and after MHW
```{r, fig.width=10, fig.height=7}
ggplot(all, aes(x=date, y=growth_rate, color = genet)) +
  geom_point(alpha=0.1) +
  facet_grid(mhw~treatment, 
             #switch = 'x',
             scales = "free_x",
             labeller = labeller(mhw = c("during" = "During MHW", "after" = "After MHW"),
                                 treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  geom_smooth(method = "lm", formula = y ~ x, se = F) +
  stat_poly_eq(use_label("eq"), formula = y ~ x, parse = TRUE, label.x.npc = "left") +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  #coord_cartesian(ylim = c(-5, 25), expand=TRUE) +   #set equal y axes coordinates
  labs(x = "Date",
       y = "Cumulative population growth (no. polyps)",
       color = "Genet")
  theme(strip.placement = "outside")

#ggsave(here("experiment", "figures", "growth_MHW_genet.png"), width=12, height=10)
```

## Plot growth rate of each genet during and after MHW - but not faceted by MHW
```{r}
label_plot <- c("0.13", "0.09", "0.071", "0.0522", "0.0761",
                "0.151", "0.0933", "0.0252", "0.116", "0.0413", #extreme during
                "0.166", "0.142", "0.0522", "0.134", "0.0902",
                "-0.0023", "0.00741", "0.026", "0.00843", "-0.0131",
                "0.0102", "0.00652", "0.00881", "0.00421", "-0.0117", #extreme after
                "-0.0104", "-0.000895", "0.0217", "0.0127", "-0.0131")
color_plot <- rep(c("#9370DB", "#C21B78", "#FF9933", "#FF3333", "#662B45"), times = 6)
treatment_plot <- rep(c(rep("cold", 5), rep("extreme", 5), rep("severe", 5)), times = 2)
date_plot <- c(rep("2023-10-01", 15), rep("2024-01-10", 15))

p <- ggplot(all, aes(x=date, y=growth_rate_cont, color=genet, shape=mhw)) +
  geom_point(alpha=0.15) +
  facet_wrap(~factor(treatment, levels=c("cold", "severe", "extreme"), labels=c("Ambient", "Severe MHW", "Extreme MHW"))) +   geom_rect(aes(xmin = as.Date("2023-11-30"), xmax = as.Date("2023-12-10"), ymin = -5, ymax = -4.5), color = "dimgray", fill = "dimgray", alpha = 0.4) +  # Add horizontal bar
    geom_rect(aes(xmin = as.Date("2024-02-14"), xmax = as.Date("2024-02-21"), ymin = -5, ymax = -4.5), color = "dimgray", fill = "dimgray", alpha = 0.4) +  # Add horizontal bar
  geom_vline(xintercept = as.Date("2023-12-10"), linetype = "dashed") +  # Add vertical line delineating during/after MHW
  geom_smooth(method = "lm", se = FALSE) +
  #stat_poly_eq(use_label("eq"), formula = y ~ x, parse = TRUE) +  #un-comment this to get updated values to add to label_plot
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_shape_manual(values = c("during" = 16, "after" = 17)) +  # Define shapes for mhw levels
  theme_minimal() +
  coord_cartesian(ylim = c(-5, 25), expand=TRUE) +
  labs(x = "Date",
       y = "Cumulative population growth (no. polyps)",
       color = "Genet") +
  guides(shape = "none")

loop_count = 0
for (i in 1:length(label_plot)) {
  p <- p + geom_text(data = data.frame(date = as.Date("2023-12-30"), treatment = treatment_plot[i], mhw = "during"), 
              label=paste0("m = ", label_plot[i]), x = as.Date(date_plot[i]), y = 25-loop_count, hjust = 0, vjust = 1, color=color_plot[i])
  loop_count <- loop_count + 1
  if (loop_count > 4) {
    loop_count <- 0  # Reset counter after every 5th iteration
  }
}
p

#ggsave(here("experiment", "figures", "growth_MHW_genet_combined.png"), width=12, height=7)
```

## Plot raw population numbers
```{r}
g <- ggplot(all, aes(x=date, y=n_true, color=genet, shape=mhw)) +
  geom_point(alpha=0.15) +
  facet_wrap(~factor(treatment, levels=c("cold", "severe", "extreme"), labels=c("Ambient", "Severe MHW", "Extreme MHW"))) +
  geom_rect(aes(xmin = as.Date("2023-11-30"), xmax = as.Date("2023-12-10"), ymin = 5, ymax = 5.5), color = "dimgray", fill = "dimgray", alpha = 0.2) +  # Add horizontal bar
  geom_rect(aes(xmin = as.Date("2024-02-14"), xmax = as.Date("2024-02-21"), ymin = 5, ymax = 5.5), color = "dimgray", fill = "dimgray", alpha = 0.2) +  # Add horizontal bar
  geom_vline(xintercept = as.Date("2023-12-10"), linetype = "dashed", linewidth = 0.5) +  # Add vertical line delineating during/after MHW
  geom_smooth(method = "lm", se = FALSE) +
 # stat_poly_eq(use_label("eq"), formula = y ~ x, parse = TRUE) +  
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_shape_manual(values = c("during" = 16, "after" = 17)) +  # Define shapes for mhw levels
  theme_minimal() +
  #coord_cartesian(ylim = c(5, 40), expand=TRUE) +
  labs(x = "Date",
       y = "Number of polyps",
       color = "Genet") +
  guides(shape = "none")

loop_count = 0
for (i in 1:length(label_plot)) {
  g <- g + geom_text(data = data.frame(date = as.Date("2023-12-30"), treatment = treatment_plot[i], mhw = "during"), 
              label=paste0("m = ", label_plot[i]), x = as.Date(date_plot[i]), y = 40-loop_count, hjust = 0, vjust = 1, color=color_plot[i])
  loop_count <- loop_count + 1
  if (loop_count > 4) {
    loop_count <- 0  # Reset counter after every 5th iteration
  }
}
g

#ggsave(here("experiment", "figures", "n_MHW_genet_combined.png"), width=12, height=7)
```

## Plot behavior data
```{r, fig.width=10}
# %closed facet grid
ggplot(all, aes(x=genet, y=percent_closed, fill=genet, group=genet)) +
  geom_point(aes(color=genet)) +
  geom_violin(position="dodge", alpha=0.5, outlier.colour="transparent") +
  facet_grid(mhw~treatment)

# %closed facet wrap
ggplot(all, aes(x = genet, y = percent_closed, fill = genet, group = interaction(genet, mhw), shape = mhw)) +
  geom_point(aes(color = genet), position = position_dodge(width = 0.9)) +
  geom_violin(aes(size = mhw), position = position_dodge(width = 0.9), alpha = 0.5, outlier.colour = "transparent") +
  facet_wrap(~treatment) +
  scale_size_manual(values = c("during" = 1, "after" = 0.5)) + # Define sizes for mhw levels
  scale_shape_manual(values = c("during" = 16, "after" = 17))  # Define shapes for mhw levels

# %fullyOpen facet wrap
ggplot(all, aes(x = genet, y = percent_fully_open, fill = genet, group = interaction(genet, mhw), shape = mhw)) +
  geom_point(aes(color = genet), position = position_dodge(width = 0.9), alpha = 0.5) +
  geom_violin(aes(size = mhw), position = position_dodge(width = 0.9), alpha = 0.3, color = alpha("black", 0.75)) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_size_manual(values = c("during" = 1, "after" = 0.5), labels = c("During MHW", "After MHW")) + # Define sizes for mhw levels
  scale_shape_manual(values = c("during" = 16, "after" = 17), labels = c("During MHW", "After MHW")) + # Define shapes for mhw levels
  # change color of violin 
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  labs(x = "Genet",
       y = "% open polyps",
       color = "Genet",
       shape = "MHW",
       size = "MHW") +
  guides(fill = "none",
         color = "none")

#ggsave(here("experiment", "figures", "open_MHW_genet.png"), width=12, height=7)
```

## Plot total biomass over time
```{r}
ggplot(all, aes(x=date, y=total_biomass)) +
  geom_point(aes(color=genet), size=0.5) +
  geom_smooth(aes(fill=genet, color=genet), method="lm", se=T) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  labs(x = "Date",
       y = "Total biomass (g)",
       color = "Genet",
       fill = "Genet")

#ggsave(here("experiment", "figures", "total_biomass_mhw.png"), width=12, height=7)
```

## Plot average body size over time
```{r}
ggplot(all, aes(x=date, y=avg_size)) +
  geom_point(aes(color=genet), size=0.5) +
  geom_smooth(aes(fill=genet, color=genet), method="loess", se=T) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  theme_minimal() +
  labs(x = "Date",
       y = "Average body size (mm)",
       color = "Genet",
       fill = "Genet")

#ggsave(here("experiment", "figures", "avg_size_mhw.png"), width=12, height=7)
```

## Plot mortality (lol)
```{r}
ggplot(all, aes(x=date, y=dying_dead, color = treatment)) +
  geom_point(alpha=0.1) +
  facet_wrap(~treatment, 
             labeller = labeller(treatment = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  geom_smooth(method = "lm", formula = y ~ x, se = F) +
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00")) +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0, 10), labels = function(x) format(x, nsmall = 0)) +
  labs(x = "Date",
       y = "Mortality (no. polyps)",
       color = "Treatment") +
  guides(color = "none")

#ggsave(here("experiment", "figures", "mortality_MHW.png"), width=12, height=7)
```

############################################################################################################

# Start running stats

## Count data - GLARMA 

### Prepare data
```{r}
all_count <- all %>%
  #convert date to a numeric sequence
  mutate(day_numeric = as.numeric(as.Date(date)-min(as.Date(date))),
         mhw_binary = as.factor(ifelse(mhw == "during", 0, 1))) %>%
  dplyr::select(day_numeric, date, tank, genet, treatment, mhw, mhw_binary, n_true, growth_rate, growth_rate_cont) %>%
  rename(growth_rate_mhw = growth_rate) %>% #relabel for better understanding. growth_rate_mhw is where the growth rate resets to zero when the MHW ends. I believe I won't need growth_rate_mhw for GLARMA model because I can add state as a potential covariate.
  mutate(genet_tank = paste(tank, genet, sep = ""), #merge genet and tank because each unique genet x tank value is going to be autocorrelated
         intercept = 1) # create intercept of 1s for glarma
```

### Check distribution
```{r}
hist(all_count$growth_rate_cont, breaks = seq(min(all_count$growth_rate_cont), max(all_count$growth_rate_cont) + 0.5, by = 2), 
     main = "Histogram of Growth Rate", xlab = "Growth Rate", 
     ylab = "Frequency", col = "lightblue")

hist(all_count$n_true, breaks = seq(min(all_count$n_true)-0.5, max(all_count$n_true) + 0.5, by = 2), 
     main = "Histogram of Counts", xlab = "Count", 
     ylab = "Frequency", col = "lightblue")
```

### Compare Poisson with NB model
```{r}
# Fit a Poisson model
poisson_growth <- glmer((growth_rate_cont+4) ~ treatment + day_numeric + (1|tank), family = poisson, data = all_count)
poisson_n <- glmer(n_true ~ treatment + day_numeric + (1|tank), family = poisson, data = all_count)

# Fit a Negative Binomial model
nb_growth <- glmer.nb((growth_rate_cont+4) ~ treatment + day_numeric + (1|tank), data = all_count)
nb_n <- glmer.nb(n_true ~ treatment + day_numeric + (1|tank), data = all_count)

# Compare models
AIC(poisson_growth, nb_growth, poisson_n, nb_n)
```
NB is a better fit with random Tank effect, but Poisson is better fit with random GenetTank effect

### Growth rate: QQ plot, chi square test, overdispersion ratio
```{r}
qqnorm(all_count$growth_rate_cont)
qqline(all_count$growth_rate_cont, col = "red")

# Create a table of observed counts
observed_counts <- table(all_count$growth_rate_cont)
# Calculate expected counts based on Poisson distribution
lambda <- mean(all_count$growth_rate_cont)
expected_counts <- dpois(as.numeric(names(observed_counts)), lambda) * length(observed_counts)
# Perform the Chi-squared test
chisq.test(observed_counts, p = expected_counts, rescale.p = TRUE)

growth_mean <- mean(all_count$growth_rate_cont)
growth_var <- var(all_count$growth_rate_cont)
# Check ratio
overdispersion_ratio <- growth_var / growth_mean
print(overdispersion_ratio) #growth rate is overdispersed

shapiro.test(all_count$growth_rate_cont)
```

### Counts: QQ plot, chi square test, overdispersion ratio
```{r}
qqnorm(all_count$n_true)
qqline(all_count$n_true, col = "red")

# Create a table of observed counts
observed_counts <- table(all_count$n_true)
# Calculate expected counts based on Poisson distribution
lambda <- mean(all_count$n_true)
expected_counts <- dpois(as.numeric(names(observed_counts)), lambda) * length(observed_counts)
# Perform the Chi-squared test
chisq.test(observed_counts, p = expected_counts, rescale.p = TRUE)

n_mean <- mean(all_count$n_true)
n_var <- var(all_count$n_true)
# Check ratio
overdispersion_ratio <- n_var / n_mean
print(overdispersion_ratio) #growth rate is overdispersed

shapiro.test(all_count$n_true)
```

Neither count data nor growth rate appear to follow poisson distribution. And both data metrics are overdispersed (mean << variance), so let's check with negative binomial distribution.

Growth rate contains negative numbers, which complicates the fact that it appears to follow negative binomial distribution. Let's focus on the raw count numbers for now and come back to growth rate later.

### Negative binomial distribution?
```{r}
nb_fit <- glmmTMB(n_true ~ time + (1|genet_tank), data = all_count, family = nbinom1)
summary(nb_fit)

residuals_nb <- residuals(nb_fit)
fitted_values_nb <- fitted(nb_fit)

glm.nb(n_true ~ 1, data = all_count) %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)

plot(residuals_nb, fitted_values_nb,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs. Fitted Values")
abline(h = 0, col = "red", lty = 2)
```
Doesn't look too good. 

### Let's just jump straight into GLARMA and see what happens
```{r}
y <- all_count$n_true %>%
  as.matrix()
X <- model.matrix(n_true ~ treatment + genet+ treatment:genet + mhw, data = all_count)
x <- model.matrix(n_true ~ treatment + genet + treatment:genet, data = all_count)

#equivalent but more wordy
#all_count_glarma <- all_count %>%
#  mutate(dummy = 1) %>%
#  pivot_wider(names_from = treatment, values_from = dummy, values_fill = #list(dummy = 0)) %>%
#  mutate(dummy = 1) %>%
#  pivot_wider(names_from = genet, values_from = dummy, values_fill = #list(dummy = 0)) %>%
#  mutate(dummy = 1) %>%
#  pivot_wider(names_from = genet_tank, values_from = dummy, values_fill = #list(dummy = 0))
# x <- all_count_glarma %>%
#   select(intercept, severe, extreme, B, C, D, E) %>%
#   as.matrix()

#glarma_model <- glarma(y, X, phiLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model <- glarma(y, x, phiLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_state <- glarma(y, X, phiLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

summary(glarma_model)
summary(glarma_model_state)
#interaction model is better all around - which doesn't deviate from the GLMM
# adding in state (MHW before vs after) improves model fit
```

#### Let's test GLARMA models now with different lags and distributions
```{r}
glarma_model_nr <- glarma(y, x, phiLags = c(1), type = "Poi", method = "NR", residuals = "Pearson", maxit = 100, grad = 1e-6)
summary(glarma_model_nr) #no effect of NR vs FS other than computation time being faster for NR
glarma_model_nb <- glarma(y, x, phiLags = c(1), type = "NegBin", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6, alphaInit = 0.1)
#NegBin doesn't run --> WHY?????? My data do not follow poisson

#Let's just keep doing model comparison with different lags I guess...
glarma_model_2 <- glarma(y, x, phiLags = c(2), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_3 <- glarma(y, x, phiLags = c(3), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_4 <- glarma(y, x, phiLags = c(4), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_5 <- glarma(y, x, phiLags = c(5), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_6 <- glarma(y, x, phiLags = c(6), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_7 <- glarma(y, x, phiLags = c(7), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_8 <- glarma(y, x, phiLags = c(8), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

print(c(extractAIC(glarma_model), extractAIC(glarma_model_2), extractAIC(glarma_model_3), extractAIC(glarma_model_4), extractAIC(glarma_model_5), extractAIC(glarma_model_6), extractAIC(glarma_model_7)))
#lag 6 has lowest AIC, lag 5 has second lowest AIC
summary(glarma_model)
summary(glarma_model_6)

glarma_model_1_2 <- glarma(y, x, phiLags = c(1:2), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_3 <- glarma(y, x, phiLags = c(1:3), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_6 <- glarma(y, x, phiLags = c(1:6), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
print(c(extractAIC(glarma_model), extractAIC(glarma_model_6), extractAIC(glarma_model_1_6)))
summary(glarma_model_1_6)

glarma_model_1_7 <- glarma(y, x, phiLags = c(1:7), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_8 <- glarma(y, x, phiLags = c(1:8), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_9 <- glarma(y, x, phiLags = c(1:9), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_15 <- glarma(y, x, phiLags = c(1:15), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_30 <- glarma(y, x, phiLags = c(1:30), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

glarma_model_30 <- glarma(y, x, phiLags = c(30), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

models <- list(
  glarma_model = glarma_model,
  glarma_model_2 = glarma_model_2,
  glarma_model_3 = glarma_model_3,
  glarma_model_4 = glarma_model_4,
  glarma_model_6 = glarma_model_6,
  glarma_model_30 = glarma_model_30,
  glarma_model_1_2 = glarma_model_1_2,
  glarma_model_1_6 = glarma_model_1_6,
  glarma_model_1_15 = glarma_model_1_15,
  glarma_model_1_30 = glarma_model_1_30
)

for (model_name in names(models)) {
  aic_value <- extractAIC(models[[model_name]])  # [2] extracts the AIC value
  aic_value <- format(aic_value, nsmall = 2)  # Format to 2 decimal places
  print(paste(model_name, "=", aic_value))
}
# grad = 2.22e-16 is a more computationally intensive/"precise" option
```

#### Examine residuals
```{r}
plot_residuals <- function(model, model_name) {
  # Extract residuals and fitted values
  res <- residuals(model)
  fitted_values <- fitted(model)
  
  # Create a data frame for plotting
  residuals_data <- data.frame(
    Fitted = fitted_values,
    Residuals = res
  )
  
    # Residuals vs Fitted Plot
  p1 <- ggplot(residuals_data, aes(x = Fitted, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = paste(model_name, " - Residuals vs Fitted"), 
         x = "Fitted Values", 
         y = "Residuals") +
    theme_minimal()
  
   # Histogram of Residuals
  p2 <- ggplot(residuals_data, aes(x = Residuals)) +
    geom_histogram(binwidth = 0.1, fill = "blue", color = "black", alpha = 0.7) +
    labs(title = paste(model_name, " - Histogram of Residuals"), 
         x = "Residuals", 
         y = "Frequency") +
    theme_minimal()
  
  # QQ Plot
  p3 <- ggplot(residuals_data, aes(sample = Residuals)) +
    geom_qq() +
    geom_qq_line(color = "red") +
    labs(title = paste(model_name, " - QQ Plot of Residuals")) +
    theme_minimal()
  
  # Print plots
  print(p1)
  print(p2)
  print(p3)
}

#glarma_model_1_10 <- glarma(y, x, phiLags = c(1:10), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
#glarma_model_1_11 <- glarma(y, x, phiLags = c(1:11), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_12 <- glarma(y, x, phiLags = c(1:12), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_13 <- glarma(y, x, phiLags = c(1:13), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_1_14 <- glarma(y, x, phiLags = c(1:14), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
```

#### Plot residuals
```{r}
plot_residuals(glarma_model_1_12, "1-12")
plot_residuals(glarma_model_1_13, "1-13")
```
Between lag 1:12 and 1:13 is where the QQ plot and residuals plot changes shape significantly and shifts to a new regime

#### Check lags with day + week/month
```{r}
glarma_model_week <- glarma(y, x, phiLags = c(1, 2), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_2week <- glarma(y, x, phiLags = c(1, 4), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_month <- glarma(y, x, phiLags = c(1, 8), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_2month <- glarma(y, x, phiLags = c(1, 16), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
```

#### Plot it
```{r}
#plot_residuals(glarma_model_week, "day+week")
plot_residuals(glarma_model_2week, "day+2week")
plot_residuals(glarma_model_1_2, "dayTHROUGHweek")
plot_residuals(glarma_model_month, "day+1month")
plot_residuals(glarma_model_1_8, "dayTHROUGHmonth")
plot_residuals(glarma_model_2month, "day+2month")
```

All of these are way worse fits compared to the 1:12 models, at least in terms of residual gaps

```{r}
#plot.glarma(glarma_model) #not good fit
plot.glarma(glarma_model_3)
plot.glarma(glarma_model_week)
plot.glarma(glarma_model_6)
plot.glarma(glarma_model_1_6)
```

### Add thetaLag
```{r}
#glarma_model_theta <- glarma(y, x, thetaLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
extractAIC(glarma_model_theta)
extractAIC(glarma_model)
plot.glarma(glarma_model)
plot.glarma(glarma_model_theta)
```
Very similar fits, phiLag is slightly better AIC so let's just stick with phiLag

### Compare "final" contender models
```{r}
summary(glarma_model_1_3) #one day through 1 week
summary(glarma_model_1_9) #one day through 1 month
summary(glarma_model_1_12) #one day through 5 weeks
summary(glarma_model_1_13) #one day through 6 weeks
summary(glarma_model_1_30) #one through 30

glarma_model_1_end <- glarma(y, x, phiLags = c(1:36), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
extractAIC(glarma_model_1_end)
summary(glarma_model_1_end) #one day ago through "start" of experiment
```
Glarma model with phiLags(1:36) is all around the best fit... but that seems fishy. Ask Chris! And let's move on.

### Let's check the lag values with the state model
```{r}
glarma_model_week <- glarma(y, X, phiLags = c(16), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_2week <- glarma(y, X, phiLags = c(1, 16), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_month <- glarma(y, X, phiLags = c(1:8), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
glarma_model_2month <- glarma(y, X, phiLags = c(1:16), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)

models <- list(
  glarma_model_state = glarma_model,
  glarma_model_week = glarma_model_week,
  glarma_model_2week = glarma_model_2week,
  glarma_model_month = glarma_model_month,
  glarma_model_2month = glarma_model_2month
)

for (model_name in names(models)) {
  aic_value <- extractAIC(models[[model_name]])  # [2] extracts the AIC value
  aic_value <- format(aic_value, nsmall = 2)  # Format to 2 decimal places
  print(paste(model_name, "=", aic_value))
}
#phiLags (1,16) significantly better, 1,2 1,4, 1,8 have equivalent AIC
#phiLags (1:16) is significantly better, 1:8 is also good; 1:2, 1:4 have similar AIC to 1,2 and 1,4
#phiLags (16) is best, 2, 4, 8 have similar AIC values
#comparing "best of the best", 16 and 1, 16 are worst. 1:8 and 1:16 are best

glarma_model_long <- glarma(y, X, phiLags = c(1:36), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
extractAIC(glarma_model_long)
summary(glarma_model_long)

plot_residuals(glarma_model_state, "Lag1")
plot.glarma(glarma_model_state)

glarma_model_state_13<- glarma(y, X, phiLags = c(1:13), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
```

### Plot it
```{r}
plot_residuals(glarma_model_state_13, "Lag13")
plot.glarma(glarma_model_state_13)
plot_residuals(glarma_model_long, "LagAll")
plot.glarma(glarma_model_long)
```

Once you get to lags(1:36), the addition of the state variable doesn't matter (the model AICs are equivalent between phiLag(1:36) for model with state versus model without state) So... where to go from here? Probably compare with summaries. 

### Let's try working with the lagged count data?
```{r}
count_glarma_lag <- all_count %>%
  arrange(date) %>%
  group_by(tank, genet) %>%
  mutate(lag1 = lag(n_true, 1))

count_glarma_mhw <- all_count %>%
  filter(mhw == "during")
  
y2 <- count_glarma_mhw$n_true %>%
  as.matrix()
x2 <- model.matrix(n_true ~ treatment+genet, data = count_glarma_mhw)
glarma_model_mhwonly <- glarma(y2, x2, phiLags = c(1), type = "Poi", method = "FS", residuals = "Pearson", maxit = 100, grad = 1e-6)
plot_residuals(glarma_model_mhwonly, "Lag13")
plot.glarma(glarma_model_mhwonly)
fitted_values <- fitted(glarma_model_mhwonly)
result_df <- data.frame(PredictorVariables = x2, FittedValues = fitted_values) %>%
  filter(fitted_values < 15)

res <- residuals(glarma_model_mhwonly)
fitted_values <- fitted(glarma_model_mhwonly)
  
  # Create a data frame for plotting
residuals_data <- data.frame(
    Fitted = fitted_values,
    Residuals = res
  )

ggplot(residuals_data, aes(x = Fitted, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = paste(model_name, " - Residuals vs Fitted"), 
         x = "Fitted Values", 
         y = "Residuals") +
    theme_bw() 
```

## Count data - something different, GLMM with autocorrelation
```{r}
count_glmm1 <- lme(n_true ~ treatment + genet,
                   random = ~1|tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|tank))
count_glmm2 <- lme(n_true ~ treatment + genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
AIC(count_glmm1, count_glmm2)
summary(count_glmm1)
summary(count_glmm2)

#genet_tank is definitely the correct "random effect" I need to incorporate, especially because it has lower residual standard dev and high autocorrelation (Phi) and MUCH lower AIC

count_glmm3 <- lme(n_true ~ treatment + genet + treatment:genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))

AIC(count_glmm2, count_glmm3)
summary(count_glmm2)
summary(count_glmm3)

#even with added complexity and higher DF, count_glmm3 has a lower AIC suggesting the interaction improves the model fit significantly enough to outweigh the penalty from model complexity.

count_glmm4 <- lme(n_true ~ treatment + genet + treatment:genet, #same syntax as treatment*genet
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1())
AIC(count_glmm4, count_glmm3) #same AIC
```

### Check residuals
```{r}
residual_values <- residuals(count_glmm3)
fitted_values <- fitted(count_glmm3)

plot(fitted_values, residual_values,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs. Fitted Values",
     pch = 19,
     col = "blue")

# Add a horizontal line at 0 for reference
abline(h = 0, col = "red", lty = 2)

qqnorm(residual_values)
qqline(residual_values, col = "red")
title("QQ Plot of Residuals")
```
Residual plot looks off...

### Simpler models
```{r}
#count_temp <- all_count %>%
#  mutate(week = as.numeric(difftime(date, min(weekly_temp$friday), units = "weeks")) + 1,
#         tank = as.numeric(tank)) %>%
#  #select(date, week, tank, mhw, genet, avg_size_log) %>%
#  left_join(weekly_temp, by = c("date" = "friday", "tank"))

count_glmm5 <- lme(n_true ~ treatment,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
count_glmm6 <- lme(n_true ~ genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
summary(count_glmm5)
summary(count_glmm6)
AIC(count_glmm4, count_glmm5, count_glmm6) #Treatment only is the worst AIC. so, treatment alone isn't best predictor.
BIC(count_glmm4, count_glmm5, count_glmm6) #Treatment only is the worst AIC. so, treatment alone isn't best predictor.

count_glmm7 <- lme(n_true ~ treatment*genet + mhw,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
AIC(count_glmm7) #adding during vs after MHW (state) is worse 
summary(count_glmm7)

count_glmm8 <- lme(n_true ~ treatment*mhw,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corAR1(form = ~ 1|genet_tank))
AIC(count_glmm8) #bad
```

At least for the GLMM, the treatment*genet with genet_tank random effect is best performing.

### Let's super simplify by only looking at during MHW
```{r}
all_count_mhw <- all_count %>%
  filter(mhw == "during")

count_glmm10 <- lme(n_true ~ treatment,
                   random = ~1|genet_tank,
                   data = all_count_mhw,
                   correlation = corAR1(form = ~ 1|genet_tank))

count_glmm11 <- lme(n_true ~ genet,
                   random = ~1|genet_tank,
                   data = all_count_mhw,
                   correlation = corAR1(form = ~ 1|genet_tank))

count_glmm12 <- lme(n_true ~ treatment*genet,
                   random = ~1|genet_tank,
                   data = all_count_mhw,
                   correlation = corAR1(form = ~ 1|genet_tank))

AIC(count_glmm10, count_glmm11, count_glmm12) #treatment alone still the worst
BIC(count_glmm10, count_glmm11, count_glmm12) #treatment alone still the worst

summary(count_glmm11)
summary(count_glmm12)
```
Treatment alone is the WORST, genet alone doesn't make a lot of sense especially when plotting the faceted data and my understanding of the experiment, so let's move forward with treatmentxgenet interaction model. 

### Let's try other correlation options
```{r}
autoArima <- auto.arima(all_count$n_true, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(autoArima)

count_glmm13 <- lme(n_true ~ treatment*genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 1, q = 1))

count_glmm14 <- lme(n_true ~ treatment*genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 4, q = 1))

summary(count_glmm13)
summary(count_glmm14)
AIC(count_glmm13, count_glmm14, count_glmm4) #4 worst, 14 best
BIC(count_glmm13, count_glmm14, count_glmm4) #4 worst, 14 best

count_glmm15 <- lme(n_true ~ treatment*genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corCAR1(form = ~ day_numeric | genet_tank))
summary(count_glmm15)
AIC(count_glmm14, count_glmm15) #corARMA better
BIC(count_glmm14, count_glmm15) #corARMA better
```

### Let's look at "final" GLMM options with corARMA
```{r}
count_glmm14 <- lme(n_true ~ treatment*genet,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 4, q = 1))

count_glmm16 <- lme(n_true ~ treatment*genet + mhw,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 4, q = 1))

count_glmm17 <- lme(n_true ~ treatment*genet + mhw:treatment,
                   random = ~1|genet_tank,
                   data = all_count,
                   correlation = corARMA(p = 4, q = 1),
                   control = lmeControl(maxIter = 100, msMaxIter = 100))
summary(count_glmm14)
summary(count_glmm16)
summary(count_glmm17)
AIC(count_glmm14, count_glmm16, count_glmm17) 
BIC(count_glmm14, count_glmm16, count_glmm17)
```
Adding in the "state" of the marine heatwave makes things worse all around / mhw isn't significant. Which I guess makes sense since even in the control the curves flattened out?


### Let's plot something up for the GLMM
```{r}
all_count_plot <- all_count %>%
  dplyr::select(date, tank, genet, genet_tank, treatment, n_true) %>%
  group_by(date, treatment, genet) %>%
  summarize(mean = mean(n_true),
            sd = sd(n_true),
            se = sd(n_true)/sqrt(length(n_true)),
            ci = qt(0.95, length(n_true)-1)*se,
            lower = mean - ci,
            upper = mean + ci) %>%
  dplyr::select(date, treatment, genet, mean, sd, lower, upper)


ggplot(all_count_plot, aes(x=date,color=genet)) +
  geom_point(aes(y=mean)) +
  #geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +  # Error bars
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = genet), alpha = 0.2, color = NA) +  # Cloud effect
  facet_wrap(~treatment) +
  labs(x = "Date",
       y = "Count")

# Calculate residuals
all_count$residuals <- residuals(count_glmm14)

# Plot residuals
ggplot(all_count, aes(x = predicted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Predicted Values", y = "Residuals") +
  theme_minimal()

ggplot(all_count, aes(x = treatment, y = n_true, color = genet)) +
  geom_point(position = position_jitter(width = 0.2)) +
  stat_summary(fun = mean, geom = "line", aes(group = genet)) +
  labs(x = "Treatment", y = "n_true") +
  theme_minimal()
```

#### We might need a nonlinear regression, the curves are logistic
```{r}
glmer1 <- glmer.nb(n_true ~ treatment*genet + (1|genet_tank), 
                data = all_count)

summary(glmer1)
#Well that is terrible.
```

### Check dispersion
```{r}
all_count_dispersion <- all_count %>%
  group_by(genet_tank) %>%
  summarize(mean = mean(n_true),
            var = var(n_true),
            ratio = var/mean)

all_count_dispersion_genet <- all_count %>%
  group_by(genet) %>%
  summarize(mean = mean(n_true),
            var = var(n_true),
            ratio = var/mean)
```
Actually, if the data is organized by individual groupings (genet_tank OR genet only) it's not actually that overdispersed. Some of them are actually underdispersed. Let's keep this in mind for model interpretation. 

## Plot up count data for WSN 
### Treatment not significant
```{r}
all_count_treatment <- all_count %>%
  dplyr::select(date, tank, genet, genet_tank, treatment, n_true) %>%
  group_by(date, treatment) %>%
  summarize(mean = mean(n_true),
            sd = sd(n_true),
            se = sd(n_true)/sqrt(length(n_true)),
            ci = qt(0.95, length(n_true)-1)*se,
            lower = mean - ci,
            upper = mean + ci) %>%
  dplyr::select(date, treatment, mean, sd, lower, upper)

image <- ggplot(all_count_treatment, aes(x=date,color=treatment)) +
  geom_vline(xintercept = as.numeric(as.POSIXct("2023-12-12")), linetype = "dashed", color = "black", alpha = 0.8) +
  geom_smooth(aes(y=mean)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = treatment), alpha = 0.2, color = NA) +  # Cloud effect
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +  
  scale_fill_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +
  scale_x_datetime(date_labels = "%b %d", 
                   breaks= seq(min(all_count_treatment$date), max(all_count_treatment$date), length=7)) +
  labs(x = "Date",
       y = "Number of polyps",
       color = "") +
  guides(fill = "none",
         color=guide_legend(override.aes=list(fill=NA))) +
  theme_bw() +
  theme(text = element_text(size = 20),
        legend.position = "top")

#ggsave(file = here("experiment", "figures", "WSN","count_mhw.svg"), plot = image, width=12, height=7)
```


```{r}
ggplot(all_count_plot, aes(x=date,color=treatment)) +
  geom_vline(xintercept = as.numeric(as.POSIXct("2023-12-12")), linetype = "dashed", color = "black", alpha = 0.8) +
  geom_smooth(aes(y=mean), span = 0.5) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = genet), alpha = 0.2, color = NA) +  # Cloud effect
  scale_color_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  scale_fill_manual(values = c("A" = "#9370DB", "B" = "#C21B78", "C" = "#FF9933", "D" ="#FF3333", "E" = "#662B45")) +
  facet_wrap(~treatment,
             labeller =  labeller(treatment = c("cold" = "Control", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_x_datetime(date_labels = "%b %d", 
                   breaks= seq(min(all_count_plot$date), max(all_count_plot$date), length=7)) +
  labs(x = "Date",
       y = "Number of polyps",
       color = "Genet") +
  guides(fill = "none",
         color=guide_legend(override.aes=list(fill=NA))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 20),
        legend.position = "top")

#ggsave(file = here("experiment", "figures", "WSN","count_treatment_genet.svg"), plot = image, width=12, height=7)

image <- ggplot(all_count_plot, aes(x=date,color=treatment)) +
  geom_vline(xintercept = as.numeric(as.POSIXct("2023-12-12")), linetype = "dashed", color = "black", alpha = 0.8) +
  geom_smooth(aes(y=mean), span = 0.5) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = treatment), alpha = 0.2, color = NA) +  # Cloud effect
  scale_color_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +  
  scale_fill_manual(values = c("cold" = "#0072B2", "severe" = "#E69F00", "extreme" = "#D55E00"),
                     labels = c("cold" = "Ambient", "severe" = "Severe MHW", "extreme" = "Extreme MHW")) +
  facet_wrap(~genet,
             scales = "free_y",
             labeller =  labeller(treatment = c("cold" = "Control", "severe" = "Severe MHW", "extreme" = "Extreme MHW"))) +
  scale_x_datetime(date_labels = "%b %d", 
                   breaks= seq(min(all_count_plot$date), max(all_count_plot$date), length=7)) +
  labs(x = "Date",
       y = "Number of polyps",
       color = "") +
  guides(fill = "none",
         color=guide_legend(override.aes=list(fill=NA))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 20),
        legend.position = "top")

#ggsave(file = here("experiment", "figures", "WSN","count_genet_treatment.svg"), plot = image, width=12, height=7)
```


```{r}
ggplot(all_count, aes(x = treatment, y = n_true, color = genet)) +
  geom_point(position = position_jitter(width = 0.2)) +
  stat_summary(fun = mean, geom = "line", aes(group = genet)) +
  labs(x = "Treatment", y = "n_true") +
  theme_minimal()
```


## Body size data - ARIMA forecasting
https://stats.stackexchange.com/questions/281666/how-does-acf-pacf-identify-the-order-of-ma-and-ar-terms
https://stats.stackexchange.com/questions/32634/difference-time-series-before-arima-or-within-arima

### Log transform data
```{r}
# scatter plot of transformed data
ggplot(all_size, aes(x=date, y=avg_size_log)) +
  geom_point(aes(color=treatment)) +
  labs(x = "Average body size (mm)",
       y = "Frequency")
# histogram of transformed data
ggplot(all_size, aes(x=avg_size_log)) +
  geom_histogram(binwidth=0.1) +
  labs(x = "log(Average body size (mm))",
       y = "Frequency")
# transformed data density plot
ggplot(all_size, aes(x = avg_size_log)) +
  geom_density() +
  ggtitle("Density Plot of Transformed Data")
# Q-Q plot of log-transformed data
qqnorm(all_size$avg_size_log)
qqline(all_size$avg_size_log, col = "red")
```
It's not perfectly normal but definitely better log-transformed. Looks a bit like exponential decay, so let's make the data stationary by differencing.

### Difference to make the data stationary
```{r}
ggplot(all_size, aes(x=date)) +
  geom_point(aes(y=avg_size_diff), color="black", alpha=0.3) +
  geom_point(aes(y=avg_size_diff2), color="blue", alpha=0.3) +
  geom_smooth(aes(y=avg_size_diff), method="lm", color="black") +
  geom_smooth(aes(y=avg_size_diff2), method="lm", color="blue") +
  labs(x = "Average body size (mm)",
       y = "Frequency")
```

### Plot a slightly more complex vis of the first differenced data
```{r, fig.width=10}
# Fit linear models and extract coefficients
models <- all_size %>%
  group_by(treatment, genet, mhw) %>%
  do(tidy(lm(avg_size_diff ~ date, data = .))) %>%
  select(treatment, genet, term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  mutate(equation = sprintf("y = %.2f + %.2e * x", `(Intercept)`, date)) %>%
  rename(slope = date) %>%
  ungroup() %>%
  group_by(mhw) %>%
  mutate(x_axis = as.POSIXct(ifelse(mhw == "during", "2023-10-15", "2024-01-15")),
         y_axis = rep(seq(from = 0.2, to = 0.5, length.out = 5), length.out = n())) %>%
  ungroup() %>%
  select(-mhw)

# Join regression equations with the original data
all_size_reg <- all_size %>%
  left_join(models, by = c("treatment", "genet"), relationship = "many-to-many")

ggplot(all_size_reg) +
  geom_point(aes(x=date, y=avg_size_diff, color=genet), alpha=0.3) +
  geom_smooth(aes(x=date, y=avg_size_diff, color=genet ,linetype = mhw), method="lm") +
  facet_wrap(~treatment) +
  geom_label_repel(data = models, aes(x=x_axis, y=y_axis, label = equation, color = genet), size = 3) +
  labs(x = "Date",
       y = "Differenced average body size (mm)") +
  theme_bw()

#ggsave(here("experiment", "figures", "body_size_regression.png"), width=15, height=7)
```

### Run ADF and KPSS tests on first and second differenced data
```{r}
# List to store results
adf_results <- list()

# Loop through each group
for (group in unique(all_size$treatment)) {
  group_data <- all_size %>%
    filter(treatment == group,
           !is.na(avg_size_diff2)) %>%
    pull(avg_size_diff2) #chnage this to avg_size_diff for first differenced data
  # Perform the ADF test
  adf_test <- adf.test(group_data, alternative = "stationary")
  # Store the result
  adf_results[[group]] <- adf_test
}

# Print results
adf_results

# List to store results
kpss_results <- list()
# Loop through each group
for (group in unique(all_size$treatment)) {
  group_data <- all_size %>%
    filter(treatment == group,
           !is.na(avg_size_diff2)) %>%
    pull(avg_size_diff2) #chnage this to avg_size_diff for first differenced data
  # Perform the KPSS test
  kpss_test <- ur.kpss(group_data, type = "mu")  # Use "tau" if you expect trend stationarity
  # Store the result
  kpss_results[[group]] <- summary(kpss_test)
}

# Print results
kpss_results
```
Both p-value is less than 0.01 and test statistic is lower than critical values, so second-order differenced data is now almost certainly stationary. First-order differencing only showed strong evidence for stationarity with the ADF test.

### Plot ACF and PACF for the log-transformed body size data
```{r}
Acf(all_size$avg_size_log, main = "ACF of Log-transformed Series")
Pacf(all_size$avg_size_log, main = "PACF of Log-transformed Series")

Acf(all_size$avg_size_diff, main = "ACF of Differenced Series")
Pacf(all_size$avg_size_diff, main = "PACF of Differenced Series")

Acf(all_size$avg_size_diff2, main = "ACF of 2-Differenced Series")
Pacf(all_size$avg_size_diff2, main = "PACF of 2-Differenced Series")
```
There is definitely a lot of autocorrelation with the data, for both 1-diff and 2-diff. Makes sense because body size should be highly correlated over time.

### Remove NA values and convert factors to numeric
```{r}
all_size_na <- all_size %>%
  filter(!is.na(avg_size_diff)) %>%
  mutate(genet = as.numeric(genet),
         treatment = as.numeric(treatment),
         mhw = as.numeric(mhw)) %>%
  select(date, tank, genet, n, treatment, mhw, avg_size, avg_size_log, avg_size_diff, avg_size_diff2)
```

### First-differenced data

#### Auto ARIMA - basic
```{r}
autoArima <- auto.arima(all_size_na$avg_size_diff, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(autoArima)
```
I ran this all again with second-differenced data, and the model doesn't appear significantly better (in some cases like BIC, not better) and so it doesn't justify the added complexity of a second-differenced model. so let's stick with first-differenced model.

#### Check model residuals
```{r}
residuals_auto <- residuals(autoArima)
plot(residuals_auto, main = "Residuals of AUTOSARIMA Model")
Acf(residuals_auto, main = "ACF of Residuals")
Box.test(residuals_auto, type = "Ljung-Box")

adf_test_residuals <- adf.test(residuals_auto, alternative = "stationary") #definitely stationary
qqnorm(residuals_auto)
qqline(residuals_auto)
shapiro.test(residuals_auto)

ggplot() +
  geom_histogram(aes(x = residuals_auto), bins = 20, color = "black") +
  labs(x = "Residuals",
       y = "Frequency")

fitted_values <- fitted(autoArima)
plot(fitted_values, residuals_auto, main="Residuals vs Fitted Values")
abline(h=0, col="red")
```
2-order differencing is not significantly better than 1-order differencing so let's stick with 1-order. 

AR(1) model: This is autoregressive data, and the current value of the time series depends linearly on its immediately preceding value - makes sense because body size shouldn't change that drastically week by week. 
I(1): First-order differencing applied to make the series stationary - which makes sense because preliminary analyses showed slope of zero with second-differenced data. 
MA(2): current value of the time series depends on the last two periods’ forecast errors - means that the current value of the series depends on the errors made in the previous two periods, which doesn't make a ton of intuitive sense except maybe if there are big changes to body size (i.e. reproduction?) that just means we won't see those changes finish until 2 weeks later? <<< CHECK THIS LOGIC.

#### Check ts trends/seasonality
```{r}
all_size_ts <- ts(all_size_na$avg_size_diff, frequency = 52) #weekly data as ts object
class(all_size_ts)
summary(all_size_ts)
acf(all_size_ts)
pacf(all_size_ts)

decomposition <- stl(all_size_ts, s.window = "periodic")
plot(decomposition)
```
There's no apparent seasonality and a slight positive trend in the data

#### Fit ARIMA with different I values
```{r}
# Fit ARIMA with different seasonal orders
model1 <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 1))
model2 <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2))
model3 <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 3))

Acf(residuals(model1), main = paste("ACF of Residuals model1"))
Pacf(residuals(model1), main = paste("PACF of Residuals model1"))

Acf(residuals(model2), main = paste("ACF of Residuals model2"))
Pacf(residuals(model2), main = paste("PACF of Residuals model2"))

Acf(residuals(model3), main = paste("ACF of Residuals model3"))
Pacf(residuals(model3), main = paste("PACF of Residuals model3"))

Box.test(residuals(model1), lag = 20, type = "Ljung-Box")
Box.test(residuals(model2), lag = 20, type = "Ljung-Box")
Box.test(residuals(model3), lag = 20, type = "Ljung-Box")
```
I(2) is definitely the best fit. 

#### Consider potential covariates
```{r}
#create matrix of dummy variables incorporating all potential covariates
xreg_All <- model.matrix(~ treatment + genet + mhw - 1, data = all_size_na)
xreg_NoGenet <- model.matrix(~ treatment + mhw - 1, data = all_size_na)
xreg_MHW <- model.matrix(~ mhw - 1, data = all_size_na)
xreg_Treatment <- model.matrix(~ treatment - 1, data = all_size_na)

arima_covariates_all <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2), xreg = xreg_All)
summary(arima_covariates_all)

arima_covariates_noGenet <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2), xreg = xreg_NoGenet)
summary(arima_covariates_noGenet)

arima_covariates_mhw <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2), xreg = xreg_MHW)
summary(arima_covariates_mhw)

arima_covariates_treatment <- Arima(all_size_na$avg_size_diff, order = c(1, 1, 2), xreg = xreg_Treatment)
summary(arima_covariates_treatment)
```
Looks like genet is not a significant covariate, but treatment and during/after MHW are slightly impactful. <<< DISCUSS WITH CHRIS.

Auto-ARIMA with Covariates model (check AR, I, MA)
```{r}
xreg_Covariate <- model.matrix(~ genet + mhw - 1, data = all_size_na)

autoArima_Covariate <- auto.arima(all_size_na$avg_size_diff, seasonal = TRUE, stepwise = FALSE, approximation = FALSE, xreg = xreg_Covariate)
summary(autoArima_Covariate)
```
No change to AR, I, or MA with any of the potential covariates.

*So, for first-differenced data, here are the two model options*
#### 
Basic ARIMA(1,1,2): 
Coefficients:
         ar1      ma1     ma2
      0.7682  -1.5296  0.5354
s.e.  0.0518   0.0662  0.0646

sigma^2 = 0.002058:  log likelihood = 2258.51
AIC=-4509.02   AICc=-4508.99   BIC=-4488.19

                      ME       RMSE        MAE MPE MAPE      MASE         ACF1
Training set 0.001281871 0.04529954 0.03479757 Inf  Inf 0.7851294 -0.009148644

####
####
ARIMA(1,1,2) with MHW/Treatment covariates:
Coefficients:
         ar1      ma1     ma2  treatment     mhw
      0.7661  -1.5293  0.5351    -0.0029  0.0012
s.e.  0.0539   0.0682  0.0662     0.0025  0.0155

sigma^2 = 0.002059:  log likelihood = 2259.17
AIC=-4506.35   AICc=-4506.28   BIC=-4475.1

                      ME       RMSE        MAE MPE MAPE      MASE         ACF1
Training set 0.001324029 0.04527702 0.03477904 NaN  Inf 0.7847113 -0.008522662
####

#### Check model residuals for MHW/Treatment ARIMA
```{r}
residuals_auto <- residuals(arima_covariates_noGenet)
plot(residuals_auto, main = "Residuals of AUTOSARIMA Model")
Acf(residuals_auto, main = "ACF of Residuals")
Box.test(residuals_auto, type = "Ljung-Box")

adf_test_residuals <- adf.test(residuals_auto, alternative = "stationary") #definitely stationary
qqnorm(residuals_auto)
qqline(residuals_auto)
shapiro.test(residuals_auto)

ggplot() +
  geom_histogram(aes(x = residuals_auto), bins = 20, color = "black") +
  labs(x = "Residuals",
       y = "Frequency")

fitted_values <- fitted(arima_covariates_noGenet)
plot(fitted_values, residuals_auto, main="Residuals vs Fitted Values")
abline(h=0, col="red")
```

### Now, let's check out ARIMA models on second-differenced data (since it is technically more stationary)
```{r}
autoArima2 <- auto.arima(all_size_na$avg_size_diff2, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(autoArima2)
```

AR(1) model: This is autoregressive data, and the current value of the time series depends linearly on its immediately preceding value - makes sense because body size shouldn't change that drastically week by week. 
I(0): No differencing applied to make the series stationary - which makes sense because the body size data has been differenced twice which we know makes it stationary.
MA(4): current value of the time series depends on the last FOUR periods’ forecast errors - means that the current value of the series depends on the errors made in the previous four periods. Greater value than first-differenced data model MA(2). Meaning??

#### Check model residuals
```{r}
residuals_auto2 <- residuals(autoArima2)
plot(residuals_auto2, main = "Residuals of AUTOSARIMA Model")
Acf(residuals_auto2, main = "ACF of Residuals")
Box.test(residuals_auto2, type = "Ljung-Box")

adf_test_residuals2 <- adf.test(residuals_auto2, alternative = "stationary") #definitely stationary
qqnorm(residuals_auto2)
qqline(residuals_auto2)
shapiro.test(residuals_auto2)

ggplot() +
  geom_histogram(aes(x = residuals_auto2), bins = 20, color = "black") +
  labs(x = "Residuals",
       y = "Frequency")

fitted_values2 <- fitted(autoArima2)
plot(fitted_values2, residuals_auto2, main="Residuals vs Fitted Values")
abline(h=0, col="red")
```
Shapiro-Wilks test says residuals are close to normal distribution but not normally distributed... but the plots look fine.

#### Consider potential covariates
```{r}
#create matrix of dummy variables incorporating all potential covariates
xreg_All <- model.matrix(~ treatment + genet + mhw - 1, data = all_size_na)
xreg_NoGenet <- model.matrix(~ treatment + mhw - 1, data = all_size_na)
xreg_NoTreatment <- model.matrix(~ genet + mhw - 1, data = all_size_na)
xreg_NoMHW <- model.matrix(~ treatment + genet - 1, data = all_size_na)
xreg_MHW <- model.matrix(~ mhw - 1, data = all_size_na)
xreg_Treatment <- model.matrix(~ treatment - 1, data = all_size_na)
xreg_Genet <- model.matrix(~ genet - 1, data = all_size_na)

arima_no_covariates2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4))
summary(arima_no_covariates2)

arima_covariates_all2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_All)
summary(arima_covariates_all2)

arima_covariates_noGenet2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_NoGenet)
summary(arima_covariates_noGenet2)

arima_covariates_noTreatment2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_NoTreatment)
summary(arima_covariates_noTreatment2)

arima_covariates_noMHW2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_NoMHW)
summary(arima_covariates_noMHW2)

arima_covariates_Genet2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_Genet)
summary(arima_covariates_Genet2)

arima_covariates_mhw2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_MHW)
summary(arima_covariates_mhw2)

arima_covariates_treatment2 <- Arima(all_size_na$avg_size_diff2, order = c(1, 0, 4), xreg = xreg_Treatment)
summary(arima_covariates_treatment2)
```
TreatmentxGenetxMHW model performs the worst. TreatmentxMHW, GenetxMHW, TreatmentxGenet perform second worst. No covariates performs best, Treatment alone, Genet alone, and MHW alone all perform second best. This probably means we should use no covariates in second-differenced data model?

Auto-ARIMA with Covariates model (check AR, I, MA)
```{r}
xreg_Covariate2 <- model.matrix(~ treatment + genet + mhw - 1, data = all_size_na)

autoArima_Covariate2 <- auto.arima(all_size_na$avg_size_diff2, seasonal = TRUE, stepwise = FALSE, approximation = FALSE, xreg = xreg_Covariate2)
summary(autoArima_Covariate2)
```
Same as first-differenced data, no change to AR, I, or MA with any of the potential covariates.

*For second-differenced data, here is the apparent best model option*
####
Series: all_size_na$avg_size_diff2 
ARIMA(1,0,4) with zero mean 
Coefficients:
         ar1      ma1     ma2      ma3      ma4
      0.9289  -0.7029  0.0217  -0.0357  -0.1054
s.e.  0.0412   0.0506  0.0356   0.0353   0.0303

sigma^2 = 0.005237:  log likelihood = 1541.35
AIC=-3070.71   AICc=-3070.64   BIC=-3039.8

Training set error measures:
                       ME      RMSE        MAE       MPE    MAPE      MASE
Training set 0.0007083893 0.0722271 0.05541204 -20.12162 319.208 0.7668489
                    ACF1
Training set 0.000179781
#### 

### Plot differenced data with O.G. values

#### Genet only
```{r, fig.width=10}
# Create the plots
plot_undifferenced <- ggplot(all_size_na, aes(x = date, y = avg_size_log, group=as.factor(genet))) +
  geom_point(aes(color=as.factor(genet)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(genet)), method="lm") +
  ggtitle("Undifferenced Data") +
  xlab("Date") +
  ylab("Log Body size (mm)") +
  theme_bw()

plot_differenced <- ggplot(all_size_na, aes(x = date, y = avg_size_diff, group=as.factor(genet))) +
  geom_point(aes(color=as.factor(genet)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(genet)), method="lm") +
  ggtitle("Differenced Data") +
  xlab("Date") +
  ylab("Differenced Body Size (mm)") +
  theme_bw()

plot_differenced2 <- ggplot(all_size_na, aes(x = date, y = avg_size_diff2, group=as.factor(genet))) +
  geom_point(aes(color=as.factor(genet)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(genet)), method="lm") +
  ggtitle("2-Differenced Data") +
  xlab("Date") +
  ylab("2-Differenced Body Size (mm)") +
  theme_bw()

combined_plot <- grid.arrange(plot_undifferenced, plot_differenced, plot_differenced2, ncol = 3)

#ggsave(here("experiment", "figures", "differenced_comparison_genet.png"), combined_plot, width=14, height=7)
```
Well damn, that makes sense why genet is not a significant covariate in the differenced data models! 

#### Treatment only
```{r, fig.width=10}
# Create the plots
plot_undifferenced <- ggplot(all_size_na, aes(x = date, y = avg_size_log, group=as.factor(treatment))) +
  geom_point(aes(color=as.factor(treatment)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(treatment)), method="lm") +
  ggtitle("Undifferenced Data") +
  xlab("Date") +
  ylab("Log Body size (mm)") +
  theme_bw()

plot_differenced <- ggplot(all_size_na, aes(x = date, y = avg_size_diff, group=as.factor(treatment))) +
  geom_point(aes(color=as.factor(treatment)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(treatment)), method="lm") +
  ggtitle("Differenced Data") +
  xlab("Date") +
  ylab("Differenced Body Size (mm)") +
  theme_bw()

plot_differenced2 <- ggplot(all_size_na, aes(x = date, y = avg_size_diff2, group=as.factor(treatment))) +
  geom_point(aes(color=as.factor(treatment)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(treatment)), method="lm") +
  ggtitle("2-Differenced Data") +
  xlab("Date") +
  ylab("2-Differenced Body Size (mm)") +
  theme_bw()

combined_plot <- grid.arrange(plot_undifferenced, plot_differenced, plot_differenced2, ncol = 3)

#ggsave(here("experiment", "figures", "differenced_comparison_treatment.png"), combined_plot, width=14, height=7)
```
No difference in linear slope between three treatments

#### MHW only
```{r, fig.width=10}
# Create the plots
plot_undifferenced <- ggplot(all_size_na, aes(x = date, y = avg_size_log, group=as.factor(mhw))) +
  geom_point(aes(color=as.factor(mhw)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(mhw)), method="lm") +
  ggtitle("Undifferenced Data") +
  xlab("Date") +
  ylab("Log Body size (mm)") +
  theme_bw()

plot_differenced <- ggplot(all_size_na, aes(x = date, y = avg_size_diff, group=as.factor(mhw))) +
  geom_point(aes(color=as.factor(mhw)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(mhw)), method="lm") +
  ggtitle("Differenced Data") +
  xlab("Date") +
  ylab("Differenced Body Size (mm)") +
  theme_bw()

plot_differenced2 <- ggplot(all_size_na, aes(x = date, y = avg_size_diff2, group=as.factor(mhw))) +
  geom_point(aes(color=as.factor(mhw)), alpha=0.3) +
  geom_smooth(aes(color=as.factor(mhw)), method="lm") +
  ggtitle("2-Differenced Data") +
  xlab("Date") +
  ylab("2-Differenced Body Size (mm)") +
  theme_bw()

combined_plot <- grid.arrange(plot_undifferenced, plot_differenced, plot_differenced2, ncol = 3)

#ggsave(here("experiment", "figures", "differenced_comparison_mhw.png"), combined_plot, width=14, height=7)
```
MHW slope is definitely different in undifferenced data.

### Use basic first-differenced ARIMA to forecast.
```{r}
future_predictions <- forecast(autoArima, h = 50)  # Forecast h periods ahead

plot(future_predictions)
```

Just for conceptual understanding, create a quick autoARIMA model on undifferenced data and plot forecast
```{r}
autoArima_undiff <- auto.arima(all_size_na$avg_size_log, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(autoArima_undiff)

future_predictions <- forecast(autoArima_undiff, h = 50)  # Forecast h periods ahead
plot(future_predictions)
```

### TL;DR First Differenced AutoARIMA model = ARIMA(1,1,2)

## Body size data - MARSS, what we should have been doing all along...
https://atsa-es.github.io/MARSS/

### Prep data for MARSS model
```{r}
start_date <- min(weekly_temp$friday)

all_size_marss <- all %>%
  filter(!is.na(avg_size)) %>%
  mutate(week = as.numeric(difftime(date, start_date, units = "weeks")) + 1,
         avg_size_log = log(avg_size),
         sd_size_log = log(sd_size),
         tank = as.numeric(tank)) %>%
  select(date, week, tank, mhw, genet, avg_size_log) %>%
  left_join(weekly_temp, by = c("date" = "friday", "tank"))

marss_data <- all_size_marss %>%
  mutate(genet_tank = paste(genet, tank, sep = "_"))

marss_data_wide <- marss_data %>%
  select(-date, -tank, -genet, -mhw, -treatment, -avg_temp, -min_temp, -max_temp) %>%
  pivot_wider(names_from = genet_tank, values_from = avg_size_log) %>%
  column_to_rownames(var = "week") %>%
  as.matrix() %>%
  t()

marss_data_trimmed <- marss_data_wide[1:(nrow(marss_data_wide) - 50), 1:(ncol(marss_data_wide) - 6)]

#create temperature matrix as covariate
temp_covariate <- marss_data %>%
  select(week, avg_temp, genet_tank) %>%
  pivot_wider(names_from = genet_tank, values_from = avg_temp) %>%
  column_to_rownames(var = "week") %>%
  as.matrix() %>%
  t()

temp_covariate[duplicated(temp_covariate),]

temp_covariate_trimmed <- temp_covariate[1:(nrow(temp_covariate) - 50), 1:(ncol(temp_covariate) - 6)]

```

### Fit MARSS model
```{r}
model_list1 <- list(
  B = "identity",
  U = "unequal",
  Q = "diagonal and equal",
  Z = "identity",
  A = "scaling",
  R = "diagonal and equal"
)
fit1 <- MARSS(marss_data_trimmed, model=model_list1, method="BFGS")
tidy(fit1)
autoplot(fit1)

model_list2 <- list(
  B = "identity",
  U = "unequal",
  Q = "diagonal and equal",
  Z = "identity",
  A = "scaling",
  R = "diagonal and equal",
  C = "unconstrained",
  c = temp_covariate
)
fit2 <- MARSS(marss_data_wide, model=model_list2, method="BFGS")
tidy(fit2)
autoplot(fit2)

model_list3 <- list(
  B = "identity", # State transition; "identity" because body size is autocorrelated
  U = "unequal", # "unequal" because expect different genets/treatments respond differently to external factors
  Q = "diagonal and unequal", # Process noise; expect variability in body size differs between genets
  Z = "identity", # Measurement matrix; measuring body size directly without transformation
  A = "scaling", # Initial states; not "zero" because initial body size differed between genets
  R = "diagonal and equal", # Observation noise; expect measurement error would not change between genets
  C = "unconstrained",
  c = temp_covariate
)
fit3 <- MARSS(marss_data_wide, model=model_list3, method="BFGS")
tidy(fit3)
autoplot(fit3)

c(AIC(fit1), AIC(fit2), AIC(fit3))
```

### Let's move on to much simpler models - from ATSA plankton data
```{r}
the.mean <- apply(marss_data_trimmed, 1, mean, na.rm = TRUE)
the.sigma <- sqrt(apply(marss_data_trimmed, 1, var, na.rm = TRUE))
marss_data_trimmed <- (marss_data_trimmed - the.mean) * (1/the.sigma)

the.mean <- apply(temp_covariate_trimmed, 1, mean, na.rm = TRUE)
the.sigma <- sqrt(apply(temp_covariate_trimmed, 1, var, na.rm = TRUE))
temp_covariate_trimmed <- (temp_covariate_trimmed - the.mean) * (1/the.sigma)

#Observation error only model
Q <- U <- x0 <- "zero"
B <- Z <- "identity"
d <- temp_covariate_trimmed
A <- "zero"
D <- "unconstrained"
y <- marss_data_trimmed  # to show relationship between dat & the equation
model.list1 <- list(B = B, U = U, Q = Q, Z = Z, A = A, D = D, 
    d = d, x0 = x0)
kem <- MARSS(y, model = model.list1, method="BFGS")

#Process error only model
R <- A <- U <- "zero"
B <- Z <- "identity"
Q <- "equalvarcov"
C <- "unconstrained"
model.list2 <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
    C = C, c = temp_covariate_trimmed)
kem2 <- MARSS(marss_data_trimmed, model = model.list2, method="BFGS")

R <- A <- U <- "zero"
B <- Z <- "identity"
Q <- "equalvarcov"
C <- "unconstrained"
c <- temp_covariate_trimmed
y <- marss_data_trimmed
model.list2 <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
    C = C, c = c)
kem2 <- MARSS(y, model = model.list2)

#Process and Observation error model
D <- R <- d <- A <- U <- "zero"
Z <- "identity"
B <- "diagonal and unequal"
Q <- "equalvarcov"
C <- "unconstrained"
c <- covariates
x0 <- "unequal"
tinitx <- 1
model.list3 <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
    D = D, d = d, C = C, c = c, x0 = x0, tinitx = tinitx)
kem3 <- MARSS(marss_data_wide, model = model.list3)

kem_default <- MARSS(marss_data_trimmed)
kem_bfgs <- MARSS(marss_data_trimmed, method="BFGS")
#correlated hidden state processes
kem_unconstrained <- MARSS(marss_data_trimmed, model=list(Q = "unconstrained"), control = list(safe = TRUE, trace=1))
#equally correlated hidden state processes
kem_equalvar <- MARSS(marss_data_trimmed, model=list(Q = "equalvarcov"))
```


### Monte Carlo parameter estimation
```{r}
MARSSmcinit <- function(MLEobj,
                        control = list(
                          numInits = 500, numInitSteps = 10,
                          MCbounds = list(
                            B = c(0, 1), U = c(-1, 1), Q = c(1, 1),
                            Z = c(0, 1), A = c(-1, 1), R = c(1, 1), x0 = c(-1, 1)
                          )
                        ),
                        silent = FALSE) {
  control.default <- list(numInits = 500, numInitSteps = 10, MCbounds = list(B = c(0, 1), U = c(-1, 1), Q = c(1, 1), Z = c(0, 1), A = c(-1, 1), R = c(1, 1), x0 = c(-1, 1)))
  if (!is.null(control)) {
    if (!is.list(control)) stop("MARSSmcinit: control must be a list")
    if (any(!(names(control) %in% names(control.default)))) stop(paste("MARSSmcinit: allowed control list elements are", names(control.default)))
    control.new <- control.default
    for (i in names(control)) control.new[[i]] <- control[[i]]
    control <- control.new
  }
  drawProgressBar <- FALSE
  if (!silent) { # then we can draw a progress bar
    cat("\n")
    cat("> Starting Monte Carlo Initializations\n")
    prev <- MARSS:::progressBar() # this is an internal function to MARSS
    drawProgressBar <- TRUE
  }
  MODELobj <- MLEobj[["marss"]]
  y <- MODELobj$data
  par.dims <- attr(MODELobj, "model.dims")
  m <- par.dims[["x"]][1]
  n <- par.dims[["y"]][1]
  TT <- par.dims[["data"]][2]
  ## YM matrix for handling missing values
  YM <- matrix(as.numeric(!is.na(y)), n, TT)
  # Make sure the missing vals in y are zeroed out
  y[YM == 0] <- 0

  free.tmp <- MODELobj$free
  dim.tmp <- list(Z = c(n, m), A = c(n, 1), R = c(n, n), B = c(m, m), U = c(m, 1), Q = c(m, m), x0 = c(m, 1))
  bounds.tmp <- control$MCbounds
  init <- bestinits <- MLEobj$start
  bestLL <- -1.0e10

  # loop over numInits: # of random draws of initial values
  for (loop in 1:control$numInits) {
    init.loop <- init

    # Draw random values
    en <- c("Z", "A", "R", "B", "U", "Q", "x0")
    for (el in en) {
      dim.param <- dim.tmp[[el]]
      if (!MARSS:::is.fixed(free.tmp[[el]])) { # is.fixed is a utility func in MARSS
        bounds.param <- bounds.tmp[[el]]
        # use the first fixed and free in a temporally varying model; arbitrary
        tmp <- list(f = MARSS:::sub3D(MODELobj$fixed[[el]], t = 1), D = MARSS:::sub3D(MODELobj$free[[el]], t = 1))
        if (el %in% c("Q", "R")) { # random starts drawn from a wishart dist
          if (bounds.param[1] < dim.param[1]) {
            df <- dim.param[1]
          } else {
            df <- bounds.param[1]
          }
          S <- diag(bounds.param[2], dim.param[1])
          # draw a random matrix from wishart
          tmp.random <- MARSS:::rwishart(df, S) / df
          # reapply the sharing and fixed constraints
          par.random <- solve(t(tmp$D) %*% tmp$D) %*% t(tmp$D) %*% (MARSS:::vec(tmp.random) - tmp$f)
        } else {
          par.random <- matrix(runif(dim(tmp$D)[2], bounds.param[1], bounds.param[2]), dim(tmp$D)[2], 1)
          if (el %in% c("B")) {
            tmp.max <- max(abs(eigen(par.random, only.values = TRUE)$values))
            # rescale to bring the max abs eigenvalues to between 0 and 1
            par.random <- par.random / (tmp.max / runif(1, .01, .99))
          }
          if (el %in% c("x0")) {
            x0init <- init$x0 # where the original start is
            x.lo <- ifelse(x0init > 0, exp(bounds.param[1]) * x0init, exp(bounds.param[2]) * x0init)
            x.hi <- ifelse(x0init > 0, exp(bounds.param[2]) * x0init, exp(bounds.param[1]) * x0init)
            par.random <- matrix(runif(dim(tmp$D)[2], x.lo, x.hi), dim(tmp$D)[2], 1)
          }
        }
      } else {
        par.random <- matrix(0, 0, 1)
      }
      init.loop[[el]] <- par.random
    }

    ## Call MARSSkem() with these inits
    MLEobj$start <- init.loop
    MLEobj$control$maxit <- control$numInitSteps
    MLEobj$control$minit <- 1
    MLEobj$control$silent <- TRUE # don't output
    MLEobj <- MARSSkem(MLEobj) # get new fit using this init

    if (drawProgressBar == TRUE) prev <- MARSS:::progressBar(loop / control$numInits, prev)

    ## Check whether the likelihood is the best observed
    ## Only use bootstrap param draws where loglike did not go down during numInitSteps
    if (MLEobj$logLik > bestLL) {
      # update the best initial parameter estimates
      bestinits <- MLEobj$par
      bestLL <- MLEobj$logLik
    }
  } # end numInits loop

  return(bestinits)
}


fit1 <- MARSS(marss_data_wide)
MCinits <- MARSSmcinit(fit1, control = list(numInits = 10))
fit2 <- MARSS(marss_data_wide, inits = MCinits)
str(MCinits)
```
MARSSmcinit suggests 
- B should be identity but could be diagonal and unequal (unlikely but can try diagonal and equal). 
        Identity AIC: -4110, AICc -3949, log-likelihood: 2355
        Diagonal and equal AIC: -4580, AICc -4418, log-likelihood: 2591
        Diagonal and unequal AIC: 
- U should be unequal or unconstrained
- Q should be either diagonal and unequal or unconstrained
- Z should be identity
- A should be zero but scalar is possible (but unlikely)
- R could be anything but Identity (because value is 0.00187)
        Equalvarcov: measurement error equal but might be different
        Diag+Equal: Equal variance for all observations but covariance is zero
        Diag+Unequal: Some measurements might have more error variability than others
        Unconstranied: Most complex, do this at the very end (and at end of day)

structure of U and Q specify our hypotheses/assumptions about how the environment is shared between populations

### testing basic models and working our way up
#### Basic model
```{r}
model_list2 <- list(
  B = "identity",
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = "zero",
  R = "identity"
)
fit2 <- MARSS(marss_data_wide, model=model_list2)
```

#### A 
```{r}
#genotype_vec <- rep(c("A", "B", "C", "D", "E"), times = 15)
#AA <- model.matrix(~ (genotype_vec) - 1) 
#colnames(AA) <- levels(factor(genotype_vec))

# Create the genotype vector
genotype_vec <- rep(c("A", "B", "C", "D", "E"), times = 15)
genotype_numeric <- as.numeric(factor(genotype_vec, levels = c("A", "B", "C", "D", "E")))
A_matrix <- matrix(genotype_numeric, ncol = 1)

model_list4 <- list(
  B = "identity",
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit4 <- MARSS(marss_data_wide, model=model_list4)
```

#### B
```{r}
#create unique B matrix where each genet has its own ID
n_genets <- 5
n_tanks <- 15
n_time <- 19
n_states <- n_genets * n_tanks
B_matrix <- matrix(0, n_states, n_states)
for (i in 0:(n_genets - 1)) {
  start_row <- i * n_tanks + 1
  end_row <- (i + 1) * n_tanks
  B_matrix[start_row:end_row, start_row:end_row] <- diag(n_tanks)
}

model_list5 <- list(
  B = B_matrix,
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit5 <- MARSS(marss_data_wide, model=model_list5)

model_list6 <- list(
  B = "diagonal and unequal",
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit6 <- MARSS(marss_data_wide, model=model_list6)

model_list7 <- list(
  B = "diagonal and equal",
  U = "zero",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit7 <- MARSS(marss_data_wide, model=model_list7)

#diagonal and equal fits less good but is way faster to run - by 10+ minutes. need to follow up with biological interpretation since "diagonal and equal" means genet responses to environment is uniform vs. "diagonal and unequal" means each genet responds differently to environment (aka one genet thrives more)

# Fit model with priors
priors5 <- MARSS(marss_data_wide, model=model_list5,control=list(maxit=100, allow.degen=TRUE, trace=1, safe=TRUE), fit=TRUE)
fit8 <- MARSS(marss_data_wide, model=model_list5, control=list(maxit = 1000), inits=priors5$par)

#doesn't improve fit BUT it did speed everything up so definitely do this with legit models in the future.
```

#### U
```{r}
model_list9 <- list(
  B = "diagonal and equal",
  U = "unequal",
  Q = "identity",
  Z = "identity",
  A = A_matrix,
  R = "identity"
)
fit9 <- MARSS(marss_data_wide, model=model_list9)
#priors9 <- MARSS(marss_data_wide, model=model_list9,control=list(maxit=100, allow.degen=TRUE, trace=1, safe=TRUE), fit=TRUE)
#fit9prior <- MARSS(marss_data_wide, model=model_list9, control=list(maxit = 1000), inits=priors9$par)

model_list10 <- list(
  B = "diagonal and equal",
  U = "unconstrained",
  Q = "identity",
  Z = "identity", 
  A = A_matrix,
  R = "identity"
)
fit10 <- MARSS(marss_data_wide, model=model_list10)
```
Unconstrained U was the same model results as unequal U, meaning that process error was UNEQUAL (and not potentially the same, which would have been an option with unconstrained)

#### Q
On FUERTE mac, I ran diagonalxequal, diagonalxunequal, and unconstrained and found that diagonal and equal and unconstrained were better fits (though none converged) and had the same AIC values
```{r}
```

#### R
On FUERTE mac, I ran everything except R=identity. Only the two diagonal models converged, and "diagonal and equal" had a slightly better fit. 
```{r}
```

### Add tempearture covariate
```{r}
# Make C matrix for temperature covariate
C_matrix <- matrix(0, nrow = 75, ncol = 19)

# Fill the C matrix
for (time_point in 1:19) {
  for (genotype in 1:5) {
    # Each group of 5 genotypes is influenced by the temperature at the given time point
    C_matrix[(time_point - 1) * 5 + genotype, time_point] <- 1
  }
}
```


### harbor seal data
```{r}
data(harborSealWA, package = "MARSS")
dat <- MARSS::harborSealWA
years <- dat[, "Year"]
dat <- dat[, !(colnames(dat) %in% c("Year", "HC"))]
dat <- t(dat)  # transpose to have years across columns
colnames(dat) <- years
n <- nrow(dat) - 1

kem <- MARSS(dat)

mod.list.0 <- list(B = matrix(1), U = matrix("u"), Q = matrix("q"), 
                   Z = matrix(1, 4, 1), A = "scaling", R = "diagonal and unequal", 
                   x0 = matrix("mu"), tinitx = 0)
fit.0 <- MARSS(dat, model = mod.list.0)
autoplot(fit.0)

mod.list.1 <- list(B = matrix(1), U = matrix("u"), Q = matrix("q"), 
                   Z = matrix(1, 4, 1), A = "scaling", R = "diagonal and equal", 
                   x0 = matrix("mu"), tinitx = 0)
fit.1 <- MARSS(dat, model = mod.list.1)
# more supported
c(AIC(fit.0), AIC(fit.1))

# Q2. Change to observation errors R="unconstrained" What does that mean? Are the errors correlated across sites? Why might that happen?
mod.list.2 <- list(B = matrix(1), U = matrix("u"), Q = matrix("q"), 
                   Z = matrix(1, 4, 1), A = "scaling", R = "unconstrained", 
                   x0 = matrix("mu"), tinitx = 0)
fit.2 <- MARSS(dat, model = mod.list.2)
# less supported
c(AIC(fit.0), AIC(fit.1), AIC(fit.2))
# Means that the observation variances are correlated across sites

# Q3. Look at the observation variance and correlation matrix
M=coef(fit.2, type="matrix")$R # Variance matrix
cov2cor(M) # Correlation matrix
# Some of the errors seem to be correlated across sites, some positive, some negative

# Task Group 2
# https://nwfsc-timeseries.github.io/atsa-labs/sec-mss-segind.html
# Fit a model with four underlying state (population process)
mod.list.1 <- list(B = diag(1,4), U = matrix("u",4,1), Q = "diagonal and equal", 
                   Z = diag(1,4), A = "scaling", R = "diagonal and unequal", 
                   x0 = "unequal", tinitx = 0)
fit.1 <- MARSS(dat, model = mod.list.1)
# Q1. Look at the plots
autoplot(fit.1)

# Plot 2. States - That's the population estimate
# Q2. In the model you fit, are the 4 state processes (the lines) correlated with each other? Just based on the model you fit? Think about Q.

# No, not correlated. A good year in subpop 1 does not imply a good year in subpop 2.

# Q3. What does this model say about the observation variances? Same, different?

# Different levels of variance and independent errors.

# Q4. Are we assuming that good years are correlated across sites or uncorrelated?

# uncorrelated.

# Task Group 3
# https://nwfsc-timeseries.github.io/atsa-labs/sec-mss-segind.html
# Fit a model with four underlying state (population process)
mod.list.2 <- list(B = diag(1,4), U = matrix("u",4,1), Q = "equalvarcov", 
                   Z = diag(1,4), A = "scaling", R = "diagonal and equal", 
                   x0 = "unequal", tinitx = 0)
fit.2 <- MARSS(dat, model = mod.list.2)
# Q1. Look at the plots
autoplot(fit.2)

# Plot 2. States - That's the population estimate
# Q2. In the model you specified, is the variability in the 4 state processes (the lines) correlated with each other? Think about Q.

# Correlated but with the same variance. A good year in subpop 1 does not imply a good year in subpop 2.

#Q.diag      0.01218
#Q.offdiag   0.00983

#So pretty highly correlated.


# Q3. Compare to a model with Q="diagonal and equal" and R="equalvarcov". Which one fits the data better based on AICc?
mod.list.3 <- list(B = diag(1,4), U = matrix("u",4,1), Q = "diagonal and equal", 
                   Z = diag(1,4), A = "scaling", R = "equalvarcov", 
                   x0 = "unequal", tinitx = 0)
fit.3 <- MARSS(dat, model = mod.list.3)
# The model with R="equalvarcov" is less supported
c(AIC(fit.2), AIC(fit.3))

```

### fish spawn data
```{r}
install.packages("rCAX")
library(rCAX)
f <- list(nmfs_popid=7)
tab <- rcax_hli("NOSA")

columbia.river <- tab %>%
  filter(esu_dps %in% c("Steelhead (Middle Columbia River DPS)","Steelhead (Upper Columbia River DPS)","Steelhead (Lower Columbia River DPS)","Salmon, coho (Lower Columbia River ESU)","Salmon, Chinook (Lower Columbia River ESU)")) %>%
  select(species, esu_dps, majorpopgroup, esapopname, commonpopname, run, spawningyear, nosaij) %>%
  rename("value" = nosaij) %>%
  #remove NA values
  filter(!is.na(value)) %>%
  mutate(value = as.numeric(value))

esu <- unique(columbia.river$esu_dps)
colnames(columbia.river)
esuname <- esu[2]
dat <- columbia.river %>% 
  subset(esu_dps == esuname) %>% # get only this ESU
  mutate(log.spawner = log(value)) %>% # create a column called log.spawner
  select(esapopname, spawningyear, log.spawner) %>% # get just the columns that I need
  pivot_wider(names_from = "esapopname", values_from = "log.spawner") %>% 
  column_to_rownames(var = "spawningyear") %>% # make the years rownames
  as.matrix() %>% # turn into a matrix with year down the rows
  t()

dat[is.na(dat)] <- NA
tmp <- rownames(dat)
tmp <- stringr::str_replace(tmp, "Steelhead [(]Upper Columbia River DPS[)]", "")
tmp <- stringr::str_replace(tmp, "River - summer", "")
tmp <- stringr::str_trim(tmp)
rownames(dat) <- tmp


df <- columbia.river %>% subset(esu_dps %in% c("Steelhead (Lower Columbia River DPS)"))
ggplot(df, aes(x=spawningyear, y=log(value), color=majorpopgroup)) + 
  geom_point(size=0.2, na.rm = TRUE) + 
  theme(strip.text.x = element_text(size = 3)) +
  theme(axis.text.x = element_text(size = 5, angle = 90)) +
  facet_wrap(~esapopname)
  #ggtitle(paste0(esuname, collapse="\n"))

plotesu(esu[3])

cap <- cax()
datasets <- cap$datasets()
print(datasets)
columbia-river <- cap$get("dataset_name")  # Use the correct dataset identifier
load(file.path("experiment", "data", "draft", "columbia-river.rda"))
```

### lake plankton data
```{r}
data(lakeWAplankton, package = "MARSS") 
# lakeWA
fulldat <- lakeWAplanktonTrans
years <- fulldat[, "Year"] >= 1965 & fulldat[, "Year"] < 1975
dat <- t(fulldat[years, c("Greens", "Bluegreens")])
covariates <- t(fulldat[years, c("Temp", "TP")])

the.mean <- apply(dat, 1, mean, na.rm = TRUE)
the.sigma <- sqrt(apply(dat, 1, var, na.rm = TRUE))
dat <- (dat - the.mean) * (1/the.sigma)

the.mean <- apply(covariates, 1, mean, na.rm = TRUE)
the.sigma <- sqrt(apply(covariates, 1, var, na.rm = TRUE))
covariates <- (covariates - the.mean) * (1/the.sigma)
```



